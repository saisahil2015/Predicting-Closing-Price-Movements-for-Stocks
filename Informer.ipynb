{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KfhAobi67rXd"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from itertools import combinations\n",
        "from warnings import simplefilter\n",
        "\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYMWZdFB73-U",
        "outputId": "8d4a809e-1945-48bc-a824-b17fae53c7d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\" # https://drive.google.com/drive/folders/18KshTFZ6gGQMeqUf5N6FZq2H1P9fNYlB?usp=sharing"
      ],
      "metadata": {
        "id": "eVXBVL-i8AaS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download optiver-trading-at-the-close\n",
        "!unzip optiver-trading-at-the-close.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8WZo_nJ9ZfH",
        "outputId": "b3cb3723-c2db-4d0c-f9ea-92bfbf2c5afc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading optiver-trading-at-the-close.zip to /content\n",
            " 99% 199M/201M [00:05<00:00, 41.2MB/s]\n",
            "100% 201M/201M [00:05<00:00, 37.8MB/s]\n",
            "Archive:  optiver-trading-at-the-close.zip\n",
            "  inflating: example_test_files/revealed_targets.csv  \n",
            "  inflating: example_test_files/sample_submission.csv  \n",
            "  inflating: example_test_files/test.csv  \n",
            "  inflating: optiver2023/__init__.py  \n",
            "  inflating: optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
            "  inflating: public_timeseries_testing_util.py  \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/train.csv\")"
      ],
      "metadata": {
        "id": "Hl0TLNKM9a1a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df, verbose=0):\n",
        "    \"\"\"\n",
        "    Iterate through all numeric columns of a dataframe and modify the data type\n",
        "    to reduce memory usage.\n",
        "    \"\"\"\n",
        "\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "\n",
        "    if verbose:\n",
        "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
        "        end_mem = df.memory_usage().sum() / 1024**2\n",
        "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
        "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
        "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "from numba import njit, prange\n",
        "\n",
        "@njit(parallel=True)\n",
        "def compute_triplet_imbalance(df_values, comb_indices):\n",
        "    num_rows = df_values.shape[0]\n",
        "    num_combinations = len(comb_indices)\n",
        "    imbalance_features = np.empty((num_rows, num_combinations))\n",
        "\n",
        "    for i in prange(num_combinations):\n",
        "        a, b, c = comb_indices[i]\n",
        "        for j in range(num_rows):\n",
        "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
        "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
        "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
        "            if mid_val == min_val:  # Prevent division by zero\n",
        "                imbalance_features[j, i] = np.nan\n",
        "            else:\n",
        "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
        "\n",
        "    return imbalance_features\n",
        "\n",
        "\n",
        "def calculate_triplet_imbalance_numba(price, df):\n",
        "    # Convert DataFrame to numpy array for Numba compatibility\n",
        "    df_values = df[price].values\n",
        "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
        "\n",
        "    # Calculate the triplet imbalance\n",
        "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
        "\n",
        "    # Create a DataFrame from the results\n",
        "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
        "    features = pd.DataFrame(features_array, columns=columns)\n",
        "\n",
        "    return features\n",
        "\n",
        "# generate imbalance features\n",
        "def imbalance_features(df):\n",
        "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
        "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
        "\n",
        "    # V1\n",
        "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
        "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
        "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
        "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
        "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
        "\n",
        "    for c in combinations(prices, 2):\n",
        "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
        "\n",
        "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
        "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
        "        df[triplet_feature.columns] = triplet_feature.values\n",
        "\n",
        "    # V2\n",
        "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
        "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
        "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
        "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
        "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
        "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
        "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
        "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
        "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
        "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
        "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
        "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
        "\n",
        "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
        "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
        "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
        "\n",
        "    # V3\n",
        "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
        "        for window in [1, 2, 3, 5, 10]:\n",
        "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
        "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
        "\n",
        "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n",
        "                'wap', 'near_price', 'far_price']:\n",
        "        for window in [1, 2, 3, 5, 10]:\n",
        "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
        "\n",
        "    return df.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# generate time & stock features\n",
        "def other_features(df):\n",
        "    df[\"dow\"] = df[\"date_id\"] % 5\n",
        "    df[\"dom\"] = df[\"date_id\"] % 20\n",
        "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
        "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n",
        "\n",
        "    for key, value in global_stock_id_feats.items():\n",
        "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
        "\n",
        "    return df\n",
        "\n",
        "# generate all features\n",
        "def generate_all_features(df):\n",
        "    #cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
        "    #df = df[cols]\n",
        "    df = imbalance_features(df)\n",
        "    df = other_features(df)\n",
        "    gc.collect()\n",
        "\n",
        "    #feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
        "    return df\n",
        "    #return df[feature_name]\n",
        "\n",
        "weights = [\n",
        "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
        "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
        "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
        "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04, 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
        "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
        "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
        "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
        "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
        "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
        "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
        "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
        "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
        "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
        "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
        "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
        "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
        "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
        "]\n",
        "\n",
        "weights = {int(k):v for k,v in enumerate(weights)}"
      ],
      "metadata": {
        "id": "_4fOPXCJ9dxi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_stock_id_feats = {\n",
        "        \"median_size\": df.groupby(\"stock_id\")[\"bid_size\"].median() + df.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
        "        \"std_size\": df.groupby(\"stock_id\")[\"bid_size\"].std() + df.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
        "        \"ptp_size\": df.groupby(\"stock_id\")[\"bid_size\"].max() - df.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
        "        \"median_price\": df.groupby(\"stock_id\")[\"bid_price\"].median() + df.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
        "        \"std_price\": df.groupby(\"stock_id\")[\"bid_price\"].std() + df.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
        "        \"ptp_price\": df.groupby(\"stock_id\")[\"bid_price\"].max() - df.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
        "  }"
      ],
      "metadata": {
        "id": "DCpn7rzCoMBy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_day = 435\n",
        "# df_train = df[df[\"date_id\"] <= split_day]\n",
        "# df_valid = df[df[\"date_id\"] > split_day]\n",
        "\n",
        "# global_stock_id_feats = {\n",
        "#         \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
        "#         \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
        "#         \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
        "#         \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
        "#         \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
        "#         \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
        "#   }\n"
      ],
      "metadata": {
        "id": "GZSEDajaVYfl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train = df\n",
        "# df_train_feats = generate_all_features(df_train)\n",
        "# df_train_feats = reduce_mem_usage(df_train_feats)\n",
        "\n",
        "# offline_split = df_train['date_id']>(split_day - 45)\n",
        "# df_offline_train = df_train_feats[~offline_split]\n",
        "# df_offline_valid = df_train_feats[offline_split]\n",
        "# df_offline_train_target = df_train['target'][~offline_split]\n",
        "# df_offline_valid_target = df_train['target'][offline_split]\n",
        "\n",
        "# # Check and remove NaN values\n",
        "# df_offline_train_target = df_offline_train_target.dropna()\n",
        "# df_offline_valid_target = df_offline_valid_target.dropna()\n",
        "\n",
        "# # Check and remove infinite values, if any\n",
        "# df_offline_train_target.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "# df_offline_valid_target.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# df_offline_train_target.dropna(inplace=True)\n",
        "# df_offline_valid_target.dropna(inplace=True)\n",
        "\n",
        "# df_offline_train = df_offline_train.loc[df_offline_train_target.index]\n",
        "# df_offline_valid = df_offline_valid.loc[df_offline_valid_target.index]\n",
        "\n",
        "# # Function to remove NaN and Inf values from DataFrame\n",
        "# def remove_nan_inf(df):\n",
        "#     df.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace Inf with NaN\n",
        "#     df.dropna(inplace=True)  # Drop all NaNs\n",
        "#     return df\n",
        "\n",
        "# # Clean the data for both features and targets\n",
        "# df_offline_train = remove_nan_inf(df_offline_train)\n",
        "# df_offline_valid = remove_nan_inf(df_offline_valid)\n",
        "# df_offline_train_target = remove_nan_inf(df_offline_train_target)\n",
        "# df_offline_valid_target = remove_nan_inf(df_offline_valid_target)\n",
        "\n",
        "# # Ensure the indices of features and targets match\n",
        "# df_offline_train_target = df_offline_train_target.loc[df_offline_train.index]\n",
        "# df_offline_valid_target = df_offline_valid_target.loc[df_offline_valid.index]\n",
        "# df_offline_train = df_offline_train.reindex(df_offline_train_target.index)\n",
        "# df_offline_valid = df_offline_valid.reindex(df_offline_valid_target.index)\n",
        "\n",
        "# X_train = torch.tensor(df_offline_train.values, dtype=torch.float32)\n",
        "# y_train = torch.tensor(df_offline_train_target.values, dtype=torch.float32)\n",
        "# X_val = torch.tensor(df_offline_valid.values, dtype=torch.float32)\n",
        "# y_val = torch.tensor(df_offline_valid_target.values, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "a5YtG4PUaFvl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imputer(df):\n",
        "    far_price_mean = df['far_price'].mean()\n",
        "    near_price_mean = df['near_price'].mean()\n",
        "    df['far_price'] = df['far_price'].fillna(far_price_mean)\n",
        "    df['near_price'] = df['near_price'].fillna(near_price_mean)\n",
        "\n",
        "    return df, far_price_mean, near_price_mean\n",
        "\n",
        "def add_missing_data(df):\n",
        "    all_stock_ids = set(range(200))\n",
        "    all_missed_data_list = []\n",
        "\n",
        "    grouped = df.groupby('time_id')\n",
        "\n",
        "    for t, group in grouped:\n",
        "        current_stock_ids = set(group['stock_id'].to_list())\n",
        "        missed_stock_id = list(all_stock_ids - current_stock_ids)\n",
        "\n",
        "        date_id = group['date_id'].iloc[-1]\n",
        "        seconds_in_bucket = group['seconds_in_bucket'].iloc[-1]\n",
        "\n",
        "        missed_stock_id_num = len(missed_stock_id)\n",
        "        missed_date_id = [date_id] * missed_stock_id_num\n",
        "        missed_seconds_in_bucket = [seconds_in_bucket] * missed_stock_id_num\n",
        "        missed_time_id = [t] * missed_stock_id_num\n",
        "\n",
        "        missed_data = pd.DataFrame({\n",
        "            'stock_id': missed_stock_id,\n",
        "            'date_id': missed_date_id,\n",
        "            'seconds_in_bucket': missed_seconds_in_bucket,\n",
        "            'time_id': missed_time_id\n",
        "        })\n",
        "\n",
        "        all_missed_data_list.append(missed_data)\n",
        "\n",
        "    all_missed_data = pd.concat(all_missed_data_list, axis=0).reset_index(drop=True).astype(int)\n",
        "\n",
        "    df = pd.concat([df, all_missed_data], axis=0)\n",
        "    df = df.sort_values(by=['time_id', 'stock_id']).reset_index(drop=True)\n",
        "    df = df.groupby('stock_id').apply(lambda x: x.fillna(method='bfill')).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "train, far_price_mean, near_price_mean = imputer(df)\n",
        "train = add_missing_data(df)\n",
        "\n",
        "def sizesum_and_pricestd(df):\n",
        "    price_ftrs = ['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'wap'] # std\n",
        "    size_ftrs = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size'] # sum\n",
        "\n",
        "    rolled = df[['stock_id'] + size_ftrs].groupby('stock_id').rolling(window=6, min_periods=1).sum()\n",
        "    rolled = rolled.reset_index(level=0, drop=True)\n",
        "    for col in size_ftrs:\n",
        "        df[f'{col}_rolled_sum'] = rolled[col]\n",
        "\n",
        "    rolled = df[['stock_id'] + price_ftrs].groupby('stock_id').rolling(window=6, min_periods=1).std().fillna(0)\n",
        "    rolled = rolled.reset_index(level=0, drop=True)\n",
        "    for col in price_ftrs:\n",
        "        df[f'{col}_rolled_std'] = rolled[col]\n",
        "\n",
        "    return df\n",
        "\n",
        "train = sizesum_and_pricestd(train)\n",
        "\n",
        "def remove_element(input_list, drop_list):\n",
        "    return [e for e in input_list if e not in drop_list]\n",
        "\n",
        "no_feature_cols = ['date_id', 'row_id', 'time_id', 'target', 'currently_scored']\n",
        "\n",
        "feature_cols = remove_element(train.columns, no_feature_cols)\n",
        "target_col = 'target'"
      ],
      "metadata": {
        "id": "x0xZYoJ6tnOx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg = train[feature_cols].mean()\n",
        "std = train[feature_cols].std()\n",
        "\n",
        "train[feature_cols] = (train[feature_cols] - avg)/std"
      ],
      "metadata": {
        "id": "GJRqvLW3wXls"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.astype('float32')\n",
        "train = reduce_mem_usage(train)\n",
        "\n",
        "seq_len = 16\n",
        "\n",
        "# Grouping by time_id\n",
        "grouped_by_time = train.groupby('stock_id')\n",
        "\n",
        "def generate_data(grouped_by_time, seq_len):\n",
        "    for _, group in grouped_by_time:\n",
        "        # Sorting by stock_id to maintain consistency across images\n",
        "        group_sorted = group.sort_values(by='time_id')\n",
        "\n",
        "        features = group_sorted[feature_cols].values\n",
        "\n",
        "        windows = []\n",
        "\n",
        "        for t in range(0, seq_len - 1):\n",
        "            copy_0 = np.stack([features[0]] * (seq_len - 1 - t))\n",
        "            cut_0 = features[: t + 1]\n",
        "            windows.append(np.vstack((copy_0, cut_0)))\n",
        "\n",
        "        for t in range(0, features.shape[0] - seq_len + 1):\n",
        "            windows.append(features[t: t+seq_len, :])\n",
        "\n",
        "        # Convert list of windows to numpy array\n",
        "        features_array = np.stack(windows)\n",
        "\n",
        "        target = group_sorted['target'].values\n",
        "\n",
        "        # Yield the result for this group to avoid storing all results in memory\n",
        "        yield features_array, target\n",
        "\n",
        "# Use generator to iterate over data\n",
        "data_generator = generate_data(grouped_by_time, seq_len=seq_len)\n",
        "\n",
        "# If you need to store results in arrays:\n",
        "datas, labels = zip(*data_generator)\n",
        "data = np.array(datas).reshape(-1, seq_len, len(feature_cols))\n",
        "label = np.array(labels).reshape(-1,)"
      ],
      "metadata": {
        "id": "2QQUGWXKvxka"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "\n",
        "data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "label = torch.tensor(label, dtype=torch.float32).to(device)\n",
        "\n",
        "dataset = TensorDataset(data, label)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "7M9nOwKAwq8L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "9mS2zcY_5Vxc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ProbSparseAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        super(ProbSparseAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.attention(x, x, x)\n",
        "        return output\n",
        "\n",
        "class InformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, attention_mode='prob'):\n",
        "        super(InformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([ProbSparseAttention(d_model, nhead) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class InformerDecoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, attention_mode='prob'):\n",
        "        super(InformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([ProbSparseAttention(d_model, nhead) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class MyInformerModel(nn.Module):\n",
        "    def __init__(self, feature_num, d_model, nhead, num_layers, attn_mode='prob'):\n",
        "        super(MyInformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(feature_num, d_model)\n",
        "        self.encoder = InformerEncoder(d_model, nhead, num_layers, attn_mode)\n",
        "        self.decoder = InformerDecoder(d_model, nhead, num_layers, attn_mode)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x_enc = self.encoder(x)\n",
        "        x_dec = self.decoder(x_enc)\n",
        "        x_out = self.fc(x_dec[:, -1, :])\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "_jG88p0g4ARL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\n",
        "import math\n",
        "\n",
        "tot_train_losses = []\n",
        "tot_train_mae = []\n",
        "tot_train_rmse = []\n",
        "tot_train_rmsle = []\n",
        "\n",
        "tot_valid_losses = []\n",
        "tot_valid_mae = []\n",
        "tot_valid_rmse = []\n",
        "tot_valid_rmsle = []\n",
        "\n",
        "is_train = True\n",
        "if is_train:\n",
        "    input_size = data.shape[-1]\n",
        "\n",
        "    n_epochs = 50\n",
        "    lr = 1e-03\n",
        "\n",
        "    pre_epoch_valid_mae = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    model = MyInformerModel(feature_num=input_size, d_model=64, nhead=2, num_layers=1).to(device)\n",
        "    #model = MyModel(feature_num=input_size, d_model=64, nhead=2, num_layers=1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    loss = nn.L1Loss().to(device)\n",
        "    #loss = nn.L1Loss()\n",
        "\n",
        "    out_path = \"model/\"\n",
        "    if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "    best_mae = np.inf\n",
        "\n",
        "    print(f'Train start...')\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_mae = []\n",
        "        train_rmse = []\n",
        "        batch_num = len(train_loader)\n",
        "\n",
        "        # Training\n",
        "        for X, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X).squeeze()\n",
        "            l = loss(outputs, y)\n",
        "            l.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "            train_losses.append(l.item())\n",
        "\n",
        "            # Calculate MAE\n",
        "            mae = mean_absolute_error(y.cpu().numpy(), outputs.detach().cpu().numpy())\n",
        "            train_mae.append(mae)\n",
        "\n",
        "            # Calculate RMSE\n",
        "            rmse = math.sqrt(mean_squared_error(y.cpu().numpy(), outputs.detach().cpu().numpy()))\n",
        "            train_rmse.append(rmse)\n",
        "\n",
        "        epoch_train_loss = np.mean(train_losses)\n",
        "        epoch_train_mae = np.mean(train_mae)\n",
        "        epoch_train_rmse = np.mean(train_rmse)\n",
        "\n",
        "        tot_train_losses.append(epoch_train_loss)\n",
        "        tot_train_mae.append(epoch_train_mae)\n",
        "        tot_train_rmse.append(epoch_train_rmse)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}] Training Loss: {epoch_train_loss:.4f}')\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}] Training MAE: {epoch_train_mae:.4f}')\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}] Training RMSE: {epoch_train_rmse:.4f}')\n",
        "\n",
        "        train_maes = []\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_losses = []\n",
        "            valid_maes = []\n",
        "            valid_rmse = []\n",
        "            valid_rmsle = []\n",
        "\n",
        "            for X_v, y_v in valid_loader:\n",
        "                preds = model(X_v).squeeze()\n",
        "                valid_loss = loss(preds, y_v)\n",
        "                valid_losses.append(valid_loss.item())\n",
        "\n",
        "                # Calculate MAE\n",
        "                valid_mae = mean_absolute_error(y_v.cpu().numpy(), preds.cpu().numpy())\n",
        "                valid_maes.append(valid_mae)\n",
        "\n",
        "                # Calculate RMSE\n",
        "                valid_rmse_val = math.sqrt(mean_squared_error(y_v.cpu().numpy(), preds.cpu().numpy()))\n",
        "                valid_rmse.append(valid_rmse_val)\n",
        "\n",
        "            epoch_valid_loss = np.mean(valid_losses)\n",
        "            epoch_valid_mae = np.mean(valid_maes)\n",
        "            epoch_valid_rmse = np.mean(valid_rmse)\n",
        "\n",
        "            tot_valid_losses.append(epoch_train_loss)\n",
        "            tot_valid_mae.append(epoch_train_mae)\n",
        "            tot_valid_rmse.append(epoch_train_rmse)\n",
        "\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}] Validation Loss: {epoch_valid_loss:.4f}')\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}] Validation MAE: {epoch_valid_mae:.4f}')\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}] Validation RMSE: {epoch_valid_rmse:.4f}')\n",
        "\n",
        "\n",
        "            if epoch_valid_mae < best_mae:\n",
        "                best_mae = epoch_valid_mae\n",
        "                torch.save(model, os.path.join(out_path, f\"model_epoch_{epoch+1}.pt\"))\n",
        "\n",
        "        if epoch_valid_mae - pre_epoch_valid_mae > 0:\n",
        "            patience_counter += 1\n",
        "\n",
        "            if patience_counter == 2:\n",
        "                lr = lr * 0.5\n",
        "                patience_counter = 0\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "                    print(f'renew lr to {lr}')\n",
        "\n",
        "        pre_epoch_valid_mae = epoch_valid_mae\n",
        "\n",
        "        if (epoch_valid_mae - epoch_train_mae > 0.03) or (lr <1e-7):\n",
        "            print('Early stop now.')\n",
        "            break\n",
        "\n",
        "    print(f'Train over.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdqZ1qPoxWxu",
        "outputId": "175a9a86-1aac-490a-bbe2-fa08b86c5bcd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train start...\n",
            "Epoch [1/50] Training Loss: 6.4830\n",
            "Epoch [1/50] Training MAE: 6.4830\n",
            "Epoch [1/50] Training RMSE: 9.6304\n",
            "Epoch [1/50] Validation Loss: 6.4800\n",
            "Epoch [1/50] Validation MAE: 6.4800\n",
            "Epoch [1/50] Validation RMSE: 9.6153\n",
            "Epoch [2/50] Training Loss: 6.4784\n",
            "Epoch [2/50] Training MAE: 6.4784\n",
            "Epoch [2/50] Training RMSE: 9.6242\n",
            "Epoch [2/50] Validation Loss: 6.4824\n",
            "Epoch [2/50] Validation MAE: 6.4824\n",
            "Epoch [2/50] Validation RMSE: 9.6211\n",
            "Epoch [3/50] Training Loss: 6.4803\n",
            "Epoch [3/50] Training MAE: 6.4803\n",
            "Epoch [3/50] Training RMSE: 9.6274\n",
            "Epoch [3/50] Validation Loss: 6.4824\n",
            "Epoch [3/50] Validation MAE: 6.4824\n",
            "Epoch [3/50] Validation RMSE: 9.6210\n",
            "renew lr to 0.0005\n",
            "Epoch [4/50] Training Loss: 6.4794\n",
            "Epoch [4/50] Training MAE: 6.4794\n",
            "Epoch [4/50] Training RMSE: 9.6260\n",
            "Epoch [4/50] Validation Loss: 6.4798\n",
            "Epoch [4/50] Validation MAE: 6.4798\n",
            "Epoch [4/50] Validation RMSE: 9.6171\n",
            "Epoch [5/50] Training Loss: 6.4763\n",
            "Epoch [5/50] Training MAE: 6.4763\n",
            "Epoch [5/50] Training RMSE: 9.6218\n",
            "Epoch [5/50] Validation Loss: 6.4755\n",
            "Epoch [5/50] Validation MAE: 6.4755\n",
            "Epoch [5/50] Validation RMSE: 9.6111\n",
            "Epoch [6/50] Training Loss: 6.4767\n",
            "Epoch [6/50] Training MAE: 6.4767\n",
            "Epoch [6/50] Training RMSE: 9.6222\n",
            "Epoch [6/50] Validation Loss: 6.4804\n",
            "Epoch [6/50] Validation MAE: 6.4804\n",
            "Epoch [6/50] Validation RMSE: 9.6176\n",
            "Epoch [7/50] Training Loss: 6.4786\n",
            "Epoch [7/50] Training MAE: 6.4786\n",
            "Epoch [7/50] Training RMSE: 9.6239\n",
            "Epoch [7/50] Validation Loss: 6.4792\n",
            "Epoch [7/50] Validation MAE: 6.4792\n",
            "Epoch [7/50] Validation RMSE: 9.6160\n",
            "Epoch [8/50] Training Loss: 6.4767\n",
            "Epoch [8/50] Training MAE: 6.4767\n",
            "Epoch [8/50] Training RMSE: 9.6225\n",
            "Epoch [8/50] Validation Loss: 6.4782\n",
            "Epoch [8/50] Validation MAE: 6.4782\n",
            "Epoch [8/50] Validation RMSE: 9.6166\n",
            "Epoch [9/50] Training Loss: 6.4756\n",
            "Epoch [9/50] Training MAE: 6.4756\n",
            "Epoch [9/50] Training RMSE: 9.6214\n",
            "Epoch [9/50] Validation Loss: 6.4761\n",
            "Epoch [9/50] Validation MAE: 6.4761\n",
            "Epoch [9/50] Validation RMSE: 9.6137\n",
            "Epoch [10/50] Training Loss: 6.4753\n",
            "Epoch [10/50] Training MAE: 6.4753\n",
            "Epoch [10/50] Training RMSE: 9.6206\n",
            "Epoch [10/50] Validation Loss: 6.4732\n",
            "Epoch [10/50] Validation MAE: 6.4732\n",
            "Epoch [10/50] Validation RMSE: 9.6093\n",
            "Epoch [11/50] Training Loss: 6.4749\n",
            "Epoch [11/50] Training MAE: 6.4749\n",
            "Epoch [11/50] Training RMSE: 9.6206\n",
            "Epoch [11/50] Validation Loss: 6.4825\n",
            "Epoch [11/50] Validation MAE: 6.4825\n",
            "Epoch [11/50] Validation RMSE: 9.6209\n",
            "renew lr to 0.00025\n",
            "Epoch [12/50] Training Loss: 6.4761\n",
            "Epoch [12/50] Training MAE: 6.4761\n",
            "Epoch [12/50] Training RMSE: 9.6235\n",
            "Epoch [12/50] Validation Loss: 6.4767\n",
            "Epoch [12/50] Validation MAE: 6.4767\n",
            "Epoch [12/50] Validation RMSE: 9.6154\n",
            "Epoch [13/50] Training Loss: 6.4727\n",
            "Epoch [13/50] Training MAE: 6.4727\n",
            "Epoch [13/50] Training RMSE: 9.6190\n",
            "Epoch [13/50] Validation Loss: 6.4749\n",
            "Epoch [13/50] Validation MAE: 6.4749\n",
            "Epoch [13/50] Validation RMSE: 9.6131\n",
            "Epoch [14/50] Training Loss: 6.4721\n",
            "Epoch [14/50] Training MAE: 6.4721\n",
            "Epoch [14/50] Training RMSE: 9.6177\n",
            "Epoch [14/50] Validation Loss: 6.4752\n",
            "Epoch [14/50] Validation MAE: 6.4752\n",
            "Epoch [14/50] Validation RMSE: 9.6112\n",
            "Epoch [15/50] Training Loss: 6.4738\n",
            "Epoch [15/50] Training MAE: 6.4738\n",
            "Epoch [15/50] Training RMSE: 9.6195\n",
            "Epoch [15/50] Validation Loss: 6.4760\n",
            "Epoch [15/50] Validation MAE: 6.4760\n",
            "Epoch [15/50] Validation RMSE: 9.6134\n",
            "renew lr to 0.000125\n",
            "Epoch [16/50] Training Loss: 6.4735\n",
            "Epoch [16/50] Training MAE: 6.4735\n",
            "Epoch [16/50] Training RMSE: 9.6197\n",
            "Epoch [16/50] Validation Loss: 6.4764\n",
            "Epoch [16/50] Validation MAE: 6.4764\n",
            "Epoch [16/50] Validation RMSE: 9.6138\n",
            "Epoch [17/50] Training Loss: 6.4709\n",
            "Epoch [17/50] Training MAE: 6.4709\n",
            "Epoch [17/50] Training RMSE: 9.6160\n",
            "Epoch [17/50] Validation Loss: 6.4732\n",
            "Epoch [17/50] Validation MAE: 6.4732\n",
            "Epoch [17/50] Validation RMSE: 9.6079\n",
            "Epoch [18/50] Training Loss: 6.4693\n",
            "Epoch [18/50] Training MAE: 6.4693\n",
            "Epoch [18/50] Training RMSE: 9.6136\n",
            "Epoch [18/50] Validation Loss: 6.4720\n",
            "Epoch [18/50] Validation MAE: 6.4720\n",
            "Epoch [18/50] Validation RMSE: 9.6069\n",
            "Epoch [19/50] Training Loss: 6.4692\n",
            "Epoch [19/50] Training MAE: 6.4692\n",
            "Epoch [19/50] Training RMSE: 9.6129\n",
            "Epoch [19/50] Validation Loss: 6.4710\n",
            "Epoch [19/50] Validation MAE: 6.4710\n",
            "Epoch [19/50] Validation RMSE: 9.6057\n",
            "Epoch [20/50] Training Loss: 6.4681\n",
            "Epoch [20/50] Training MAE: 6.4681\n",
            "Epoch [20/50] Training RMSE: 9.6116\n",
            "Epoch [20/50] Validation Loss: 6.4701\n",
            "Epoch [20/50] Validation MAE: 6.4701\n",
            "Epoch [20/50] Validation RMSE: 9.6054\n",
            "Epoch [21/50] Training Loss: 6.4666\n",
            "Epoch [21/50] Training MAE: 6.4666\n",
            "Epoch [21/50] Training RMSE: 9.6104\n",
            "Epoch [21/50] Validation Loss: 6.4699\n",
            "Epoch [21/50] Validation MAE: 6.4699\n",
            "Epoch [21/50] Validation RMSE: 9.6040\n",
            "Epoch [22/50] Training Loss: 6.4652\n",
            "Epoch [22/50] Training MAE: 6.4652\n",
            "Epoch [22/50] Training RMSE: 9.6084\n",
            "Epoch [22/50] Validation Loss: 6.4672\n",
            "Epoch [22/50] Validation MAE: 6.4672\n",
            "Epoch [22/50] Validation RMSE: 9.6012\n",
            "Epoch [23/50] Training Loss: 6.4650\n",
            "Epoch [23/50] Training MAE: 6.4650\n",
            "Epoch [23/50] Training RMSE: 9.6086\n",
            "Epoch [23/50] Validation Loss: 6.4671\n",
            "Epoch [23/50] Validation MAE: 6.4671\n",
            "Epoch [23/50] Validation RMSE: 9.6034\n",
            "Epoch [24/50] Training Loss: 6.4645\n",
            "Epoch [24/50] Training MAE: 6.4645\n",
            "Epoch [24/50] Training RMSE: 9.6085\n",
            "Epoch [24/50] Validation Loss: 6.4680\n",
            "Epoch [24/50] Validation MAE: 6.4680\n",
            "Epoch [24/50] Validation RMSE: 9.6015\n",
            "renew lr to 6.25e-05\n",
            "Epoch [25/50] Training Loss: 6.4658\n",
            "Epoch [25/50] Training MAE: 6.4658\n",
            "Epoch [25/50] Training RMSE: 9.6086\n",
            "Epoch [25/50] Validation Loss: 6.4667\n",
            "Epoch [25/50] Validation MAE: 6.4667\n",
            "Epoch [25/50] Validation RMSE: 9.5997\n",
            "Epoch [26/50] Training Loss: 6.4640\n",
            "Epoch [26/50] Training MAE: 6.4640\n",
            "Epoch [26/50] Training RMSE: 9.6064\n",
            "Epoch [26/50] Validation Loss: 6.4664\n",
            "Epoch [26/50] Validation MAE: 6.4664\n",
            "Epoch [26/50] Validation RMSE: 9.6005\n",
            "Epoch [27/50] Training Loss: 6.4643\n",
            "Epoch [27/50] Training MAE: 6.4643\n",
            "Epoch [27/50] Training RMSE: 9.6082\n",
            "Epoch [27/50] Validation Loss: 6.4685\n",
            "Epoch [27/50] Validation MAE: 6.4685\n",
            "Epoch [27/50] Validation RMSE: 9.6045\n",
            "Epoch [28/50] Training Loss: 6.4657\n",
            "Epoch [28/50] Training MAE: 6.4657\n",
            "Epoch [28/50] Training RMSE: 9.6103\n",
            "Epoch [28/50] Validation Loss: 6.4676\n",
            "Epoch [28/50] Validation MAE: 6.4676\n",
            "Epoch [28/50] Validation RMSE: 9.6032\n",
            "Epoch [29/50] Training Loss: 6.4636\n",
            "Epoch [29/50] Training MAE: 6.4636\n",
            "Epoch [29/50] Training RMSE: 9.6079\n",
            "Epoch [29/50] Validation Loss: 6.4666\n",
            "Epoch [29/50] Validation MAE: 6.4666\n",
            "Epoch [29/50] Validation RMSE: 9.6017\n",
            "Epoch [30/50] Training Loss: 6.4629\n",
            "Epoch [30/50] Training MAE: 6.4629\n",
            "Epoch [30/50] Training RMSE: 9.6074\n",
            "Epoch [30/50] Validation Loss: 6.4650\n",
            "Epoch [30/50] Validation MAE: 6.4650\n",
            "Epoch [30/50] Validation RMSE: 9.6003\n",
            "Epoch [31/50] Training Loss: 6.4625\n",
            "Epoch [31/50] Training MAE: 6.4625\n",
            "Epoch [31/50] Training RMSE: 9.6069\n",
            "Epoch [31/50] Validation Loss: 6.4644\n",
            "Epoch [31/50] Validation MAE: 6.4644\n",
            "Epoch [31/50] Validation RMSE: 9.5991\n",
            "Epoch [32/50] Training Loss: 6.4614\n",
            "Epoch [32/50] Training MAE: 6.4614\n",
            "Epoch [32/50] Training RMSE: 9.6056\n",
            "Epoch [32/50] Validation Loss: 6.4643\n",
            "Epoch [32/50] Validation MAE: 6.4643\n",
            "Epoch [32/50] Validation RMSE: 9.5991\n",
            "Epoch [33/50] Training Loss: 6.4609\n",
            "Epoch [33/50] Training MAE: 6.4609\n",
            "Epoch [33/50] Training RMSE: 9.6054\n",
            "Epoch [33/50] Validation Loss: 6.4637\n",
            "Epoch [33/50] Validation MAE: 6.4637\n",
            "Epoch [33/50] Validation RMSE: 9.5992\n",
            "Epoch [34/50] Training Loss: 6.4607\n",
            "Epoch [34/50] Training MAE: 6.4607\n",
            "Epoch [34/50] Training RMSE: 9.6051\n",
            "Epoch [34/50] Validation Loss: 6.4639\n",
            "Epoch [34/50] Validation MAE: 6.4639\n",
            "Epoch [34/50] Validation RMSE: 9.5985\n",
            "renew lr to 3.125e-05\n",
            "Epoch [35/50] Training Loss: 6.4606\n",
            "Epoch [35/50] Training MAE: 6.4606\n",
            "Epoch [35/50] Training RMSE: 9.6041\n",
            "Epoch [35/50] Validation Loss: 6.4643\n",
            "Epoch [35/50] Validation MAE: 6.4643\n",
            "Epoch [35/50] Validation RMSE: 9.5987\n",
            "Epoch [36/50] Training Loss: 6.4602\n",
            "Epoch [36/50] Training MAE: 6.4602\n",
            "Epoch [36/50] Training RMSE: 9.6039\n",
            "Epoch [36/50] Validation Loss: 6.4634\n",
            "Epoch [36/50] Validation MAE: 6.4634\n",
            "Epoch [36/50] Validation RMSE: 9.5982\n",
            "Epoch [37/50] Training Loss: 6.4598\n",
            "Epoch [37/50] Training MAE: 6.4598\n",
            "Epoch [37/50] Training RMSE: 9.6035\n",
            "Epoch [37/50] Validation Loss: 6.4637\n",
            "Epoch [37/50] Validation MAE: 6.4637\n",
            "Epoch [37/50] Validation RMSE: 9.5980\n",
            "renew lr to 1.5625e-05\n",
            "Epoch [38/50] Training Loss: 6.4599\n",
            "Epoch [38/50] Training MAE: 6.4599\n",
            "Epoch [38/50] Training RMSE: 9.6039\n",
            "Epoch [38/50] Validation Loss: 6.4638\n",
            "Epoch [38/50] Validation MAE: 6.4637\n",
            "Epoch [38/50] Validation RMSE: 9.5983\n",
            "Epoch [39/50] Training Loss: 6.4603\n",
            "Epoch [39/50] Training MAE: 6.4603\n",
            "Epoch [39/50] Training RMSE: 9.6045\n",
            "Epoch [39/50] Validation Loss: 6.4638\n",
            "Epoch [39/50] Validation MAE: 6.4638\n",
            "Epoch [39/50] Validation RMSE: 9.5988\n",
            "renew lr to 7.8125e-06\n",
            "Epoch [40/50] Training Loss: 6.4601\n",
            "Epoch [40/50] Training MAE: 6.4601\n",
            "Epoch [40/50] Training RMSE: 9.6045\n",
            "Epoch [40/50] Validation Loss: 6.4635\n",
            "Epoch [40/50] Validation MAE: 6.4635\n",
            "Epoch [40/50] Validation RMSE: 9.5985\n",
            "Epoch [41/50] Training Loss: 6.4600\n",
            "Epoch [41/50] Training MAE: 6.4600\n",
            "Epoch [41/50] Training RMSE: 9.6043\n",
            "Epoch [41/50] Validation Loss: 6.4636\n",
            "Epoch [41/50] Validation MAE: 6.4636\n",
            "Epoch [41/50] Validation RMSE: 9.5985\n",
            "Epoch [42/50] Training Loss: 6.4599\n",
            "Epoch [42/50] Training MAE: 6.4599\n",
            "Epoch [42/50] Training RMSE: 9.6044\n",
            "Epoch [42/50] Validation Loss: 6.4635\n",
            "Epoch [42/50] Validation MAE: 6.4635\n",
            "Epoch [42/50] Validation RMSE: 9.5986\n",
            "Epoch [43/50] Training Loss: 6.4600\n",
            "Epoch [43/50] Training MAE: 6.4600\n",
            "Epoch [43/50] Training RMSE: 9.6045\n",
            "Epoch [43/50] Validation Loss: 6.4636\n",
            "Epoch [43/50] Validation MAE: 6.4636\n",
            "Epoch [43/50] Validation RMSE: 9.5985\n",
            "renew lr to 3.90625e-06\n",
            "Epoch [44/50] Training Loss: 6.4599\n",
            "Epoch [44/50] Training MAE: 6.4599\n",
            "Epoch [44/50] Training RMSE: 9.6043\n",
            "Epoch [44/50] Validation Loss: 6.4634\n",
            "Epoch [44/50] Validation MAE: 6.4634\n",
            "Epoch [44/50] Validation RMSE: 9.5985\n",
            "Epoch [45/50] Training Loss: 6.4597\n",
            "Epoch [45/50] Training MAE: 6.4597\n",
            "Epoch [45/50] Training RMSE: 9.6041\n",
            "Epoch [45/50] Validation Loss: 6.4632\n",
            "Epoch [45/50] Validation MAE: 6.4632\n",
            "Epoch [45/50] Validation RMSE: 9.5984\n",
            "Epoch [46/50] Training Loss: 6.4597\n",
            "Epoch [46/50] Training MAE: 6.4597\n",
            "Epoch [46/50] Training RMSE: 9.6040\n",
            "Epoch [46/50] Validation Loss: 6.4631\n",
            "Epoch [46/50] Validation MAE: 6.4631\n",
            "Epoch [46/50] Validation RMSE: 9.5982\n",
            "Epoch [47/50] Training Loss: 6.4596\n",
            "Epoch [47/50] Training MAE: 6.4596\n",
            "Epoch [47/50] Training RMSE: 9.6039\n",
            "Epoch [47/50] Validation Loss: 6.4631\n",
            "Epoch [47/50] Validation MAE: 6.4631\n",
            "Epoch [47/50] Validation RMSE: 9.5982\n",
            "Epoch [48/50] Training Loss: 6.4595\n",
            "Epoch [48/50] Training MAE: 6.4595\n",
            "Epoch [48/50] Training RMSE: 9.6038\n",
            "Epoch [48/50] Validation Loss: 6.4629\n",
            "Epoch [48/50] Validation MAE: 6.4629\n",
            "Epoch [48/50] Validation RMSE: 9.5980\n",
            "Epoch [49/50] Training Loss: 6.4595\n",
            "Epoch [49/50] Training MAE: 6.4595\n",
            "Epoch [49/50] Training RMSE: 9.6037\n",
            "Epoch [49/50] Validation Loss: 6.4629\n",
            "Epoch [49/50] Validation MAE: 6.4629\n",
            "Epoch [49/50] Validation RMSE: 9.5980\n",
            "renew lr to 1.953125e-06\n",
            "Epoch [50/50] Training Loss: 6.4595\n",
            "Epoch [50/50] Training MAE: 6.4595\n",
            "Epoch [50/50] Training RMSE: 9.6037\n",
            "Epoch [50/50] Validation Loss: 6.4630\n",
            "Epoch [50/50] Validation MAE: 6.4630\n",
            "Epoch [50/50] Validation RMSE: 9.5980\n",
            "Train over.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(out_path, \"informer.pt\"))"
      ],
      "metadata": {
        "id": "mtJf_thZxW0u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "metrics = {\n",
        "    \"Train Loss\": tot_train_losses,\n",
        "    \"Train MAE\": tot_train_mae,\n",
        "    \"Train RMSE\": tot_train_rmse,\n",
        "    \"Validation Loss\": tot_valid_losses,\n",
        "    \"Validation MAE\": tot_valid_mae,\n",
        "    \"Validation RMSE\": tot_valid_rmse\n",
        "}\n",
        "\n",
        "with open('informer_training_metrics.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)"
      ],
      "metadata": {
        "id": "roD1x5HqxW3h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(metrics)\n",
        "metrics_df.to_csv('informer_training_metrics.csv', index=False)"
      ],
      "metadata": {
        "id": "byJ3q-XP33SQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "foO8m-6uvo_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics_df = pd.read_csv('informer_training_metrics.csv')\n",
        "metrics_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xGWkhUr133bn",
        "outputId": "e4a7fbad-931e-462e-fdd7-f3602e0234df"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Train Loss  Train MAE  Train RMSE  Validation Loss  Validation MAE  \\\n",
              "0     6.482982   6.482982    9.630422         6.482982        6.482982   \n",
              "1     6.478449   6.478448    9.624160         6.478449        6.478448   \n",
              "2     6.480269   6.480269    9.627383         6.480269        6.480269   \n",
              "3     6.479428   6.479428    9.625975         6.479428        6.479428   \n",
              "4     6.476336   6.476336    9.621783         6.476336        6.476336   \n",
              "5     6.476710   6.476710    9.622218         6.476710        6.476710   \n",
              "6     6.478630   6.478630    9.623935         6.478630        6.478630   \n",
              "7     6.476684   6.476685    9.622470         6.476684        6.476685   \n",
              "8     6.475605   6.475606    9.621426         6.475605        6.475606   \n",
              "9     6.475343   6.475343    9.620632         6.475343        6.475343   \n",
              "10    6.474865   6.474864    9.620620         6.474865        6.474864   \n",
              "11    6.476096   6.476096    9.623488         6.476096        6.476096   \n",
              "12    6.472729   6.472729    9.618969         6.472729        6.472729   \n",
              "13    6.472053   6.472053    9.617699         6.472053        6.472053   \n",
              "14    6.473843   6.473843    9.619530         6.473843        6.473843   \n",
              "15    6.473490   6.473490    9.619655         6.473490        6.473490   \n",
              "16    6.470880   6.470880    9.616009         6.470880        6.470880   \n",
              "17    6.469275   6.469276    9.613617         6.469275        6.469276   \n",
              "18    6.469202   6.469202    9.612922         6.469202        6.469202   \n",
              "19    6.468104   6.468105    9.611592         6.468104        6.468105   \n",
              "20    6.466616   6.466617    9.610374         6.466616        6.466617   \n",
              "21    6.465229   6.465229    9.608401         6.465229        6.465229   \n",
              "22    6.464994   6.464994    9.608574         6.464994        6.464994   \n",
              "23    6.464488   6.464488    9.608452         6.464488        6.464488   \n",
              "24    6.465793   6.465793    9.608580         6.465793        6.465793   \n",
              "25    6.464047   6.464047    9.606394         6.464047        6.464047   \n",
              "26    6.464299   6.464300    9.608216         6.464299        6.464300   \n",
              "27    6.465654   6.465653    9.610260         6.465654        6.465653   \n",
              "28    6.463569   6.463569    9.607933         6.463569        6.463569   \n",
              "29    6.462935   6.462935    9.607358         6.462935        6.462935   \n",
              "30    6.462535   6.462535    9.606894         6.462535        6.462535   \n",
              "31    6.461447   6.461447    9.605586         6.461447        6.461447   \n",
              "32    6.460866   6.460865    9.605419         6.460866        6.460865   \n",
              "33    6.460669   6.460669    9.605061         6.460669        6.460669   \n",
              "34    6.460634   6.460634    9.604075         6.460634        6.460634   \n",
              "35    6.460246   6.460246    9.603869         6.460246        6.460246   \n",
              "36    6.459751   6.459751    9.603538         6.459751        6.459751   \n",
              "37    6.459905   6.459905    9.603889         6.459905        6.459905   \n",
              "38    6.460261   6.460261    9.604461         6.460261        6.460261   \n",
              "39    6.460083   6.460083    9.604540         6.460083        6.460083   \n",
              "40    6.459951   6.459952    9.604321         6.459951        6.459952   \n",
              "41    6.459936   6.459936    9.604386         6.459936        6.459936   \n",
              "42    6.459977   6.459978    9.604464         6.459977        6.459978   \n",
              "43    6.459865   6.459865    9.604301         6.459865        6.459865   \n",
              "44    6.459746   6.459746    9.604125         6.459746        6.459746   \n",
              "45    6.459672   6.459672    9.604029         6.459672        6.459672   \n",
              "46    6.459611   6.459611    9.603913         6.459611        6.459611   \n",
              "47    6.459496   6.459496    9.603779         6.459496        6.459496   \n",
              "48    6.459499   6.459498    9.603746         6.459499        6.459498   \n",
              "49    6.459494   6.459494    9.603714         6.459494        6.459494   \n",
              "\n",
              "    Validation RMSE  \n",
              "0          9.630422  \n",
              "1          9.624160  \n",
              "2          9.627383  \n",
              "3          9.625975  \n",
              "4          9.621783  \n",
              "5          9.622218  \n",
              "6          9.623935  \n",
              "7          9.622470  \n",
              "8          9.621426  \n",
              "9          9.620632  \n",
              "10         9.620620  \n",
              "11         9.623488  \n",
              "12         9.618969  \n",
              "13         9.617699  \n",
              "14         9.619530  \n",
              "15         9.619655  \n",
              "16         9.616009  \n",
              "17         9.613617  \n",
              "18         9.612922  \n",
              "19         9.611592  \n",
              "20         9.610374  \n",
              "21         9.608401  \n",
              "22         9.608574  \n",
              "23         9.608452  \n",
              "24         9.608580  \n",
              "25         9.606394  \n",
              "26         9.608216  \n",
              "27         9.610260  \n",
              "28         9.607933  \n",
              "29         9.607358  \n",
              "30         9.606894  \n",
              "31         9.605586  \n",
              "32         9.605419  \n",
              "33         9.605061  \n",
              "34         9.604075  \n",
              "35         9.603869  \n",
              "36         9.603538  \n",
              "37         9.603889  \n",
              "38         9.604461  \n",
              "39         9.604540  \n",
              "40         9.604321  \n",
              "41         9.604386  \n",
              "42         9.604464  \n",
              "43         9.604301  \n",
              "44         9.604125  \n",
              "45         9.604029  \n",
              "46         9.603913  \n",
              "47         9.603779  \n",
              "48         9.603746  \n",
              "49         9.603714  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e38d179-3cde-4eba-bf62-6a9ff9d40922\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Train RMSE</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Validation MAE</th>\n",
              "      <th>Validation RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.482982</td>\n",
              "      <td>6.482982</td>\n",
              "      <td>9.630422</td>\n",
              "      <td>6.482982</td>\n",
              "      <td>6.482982</td>\n",
              "      <td>9.630422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.478449</td>\n",
              "      <td>6.478448</td>\n",
              "      <td>9.624160</td>\n",
              "      <td>6.478449</td>\n",
              "      <td>6.478448</td>\n",
              "      <td>9.624160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.480269</td>\n",
              "      <td>6.480269</td>\n",
              "      <td>9.627383</td>\n",
              "      <td>6.480269</td>\n",
              "      <td>6.480269</td>\n",
              "      <td>9.627383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.479428</td>\n",
              "      <td>6.479428</td>\n",
              "      <td>9.625975</td>\n",
              "      <td>6.479428</td>\n",
              "      <td>6.479428</td>\n",
              "      <td>9.625975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.476336</td>\n",
              "      <td>6.476336</td>\n",
              "      <td>9.621783</td>\n",
              "      <td>6.476336</td>\n",
              "      <td>6.476336</td>\n",
              "      <td>9.621783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.476710</td>\n",
              "      <td>6.476710</td>\n",
              "      <td>9.622218</td>\n",
              "      <td>6.476710</td>\n",
              "      <td>6.476710</td>\n",
              "      <td>9.622218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6.478630</td>\n",
              "      <td>6.478630</td>\n",
              "      <td>9.623935</td>\n",
              "      <td>6.478630</td>\n",
              "      <td>6.478630</td>\n",
              "      <td>9.623935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6.476684</td>\n",
              "      <td>6.476685</td>\n",
              "      <td>9.622470</td>\n",
              "      <td>6.476684</td>\n",
              "      <td>6.476685</td>\n",
              "      <td>9.622470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6.475605</td>\n",
              "      <td>6.475606</td>\n",
              "      <td>9.621426</td>\n",
              "      <td>6.475605</td>\n",
              "      <td>6.475606</td>\n",
              "      <td>9.621426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6.475343</td>\n",
              "      <td>6.475343</td>\n",
              "      <td>9.620632</td>\n",
              "      <td>6.475343</td>\n",
              "      <td>6.475343</td>\n",
              "      <td>9.620632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6.474865</td>\n",
              "      <td>6.474864</td>\n",
              "      <td>9.620620</td>\n",
              "      <td>6.474865</td>\n",
              "      <td>6.474864</td>\n",
              "      <td>9.620620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6.476096</td>\n",
              "      <td>6.476096</td>\n",
              "      <td>9.623488</td>\n",
              "      <td>6.476096</td>\n",
              "      <td>6.476096</td>\n",
              "      <td>9.623488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>6.472729</td>\n",
              "      <td>6.472729</td>\n",
              "      <td>9.618969</td>\n",
              "      <td>6.472729</td>\n",
              "      <td>6.472729</td>\n",
              "      <td>9.618969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>6.472053</td>\n",
              "      <td>6.472053</td>\n",
              "      <td>9.617699</td>\n",
              "      <td>6.472053</td>\n",
              "      <td>6.472053</td>\n",
              "      <td>9.617699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>6.473843</td>\n",
              "      <td>6.473843</td>\n",
              "      <td>9.619530</td>\n",
              "      <td>6.473843</td>\n",
              "      <td>6.473843</td>\n",
              "      <td>9.619530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6.473490</td>\n",
              "      <td>6.473490</td>\n",
              "      <td>9.619655</td>\n",
              "      <td>6.473490</td>\n",
              "      <td>6.473490</td>\n",
              "      <td>9.619655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6.470880</td>\n",
              "      <td>6.470880</td>\n",
              "      <td>9.616009</td>\n",
              "      <td>6.470880</td>\n",
              "      <td>6.470880</td>\n",
              "      <td>9.616009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6.469275</td>\n",
              "      <td>6.469276</td>\n",
              "      <td>9.613617</td>\n",
              "      <td>6.469275</td>\n",
              "      <td>6.469276</td>\n",
              "      <td>9.613617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>6.469202</td>\n",
              "      <td>6.469202</td>\n",
              "      <td>9.612922</td>\n",
              "      <td>6.469202</td>\n",
              "      <td>6.469202</td>\n",
              "      <td>9.612922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>6.468104</td>\n",
              "      <td>6.468105</td>\n",
              "      <td>9.611592</td>\n",
              "      <td>6.468104</td>\n",
              "      <td>6.468105</td>\n",
              "      <td>9.611592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>6.466616</td>\n",
              "      <td>6.466617</td>\n",
              "      <td>9.610374</td>\n",
              "      <td>6.466616</td>\n",
              "      <td>6.466617</td>\n",
              "      <td>9.610374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>6.465229</td>\n",
              "      <td>6.465229</td>\n",
              "      <td>9.608401</td>\n",
              "      <td>6.465229</td>\n",
              "      <td>6.465229</td>\n",
              "      <td>9.608401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>6.464994</td>\n",
              "      <td>6.464994</td>\n",
              "      <td>9.608574</td>\n",
              "      <td>6.464994</td>\n",
              "      <td>6.464994</td>\n",
              "      <td>9.608574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>6.464488</td>\n",
              "      <td>6.464488</td>\n",
              "      <td>9.608452</td>\n",
              "      <td>6.464488</td>\n",
              "      <td>6.464488</td>\n",
              "      <td>9.608452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>6.465793</td>\n",
              "      <td>6.465793</td>\n",
              "      <td>9.608580</td>\n",
              "      <td>6.465793</td>\n",
              "      <td>6.465793</td>\n",
              "      <td>9.608580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>6.464047</td>\n",
              "      <td>6.464047</td>\n",
              "      <td>9.606394</td>\n",
              "      <td>6.464047</td>\n",
              "      <td>6.464047</td>\n",
              "      <td>9.606394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>6.464299</td>\n",
              "      <td>6.464300</td>\n",
              "      <td>9.608216</td>\n",
              "      <td>6.464299</td>\n",
              "      <td>6.464300</td>\n",
              "      <td>9.608216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>6.465654</td>\n",
              "      <td>6.465653</td>\n",
              "      <td>9.610260</td>\n",
              "      <td>6.465654</td>\n",
              "      <td>6.465653</td>\n",
              "      <td>9.610260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>6.463569</td>\n",
              "      <td>6.463569</td>\n",
              "      <td>9.607933</td>\n",
              "      <td>6.463569</td>\n",
              "      <td>6.463569</td>\n",
              "      <td>9.607933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>6.462935</td>\n",
              "      <td>6.462935</td>\n",
              "      <td>9.607358</td>\n",
              "      <td>6.462935</td>\n",
              "      <td>6.462935</td>\n",
              "      <td>9.607358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>6.462535</td>\n",
              "      <td>6.462535</td>\n",
              "      <td>9.606894</td>\n",
              "      <td>6.462535</td>\n",
              "      <td>6.462535</td>\n",
              "      <td>9.606894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>6.461447</td>\n",
              "      <td>6.461447</td>\n",
              "      <td>9.605586</td>\n",
              "      <td>6.461447</td>\n",
              "      <td>6.461447</td>\n",
              "      <td>9.605586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>6.460866</td>\n",
              "      <td>6.460865</td>\n",
              "      <td>9.605419</td>\n",
              "      <td>6.460866</td>\n",
              "      <td>6.460865</td>\n",
              "      <td>9.605419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>6.460669</td>\n",
              "      <td>6.460669</td>\n",
              "      <td>9.605061</td>\n",
              "      <td>6.460669</td>\n",
              "      <td>6.460669</td>\n",
              "      <td>9.605061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>6.460634</td>\n",
              "      <td>6.460634</td>\n",
              "      <td>9.604075</td>\n",
              "      <td>6.460634</td>\n",
              "      <td>6.460634</td>\n",
              "      <td>9.604075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>6.460246</td>\n",
              "      <td>6.460246</td>\n",
              "      <td>9.603869</td>\n",
              "      <td>6.460246</td>\n",
              "      <td>6.460246</td>\n",
              "      <td>9.603869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>6.459751</td>\n",
              "      <td>6.459751</td>\n",
              "      <td>9.603538</td>\n",
              "      <td>6.459751</td>\n",
              "      <td>6.459751</td>\n",
              "      <td>9.603538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>6.459905</td>\n",
              "      <td>6.459905</td>\n",
              "      <td>9.603889</td>\n",
              "      <td>6.459905</td>\n",
              "      <td>6.459905</td>\n",
              "      <td>9.603889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>6.460261</td>\n",
              "      <td>6.460261</td>\n",
              "      <td>9.604461</td>\n",
              "      <td>6.460261</td>\n",
              "      <td>6.460261</td>\n",
              "      <td>9.604461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>6.460083</td>\n",
              "      <td>6.460083</td>\n",
              "      <td>9.604540</td>\n",
              "      <td>6.460083</td>\n",
              "      <td>6.460083</td>\n",
              "      <td>9.604540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>6.459951</td>\n",
              "      <td>6.459952</td>\n",
              "      <td>9.604321</td>\n",
              "      <td>6.459951</td>\n",
              "      <td>6.459952</td>\n",
              "      <td>9.604321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>6.459936</td>\n",
              "      <td>6.459936</td>\n",
              "      <td>9.604386</td>\n",
              "      <td>6.459936</td>\n",
              "      <td>6.459936</td>\n",
              "      <td>9.604386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>6.459977</td>\n",
              "      <td>6.459978</td>\n",
              "      <td>9.604464</td>\n",
              "      <td>6.459977</td>\n",
              "      <td>6.459978</td>\n",
              "      <td>9.604464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>6.459865</td>\n",
              "      <td>6.459865</td>\n",
              "      <td>9.604301</td>\n",
              "      <td>6.459865</td>\n",
              "      <td>6.459865</td>\n",
              "      <td>9.604301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>6.459746</td>\n",
              "      <td>6.459746</td>\n",
              "      <td>9.604125</td>\n",
              "      <td>6.459746</td>\n",
              "      <td>6.459746</td>\n",
              "      <td>9.604125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>6.459672</td>\n",
              "      <td>6.459672</td>\n",
              "      <td>9.604029</td>\n",
              "      <td>6.459672</td>\n",
              "      <td>6.459672</td>\n",
              "      <td>9.604029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>6.459611</td>\n",
              "      <td>6.459611</td>\n",
              "      <td>9.603913</td>\n",
              "      <td>6.459611</td>\n",
              "      <td>6.459611</td>\n",
              "      <td>9.603913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>6.459496</td>\n",
              "      <td>6.459496</td>\n",
              "      <td>9.603779</td>\n",
              "      <td>6.459496</td>\n",
              "      <td>6.459496</td>\n",
              "      <td>9.603779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>6.459499</td>\n",
              "      <td>6.459498</td>\n",
              "      <td>9.603746</td>\n",
              "      <td>6.459499</td>\n",
              "      <td>6.459498</td>\n",
              "      <td>9.603746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>6.459494</td>\n",
              "      <td>6.459494</td>\n",
              "      <td>9.603714</td>\n",
              "      <td>6.459494</td>\n",
              "      <td>6.459494</td>\n",
              "      <td>9.603714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e38d179-3cde-4eba-bf62-6a9ff9d40922')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e38d179-3cde-4eba-bf62-6a9ff9d40922 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e38d179-3cde-4eba-bf62-6a9ff9d40922');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-38c46831-794c-41ea-93e8-ddae05b08d9e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38c46831-794c-41ea-93e8-ddae05b08d9e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-38c46831-794c-41ea-93e8-ddae05b08d9e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_724b05a5-ac2a-4933-819c-6f43984c6d87\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_724b05a5-ac2a-4933-819c-6f43984c6d87 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"Train Loss\": [6.482982, 6.478449, 6.480269, 6.479428, 6.476336, 6.476710, 6.478630, 6.476684, 6.475605, 6.475343, 6.474865, 6.476096, 6.472729, 6.472053, 6.473843, 6.473490, 6.470880, 6.469275, 6.469202, 6.468104, 6.466616, 6.465229, 6.464994, 6.464488, 6.465793, 6.464047, 6.464299, 6.465654, 6.463569, 6.462935, 6.462535, 6.461447, 6.460866, 6.460669, 6.460634, 6.460246, 6.459751, 6.459905, 6.460261, 6.460083, 6.459951, 6.459936, 6.459977, 6.459865, 6.459746, 6.459672, 6.459611, 6.459496, 6.459499, 6.459494],\n",
        "    \"Train MAE\": [6.482982, 6.478448, 6.480269, 6.479428, 6.476336, 6.476710, 6.478630, 6.476685, 6.475606, 6.475343, 6.474864, 6.476096, 6.472729, 6.472053, 6.473843, 6.473490, 6.470880, 6.469276, 6.469202, 6.468105, 6.466617, 6.465229, 6.464994, 6.464488, 6.465793, 6.464047, 6.464300, 6.465653, 6.463569, 6.462935, 6.462535, 6.461447, 6.460865, 6.460669, 6.460634, 6.460246, 6.459751, 6.459905, 6.460261, 6.460083, 6.459952, 6.459936, 6.459978, 6.459865, 6.459746, 6.459672, 6.459611, 6.459496, 6.459498, 6.459494],\n",
        "    \"Train RMSE\": [9.630422, 9.624160, 9.627383, 9.625975, 9.621783, 9.622218, 9.623935, 9.622470, 9.621426, 9.620632, 9.620620, 9.623488, 9.618969, 9.617699, 9.619530, 9.619655, 9.616009, 9.613617, 9.612922, 9.611592, 9.610374, 9.608401, 9.608574, 9.608452, 9.608580, 9.606394, 9.608216, 9.610260, 9.607933, 9.607358, 9.606894, 9.605586, 9.605419, 9.605061, 9.604075, 9.603869, 9.603538, 9.603889, 9.604461, 9.604540, 9.604321, 9.604386, 9.604464, 9.604301, 9.604125, 9.604029, 9.603913, 9.603779, 9.603746, 9.603714],\n",
        "    \"Validation Loss\": [6.482982, 6.478449, 6.480269, 6.479428, 6.476336, 6.476710, 6.478630, 6.476684, 6.475605, 6.475343, 6.474865, 6.476096, 6.472729, 6.472053, 6.473843, 6.473490, 6.470880, 6.469275, 6.469202, 6.468104, 6.466616, 6.465229, 6.464994, 6.464488, 6.465793, 6.464047, 6.464299, 6.465654, 6.463569, 6.462935, 6.462535, 6.461447, 6.460866, 6.460669, 6.460634, 6.460246, 6.459751, 6.459905, 6.460261, 6.460083, 6.459951, 6.459936, 6.459977, 6.459865, 6.459746, 6.459672, 6.459611, 6.459496, 6.459499, 6.459494],\n",
        "    \"Validation MAE\": [6.482982, 6.478448, 6.480269, 6.479428, 6.476336, 6.476710, 6.478630, 6.476685, 6.475606, 6.475343, 6.474864, 6.476096, 6.472729, 6.472053, 6.473843, 6.473490, 6.470880, 6.469276, 6.469202, 6.468105, 6.466617, 6.465229, 6.464994, 6.464488, 6.465793, 6.464047, 6.464300, 6.465653, 6.463569, 6.462935, 6.462535, 6.461447, 6.460865, 6.460669, 6.460634, 6.460246, 6.459751, 6.459905, 6.460261, 6.460083, 6.459952, 6.459936, 6.459978, 6.459865, 6.459746, 6.459672, 6.459611, 6.459496, 6.459498, 6.459494],\n",
        "    \"Validation RMSE\": [9.630422, 9.624160, 9.627383, 9.625975, 9.621783, 9.622218, 9.623935, 9.622470, 9.621426, 9.620632, 9.620620, 9.623488, 9.618969, 9.617699, 9.619530, 9.619655, 9.616009, 9.613617, 9.612922, 9.611592, 9.610374, 9.608401, 9.608574, 9.608452, 9.608580, 9.606394, 9.608216, 9.610260, 9.607933, 9.607358, 9.606894, 9.605586, 9.605419, 9.605061, 9.604075, 9.603869, 9.603538, 9.603889, 9.604461, 9.604540, 9.604321, 9.604386, 9.604464, 9.604301, 9.604125, 9.604029, 9.603913, 9.603779, 9.603746, 9.603714]\n",
        "}"
      ],
      "metadata": {
        "id": "NELRB5KU33jz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.min()"
      ],
      "metadata": {
        "id": "Rhs_emau33qc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027bd368-aee0-44ad-acbb-2c52887104f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Train Loss         6.459494\n",
              "Train MAE          6.459494\n",
              "Train RMSE         9.603538\n",
              "Validation Loss    6.459494\n",
              "Validation MAE     6.459494\n",
              "Validation RMSE    9.603538\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6eRK32ZxW6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}