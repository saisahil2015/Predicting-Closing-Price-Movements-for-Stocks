{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfhAobi67rXd"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from itertools import combinations\n",
        "from warnings import simplefilter\n",
        "\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYMWZdFB73-U",
        "outputId": "769c947c-b3e4-4529-e318-c48c63e84385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\" # https://drive.google.com/drive/folders/18KshTFZ6gGQMeqUf5N6FZq2H1P9fNYlB?usp=sharing"
      ],
      "metadata": {
        "id": "eVXBVL-i8AaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download optiver-trading-at-the-close\n",
        "!unzip optiver-trading-at-the-close.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8WZo_nJ9ZfH",
        "outputId": "1b45c9ec-502b-4fbb-9bda-5f5c2d5477fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading optiver-trading-at-the-close.zip to /content\n",
            " 97% 194M/201M [00:02<00:00, 78.6MB/s]\n",
            "100% 201M/201M [00:02<00:00, 87.0MB/s]\n",
            "Archive:  optiver-trading-at-the-close.zip\n",
            "  inflating: example_test_files/revealed_targets.csv  \n",
            "  inflating: example_test_files/sample_submission.csv  \n",
            "  inflating: example_test_files/test.csv  \n",
            "  inflating: optiver2023/__init__.py  \n",
            "  inflating: optiver2023/competition.cpython-310-x86_64-linux-gnu.so  \n",
            "  inflating: public_timeseries_testing_util.py  \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/train.csv\")"
      ],
      "metadata": {
        "id": "Hl0TLNKM9a1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df, verbose=0):\n",
        "    \"\"\"\n",
        "    Iterate through all numeric columns of a dataframe and modify the data type\n",
        "    to reduce memory usage.\n",
        "    \"\"\"\n",
        "\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "\n",
        "    if verbose:\n",
        "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
        "        end_mem = df.memory_usage().sum() / 1024**2\n",
        "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
        "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
        "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "from numba import njit, prange\n",
        "\n",
        "@njit(parallel=True)\n",
        "def compute_triplet_imbalance(df_values, comb_indices):\n",
        "    num_rows = df_values.shape[0]\n",
        "    num_combinations = len(comb_indices)\n",
        "    imbalance_features = np.empty((num_rows, num_combinations))\n",
        "\n",
        "    for i in prange(num_combinations):\n",
        "        a, b, c = comb_indices[i]\n",
        "        for j in range(num_rows):\n",
        "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
        "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
        "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
        "            if mid_val == min_val:  # Prevent division by zero\n",
        "                imbalance_features[j, i] = np.nan\n",
        "            else:\n",
        "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
        "\n",
        "    return imbalance_features\n",
        "\n",
        "\n",
        "def calculate_triplet_imbalance_numba(price, df):\n",
        "    # Convert DataFrame to numpy array for Numba compatibility\n",
        "    df_values = df[price].values\n",
        "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
        "\n",
        "    # Calculate the triplet imbalance\n",
        "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
        "\n",
        "    # Create a DataFrame from the results\n",
        "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
        "    features = pd.DataFrame(features_array, columns=columns)\n",
        "\n",
        "    return features\n",
        "\n",
        "# generate imbalance features\n",
        "def imbalance_features(df):\n",
        "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
        "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
        "\n",
        "    # V1\n",
        "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
        "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
        "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
        "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
        "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
        "\n",
        "    for c in combinations(prices, 2):\n",
        "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
        "\n",
        "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
        "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
        "        df[triplet_feature.columns] = triplet_feature.values\n",
        "\n",
        "    # V2\n",
        "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
        "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
        "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
        "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
        "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
        "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
        "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
        "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
        "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
        "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
        "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
        "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
        "\n",
        "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
        "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
        "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
        "\n",
        "    # V3\n",
        "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
        "        for window in [1, 2, 3, 5, 10]:\n",
        "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
        "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
        "\n",
        "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n",
        "                'wap', 'near_price', 'far_price']:\n",
        "        for window in [1, 2, 3, 5, 10]:\n",
        "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
        "\n",
        "    return df.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# generate time & stock features\n",
        "def other_features(df):\n",
        "    df[\"dow\"] = df[\"date_id\"] % 5\n",
        "    df[\"dom\"] = df[\"date_id\"] % 20\n",
        "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
        "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n",
        "\n",
        "    for key, value in global_stock_id_feats.items():\n",
        "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
        "\n",
        "    return df\n",
        "\n",
        "# generate all features\n",
        "def generate_all_features(df):\n",
        "    #cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
        "    #df = df[cols]\n",
        "    df = imbalance_features(df)\n",
        "    df = other_features(df)\n",
        "    gc.collect()\n",
        "\n",
        "    #feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
        "    return df\n",
        "    #return df[feature_name]\n",
        "\n",
        "weights = [\n",
        "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
        "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
        "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
        "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04, 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
        "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
        "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
        "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
        "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
        "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
        "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
        "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
        "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
        "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
        "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
        "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
        "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
        "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
        "]\n",
        "\n",
        "weights = {int(k):v for k,v in enumerate(weights)}"
      ],
      "metadata": {
        "id": "_4fOPXCJ9dxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_stock_id_feats = {\n",
        "        \"median_size\": df.groupby(\"stock_id\")[\"bid_size\"].median() + df.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
        "        \"std_size\": df.groupby(\"stock_id\")[\"bid_size\"].std() + df.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
        "        \"ptp_size\": df.groupby(\"stock_id\")[\"bid_size\"].max() - df.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
        "        \"median_price\": df.groupby(\"stock_id\")[\"bid_price\"].median() + df.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
        "        \"std_price\": df.groupby(\"stock_id\")[\"bid_price\"].std() + df.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
        "        \"ptp_price\": df.groupby(\"stock_id\")[\"bid_price\"].max() - df.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
        "  }"
      ],
      "metadata": {
        "id": "DCpn7rzCoMBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_day = 435\n",
        "# df_train = df[df[\"date_id\"] <= split_day]\n",
        "# df_valid = df[df[\"date_id\"] > split_day]\n",
        "\n",
        "# global_stock_id_feats = {\n",
        "#         \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
        "#         \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
        "#         \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
        "#         \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
        "#         \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
        "#         \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
        "#   }\n"
      ],
      "metadata": {
        "id": "GZSEDajaVYfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train = df\n",
        "# df_train_feats = generate_all_features(df_train)\n",
        "# df_train_feats = reduce_mem_usage(df_train_feats)\n",
        "\n",
        "# offline_split = df_train['date_id']>(split_day - 45)\n",
        "# df_offline_train = df_train_feats[~offline_split]\n",
        "# df_offline_valid = df_train_feats[offline_split]\n",
        "# df_offline_train_target = df_train['target'][~offline_split]\n",
        "# df_offline_valid_target = df_train['target'][offline_split]\n",
        "\n",
        "# # Check and remove NaN values\n",
        "# df_offline_train_target = df_offline_train_target.dropna()\n",
        "# df_offline_valid_target = df_offline_valid_target.dropna()\n",
        "\n",
        "# # Check and remove infinite values, if any\n",
        "# df_offline_train_target.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "# df_offline_valid_target.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# df_offline_train_target.dropna(inplace=True)\n",
        "# df_offline_valid_target.dropna(inplace=True)\n",
        "\n",
        "# df_offline_train = df_offline_train.loc[df_offline_train_target.index]\n",
        "# df_offline_valid = df_offline_valid.loc[df_offline_valid_target.index]\n",
        "\n",
        "# # Function to remove NaN and Inf values from DataFrame\n",
        "# def remove_nan_inf(df):\n",
        "#     df.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace Inf with NaN\n",
        "#     df.dropna(inplace=True)  # Drop all NaNs\n",
        "#     return df\n",
        "\n",
        "# # Clean the data for both features and targets\n",
        "# df_offline_train = remove_nan_inf(df_offline_train)\n",
        "# df_offline_valid = remove_nan_inf(df_offline_valid)\n",
        "# df_offline_train_target = remove_nan_inf(df_offline_train_target)\n",
        "# df_offline_valid_target = remove_nan_inf(df_offline_valid_target)\n",
        "\n",
        "# # Ensure the indices of features and targets match\n",
        "# df_offline_train_target = df_offline_train_target.loc[df_offline_train.index]\n",
        "# df_offline_valid_target = df_offline_valid_target.loc[df_offline_valid.index]\n",
        "# df_offline_train = df_offline_train.reindex(df_offline_train_target.index)\n",
        "# df_offline_valid = df_offline_valid.reindex(df_offline_valid_target.index)\n",
        "\n",
        "# X_train = torch.tensor(df_offline_train.values, dtype=torch.float32)\n",
        "# y_train = torch.tensor(df_offline_train_target.values, dtype=torch.float32)\n",
        "# X_val = torch.tensor(df_offline_valid.values, dtype=torch.float32)\n",
        "# y_val = torch.tensor(df_offline_valid_target.values, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "a5YtG4PUaFvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imputer(df):\n",
        "    far_price_mean = df['far_price'].mean()\n",
        "    near_price_mean = df['near_price'].mean()\n",
        "    df['far_price'] = df['far_price'].fillna(far_price_mean)\n",
        "    df['near_price'] = df['near_price'].fillna(near_price_mean)\n",
        "\n",
        "    return df, far_price_mean, near_price_mean\n",
        "\n",
        "def add_missing_data(df):\n",
        "    all_stock_ids = set(range(200))\n",
        "    all_missed_data_list = []\n",
        "\n",
        "    grouped = df.groupby('time_id')\n",
        "\n",
        "    for t, group in grouped:\n",
        "        current_stock_ids = set(group['stock_id'].to_list())\n",
        "        missed_stock_id = list(all_stock_ids - current_stock_ids)\n",
        "\n",
        "        date_id = group['date_id'].iloc[-1]\n",
        "        seconds_in_bucket = group['seconds_in_bucket'].iloc[-1]\n",
        "\n",
        "        missed_stock_id_num = len(missed_stock_id)\n",
        "        missed_date_id = [date_id] * missed_stock_id_num\n",
        "        missed_seconds_in_bucket = [seconds_in_bucket] * missed_stock_id_num\n",
        "        missed_time_id = [t] * missed_stock_id_num\n",
        "\n",
        "        missed_data = pd.DataFrame({\n",
        "            'stock_id': missed_stock_id,\n",
        "            'date_id': missed_date_id,\n",
        "            'seconds_in_bucket': missed_seconds_in_bucket,\n",
        "            'time_id': missed_time_id\n",
        "        })\n",
        "\n",
        "        all_missed_data_list.append(missed_data)\n",
        "\n",
        "    all_missed_data = pd.concat(all_missed_data_list, axis=0).reset_index(drop=True).astype(int)\n",
        "\n",
        "    df = pd.concat([df, all_missed_data], axis=0)\n",
        "    df = df.sort_values(by=['time_id', 'stock_id']).reset_index(drop=True)\n",
        "    df = df.groupby('stock_id').apply(lambda x: x.fillna(method='bfill')).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "train, far_price_mean, near_price_mean = imputer(df)\n",
        "train = add_missing_data(df)\n",
        "\n",
        "def sizesum_and_pricestd(df):\n",
        "    price_ftrs = ['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'wap'] # std\n",
        "    size_ftrs = ['imbalance_size', 'matched_size', 'bid_size', 'ask_size'] # sum\n",
        "\n",
        "    rolled = df[['stock_id'] + size_ftrs].groupby('stock_id').rolling(window=6, min_periods=1).sum()\n",
        "    rolled = rolled.reset_index(level=0, drop=True)\n",
        "    for col in size_ftrs:\n",
        "        df[f'{col}_rolled_sum'] = rolled[col]\n",
        "\n",
        "    rolled = df[['stock_id'] + price_ftrs].groupby('stock_id').rolling(window=6, min_periods=1).std().fillna(0)\n",
        "    rolled = rolled.reset_index(level=0, drop=True)\n",
        "    for col in price_ftrs:\n",
        "        df[f'{col}_rolled_std'] = rolled[col]\n",
        "\n",
        "    return df\n",
        "\n",
        "train = sizesum_and_pricestd(train)\n",
        "\n",
        "def remove_element(input_list, drop_list):\n",
        "    return [e for e in input_list if e not in drop_list]\n",
        "\n",
        "no_feature_cols = ['date_id', 'row_id', 'time_id', 'target', 'currently_scored']\n",
        "\n",
        "feature_cols = remove_element(train.columns, no_feature_cols)\n",
        "target_col = 'target'"
      ],
      "metadata": {
        "id": "x0xZYoJ6tnOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg = train[feature_cols].mean()\n",
        "std = train[feature_cols].std()\n",
        "\n",
        "train[feature_cols] = (train[feature_cols] - avg)/std"
      ],
      "metadata": {
        "id": "GJRqvLW3wXls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.astype('float32')\n",
        "train = reduce_mem_usage(train)\n",
        "\n",
        "seq_len = 16\n",
        "\n",
        "# Grouping by time_id\n",
        "grouped_by_time = train.groupby('stock_id')\n",
        "\n",
        "def generate_data(grouped_by_time, seq_len):\n",
        "    for _, group in grouped_by_time:\n",
        "        # Sorting by stock_id to maintain consistency across images\n",
        "        group_sorted = group.sort_values(by='time_id')\n",
        "\n",
        "        features = group_sorted[feature_cols].values\n",
        "\n",
        "        windows = []\n",
        "\n",
        "        for t in range(0, seq_len - 1):\n",
        "            copy_0 = np.stack([features[0]] * (seq_len - 1 - t))\n",
        "            cut_0 = features[: t + 1]\n",
        "            windows.append(np.vstack((copy_0, cut_0)))\n",
        "\n",
        "        for t in range(0, features.shape[0] - seq_len + 1):\n",
        "            windows.append(features[t: t+seq_len, :])\n",
        "\n",
        "        # Convert list of windows to numpy array\n",
        "        features_array = np.stack(windows)\n",
        "\n",
        "        target = group_sorted['target'].values\n",
        "\n",
        "        # Yield the result for this group to avoid storing all results in memory\n",
        "        yield features_array, target\n",
        "\n",
        "# Use generator to iterate over data\n",
        "data_generator = generate_data(grouped_by_time, seq_len=seq_len)\n",
        "\n",
        "# If you need to store results in arrays:\n",
        "datas, labels = zip(*data_generator)\n",
        "data = np.array(datas).reshape(-1, seq_len, len(feature_cols))\n",
        "label = np.array(labels).reshape(-1,)"
      ],
      "metadata": {
        "id": "2QQUGWXKvxka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "\n",
        "data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "label = torch.tensor(label, dtype=torch.float32).to(device)\n",
        "\n",
        "dataset = TensorDataset(data, label)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "7M9nOwKAwq8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "9mS2zcY_5Vxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# class MyModel(nn.Module):\n",
        "#     def __init__(self, feature_num, d_model, nhead, num_layers):\n",
        "#         super(MyModel, self).__init__()\n",
        "#         self.embedding = nn.Linear(feature_num, d_model)\n",
        "#         self.tf1 = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(d_model, d_model)\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "#         self.tf2 = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, batch_first=True)\n",
        "#         self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x = self.tf1.encoder(x)\n",
        "#         x = x[:, -1, :]\n",
        "#         x = self.fc(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.tf2.encoder(x)\n",
        "#         x = self.decoder(x)\n",
        "\n",
        "#         return x"
      ],
      "metadata": {
        "id": "oUUusnbiyJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TrendExtractor(nn.Module):\n",
        "#     \"\"\"\n",
        "#     A simplified trend extractor. In a real Autoformer, this would be more complex,\n",
        "#     possibly using a decomposition method.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, input_dim, hidden_dim):\n",
        "#         super(TrendExtractor, self).__init__()\n",
        "#         self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.linear(x)\n",
        "\n",
        "# class SimplifiedAutoformerLayer(nn.Module):\n",
        "#     \"\"\"\n",
        "#     A simplified version of an Autoformer layer. In the actual Autoformer,\n",
        "#     this would include an efficient attention mechanism and other components\n",
        "#     specific to time series data.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, d_model, nhead):\n",
        "#         super(SimplifiedAutoformerLayer, self).__init__()\n",
        "#         self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
        "#         self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         attn_output, _ = self.self_attn(x, x, x)\n",
        "#         return F.relu(self.linear(attn_output))\n",
        "\n",
        "# class MyModel(nn.Module):\n",
        "#     def __init__(self, feature_num, d_model, nhead, num_layers):\n",
        "#         super(MyModel, self).__init__()\n",
        "#         self.embedding = nn.Linear(feature_num, d_model)\n",
        "\n",
        "#         # Trend extraction layer\n",
        "#         self.trend_extractor = TrendExtractor(d_model, d_model)\n",
        "\n",
        "#         # Autoformer layers\n",
        "#         self.autoformer_layers = nn.ModuleList([SimplifiedAutoformerLayer(d_model, nhead) for _ in range(num_layers)])\n",
        "\n",
        "#         self.fc = nn.Linear(d_model, d_model)\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "#         # Final decoder layer\n",
        "#         self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "\n",
        "#         # Trend extraction\n",
        "#         trend = self.trend_extractor(x)\n",
        "\n",
        "#         # Autoformer layers\n",
        "#         for layer in self.autoformer_layers:\n",
        "#             x = layer(x)\n",
        "\n",
        "#         x = x + trend  # Combining with the trend component\n",
        "#         x = x[:, -1, :]  # Assuming we need the last timestep\n",
        "#         x = self.fc(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.decoder(x)\n",
        "\n",
        "#         return x"
      ],
      "metadata": {
        "id": "uQche4e22tWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LongRangeTransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        super(LongRangeTransformerLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
        "        self.linear1 = nn.Linear(d_model, d_model)\n",
        "        self.linear2 = nn.Linear(d_model, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.self_attn(x, x, x)[0])\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = x + self.dropout(F.relu(self.linear1(x)))\n",
        "        x = self.norm2(self.linear2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, feature_num, d_model, nhead, num_layers):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = nn.Linear(feature_num, d_model)\n",
        "\n",
        "        self.long_range_transformer_layers = nn.ModuleList([LongRangeTransformerLayer(d_model, nhead) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        for layer in self.long_range_transformer_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_jG88p0g4ARL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\n",
        "import math\n",
        "\n",
        "tot_train_losses = []\n",
        "tot_train_mae = []\n",
        "tot_train_rmse = []\n",
        "tot_train_rmsle = []\n",
        "\n",
        "tot_valid_losses = []\n",
        "tot_valid_mae = []\n",
        "tot_valid_rmse = []\n",
        "tot_valid_rmsle = []\n",
        "\n",
        "is_train = True\n",
        "if is_train:\n",
        "    input_size = data.shape[-1]\n",
        "\n",
        "    n_epochs = 50\n",
        "    lr = 1e-03\n",
        "\n",
        "    pre_epoch_valid_mae = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    model = MyModel(feature_num=input_size, d_model=64, nhead=2, num_layers=1).to(device)\n",
        "    #model = MyModel(feature_num=input_size, d_model=64, nhead=2, num_layers=1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    loss = nn.L1Loss().to(device)\n",
        "    #loss = nn.L1Loss()\n",
        "\n",
        "    out_path = \"model/\"\n",
        "    if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "    best_mae = np.inf\n",
        "\n",
        "    print(f'Train start...')\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_mae = []\n",
        "        train_rmse = []\n",
        "        batch_num = len(train_loader)\n",
        "\n",
        "        # Training\n",
        "        for X, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X).squeeze()\n",
        "            l = loss(outputs, y)\n",
        "            l.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "            train_losses.append(l.item())\n",
        "\n",
        "            # Calculate MAE\n",
        "            mae = mean_absolute_error(y.cpu().numpy(), outputs.detach().cpu().numpy())\n",
        "            train_mae.append(mae)\n",
        "\n",
        "            # Calculate RMSE\n",
        "            rmse = math.sqrt(mean_squared_error(y.cpu().numpy(), outputs.detach().cpu().numpy()))\n",
        "            train_rmse.append(rmse)\n",
        "\n",
        "        epoch_train_loss = np.mean(train_losses)\n",
        "        epoch_train_mae = np.mean(train_mae)\n",
        "        epoch_train_rmse = np.mean(train_rmse)\n",
        "\n",
        "        tot_train_losses.append(epoch_train_loss)\n",
        "        tot_train_mae.append(epoch_train_mae)\n",
        "        tot_train_rmse.append(epoch_train_rmse)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}] Training Loss: {epoch_train_loss:.4f}')\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}] Training MAE: {epoch_train_mae:.4f}')\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}] Training RMSE: {epoch_train_rmse:.4f}')\n",
        "\n",
        "        train_maes = []\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_losses = []\n",
        "            valid_maes = []\n",
        "            valid_rmse = []\n",
        "            valid_rmsle = []\n",
        "\n",
        "            for X_v, y_v in valid_loader:\n",
        "                preds = model(X_v).squeeze()\n",
        "                valid_loss = loss(preds, y_v)\n",
        "                valid_losses.append(valid_loss.item())\n",
        "\n",
        "                # Calculate MAE\n",
        "                valid_mae = mean_absolute_error(y_v.cpu().numpy(), preds.cpu().numpy())\n",
        "                valid_maes.append(valid_mae)\n",
        "\n",
        "                # Calculate RMSE\n",
        "                valid_rmse_val = math.sqrt(mean_squared_error(y_v.cpu().numpy(), preds.cpu().numpy()))\n",
        "                valid_rmse.append(valid_rmse_val)\n",
        "\n",
        "            epoch_valid_loss = np.mean(valid_losses)\n",
        "            epoch_valid_mae = np.mean(valid_maes)\n",
        "            epoch_valid_rmse = np.mean(valid_rmse)\n",
        "\n",
        "            tot_valid_losses.append(epoch_train_loss)\n",
        "            tot_valid_mae.append(epoch_train_mae)\n",
        "            tot_valid_rmse.append(epoch_train_rmse)\n",
        "\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}] Validation Loss: {epoch_valid_loss:.4f}')\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}] Validation MAE: {epoch_valid_mae:.4f}')\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}] Validation RMSE: {epoch_valid_rmse:.4f}')\n",
        "\n",
        "\n",
        "            if epoch_valid_mae < best_mae:\n",
        "                best_mae = epoch_valid_mae\n",
        "                torch.save(model, os.path.join(out_path, f\"model_epoch_{epoch+1}.pt\"))\n",
        "\n",
        "        if epoch_valid_mae - pre_epoch_valid_mae > 0:\n",
        "            patience_counter += 1\n",
        "\n",
        "            if patience_counter == 2:\n",
        "                lr = lr * 0.5\n",
        "                patience_counter = 0\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "                    print(f'renew lr to {lr}')\n",
        "\n",
        "        pre_epoch_valid_mae = epoch_valid_mae\n",
        "\n",
        "        if (epoch_valid_mae - epoch_train_mae > 0.03) or (lr <1e-7):\n",
        "            print('Early stop now.')\n",
        "            break\n",
        "\n",
        "    print(f'Train over.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdqZ1qPoxWxu",
        "outputId": "c618d92d-4bd6-4180-9d83-03ffc6815def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train start...\n",
            "Epoch [1/50] Training Loss: 6.3510\n",
            "Epoch [1/50] Training MAE: 6.3510\n",
            "Epoch [1/50] Training RMSE: 9.3998\n",
            "Epoch [1/50] Validation Loss: 6.2801\n",
            "Epoch [1/50] Validation MAE: 6.2801\n",
            "Epoch [1/50] Validation RMSE: 9.2793\n",
            "Epoch [2/50] Training Loss: 6.2812\n",
            "Epoch [2/50] Training MAE: 6.2812\n",
            "Epoch [2/50] Training RMSE: 9.2843\n",
            "Epoch [2/50] Validation Loss: 6.2655\n",
            "Epoch [2/50] Validation MAE: 6.2655\n",
            "Epoch [2/50] Validation RMSE: 9.2652\n",
            "Epoch [3/50] Training Loss: 6.2736\n",
            "Epoch [3/50] Training MAE: 6.2736\n",
            "Epoch [3/50] Training RMSE: 9.2776\n",
            "Epoch [3/50] Validation Loss: 6.2593\n",
            "Epoch [3/50] Validation MAE: 6.2593\n",
            "Epoch [3/50] Validation RMSE: 9.2621\n",
            "Epoch [4/50] Training Loss: 6.2707\n",
            "Epoch [4/50] Training MAE: 6.2707\n",
            "Epoch [4/50] Training RMSE: 9.2747\n",
            "Epoch [4/50] Validation Loss: 6.2550\n",
            "Epoch [4/50] Validation MAE: 6.2550\n",
            "Epoch [4/50] Validation RMSE: 9.2592\n",
            "Epoch [5/50] Training Loss: 6.2661\n",
            "Epoch [5/50] Training MAE: 6.2661\n",
            "Epoch [5/50] Training RMSE: 9.2702\n",
            "Epoch [5/50] Validation Loss: 6.2511\n",
            "Epoch [5/50] Validation MAE: 6.2511\n",
            "Epoch [5/50] Validation RMSE: 9.2540\n",
            "Epoch [6/50] Training Loss: 6.2634\n",
            "Epoch [6/50] Training MAE: 6.2634\n",
            "Epoch [6/50] Training RMSE: 9.2675\n",
            "Epoch [6/50] Validation Loss: 6.2468\n",
            "Epoch [6/50] Validation MAE: 6.2468\n",
            "Epoch [6/50] Validation RMSE: 9.2497\n",
            "Epoch [7/50] Training Loss: 6.2592\n",
            "Epoch [7/50] Training MAE: 6.2592\n",
            "Epoch [7/50] Training RMSE: 9.2633\n",
            "Epoch [7/50] Validation Loss: 6.2474\n",
            "Epoch [7/50] Validation MAE: 6.2474\n",
            "Epoch [7/50] Validation RMSE: 9.2488\n",
            "Epoch [8/50] Training Loss: 6.2567\n",
            "Epoch [8/50] Training MAE: 6.2567\n",
            "Epoch [8/50] Training RMSE: 9.2607\n",
            "Epoch [8/50] Validation Loss: 6.2456\n",
            "Epoch [8/50] Validation MAE: 6.2456\n",
            "Epoch [8/50] Validation RMSE: 9.2461\n",
            "Epoch [9/50] Training Loss: 6.2554\n",
            "Epoch [9/50] Training MAE: 6.2554\n",
            "Epoch [9/50] Training RMSE: 9.2589\n",
            "Epoch [9/50] Validation Loss: 6.2399\n",
            "Epoch [9/50] Validation MAE: 6.2399\n",
            "Epoch [9/50] Validation RMSE: 9.2442\n",
            "Epoch [10/50] Training Loss: 6.2539\n",
            "Epoch [10/50] Training MAE: 6.2539\n",
            "Epoch [10/50] Training RMSE: 9.2570\n",
            "Epoch [10/50] Validation Loss: 6.2406\n",
            "Epoch [10/50] Validation MAE: 6.2406\n",
            "Epoch [10/50] Validation RMSE: 9.2438\n",
            "renew lr to 0.0005\n",
            "Epoch [11/50] Training Loss: 6.2479\n",
            "Epoch [11/50] Training MAE: 6.2479\n",
            "Epoch [11/50] Training RMSE: 9.2512\n",
            "Epoch [11/50] Validation Loss: 6.2378\n",
            "Epoch [11/50] Validation MAE: 6.2378\n",
            "Epoch [11/50] Validation RMSE: 9.2375\n",
            "Epoch [12/50] Training Loss: 6.2473\n",
            "Epoch [12/50] Training MAE: 6.2473\n",
            "Epoch [12/50] Training RMSE: 9.2498\n",
            "Epoch [12/50] Validation Loss: 6.2361\n",
            "Epoch [12/50] Validation MAE: 6.2361\n",
            "Epoch [12/50] Validation RMSE: 9.2363\n",
            "Epoch [13/50] Training Loss: 6.2461\n",
            "Epoch [13/50] Training MAE: 6.2461\n",
            "Epoch [13/50] Training RMSE: 9.2482\n",
            "Epoch [13/50] Validation Loss: 6.2333\n",
            "Epoch [13/50] Validation MAE: 6.2333\n",
            "Epoch [13/50] Validation RMSE: 9.2352\n",
            "Epoch [14/50] Training Loss: 6.2455\n",
            "Epoch [14/50] Training MAE: 6.2455\n",
            "Epoch [14/50] Training RMSE: 9.2475\n",
            "Epoch [14/50] Validation Loss: 6.2338\n",
            "Epoch [14/50] Validation MAE: 6.2338\n",
            "Epoch [14/50] Validation RMSE: 9.2343\n",
            "Epoch [15/50] Training Loss: 6.2447\n",
            "Epoch [15/50] Training MAE: 6.2447\n",
            "Epoch [15/50] Training RMSE: 9.2465\n",
            "Epoch [15/50] Validation Loss: 6.2345\n",
            "Epoch [15/50] Validation MAE: 6.2345\n",
            "Epoch [15/50] Validation RMSE: 9.2340\n",
            "renew lr to 0.00025\n",
            "Epoch [16/50] Training Loss: 6.2419\n",
            "Epoch [16/50] Training MAE: 6.2419\n",
            "Epoch [16/50] Training RMSE: 9.2429\n",
            "Epoch [16/50] Validation Loss: 6.2314\n",
            "Epoch [16/50] Validation MAE: 6.2314\n",
            "Epoch [16/50] Validation RMSE: 9.2300\n",
            "Epoch [17/50] Training Loss: 6.2415\n",
            "Epoch [17/50] Training MAE: 6.2415\n",
            "Epoch [17/50] Training RMSE: 9.2425\n",
            "Epoch [17/50] Validation Loss: 6.2324\n",
            "Epoch [17/50] Validation MAE: 6.2324\n",
            "Epoch [17/50] Validation RMSE: 9.2309\n",
            "Epoch [18/50] Training Loss: 6.2410\n",
            "Epoch [18/50] Training MAE: 6.2410\n",
            "Epoch [18/50] Training RMSE: 9.2420\n",
            "Epoch [18/50] Validation Loss: 6.2330\n",
            "Epoch [18/50] Validation MAE: 6.2330\n",
            "Epoch [18/50] Validation RMSE: 9.2309\n",
            "renew lr to 0.000125\n",
            "Epoch [19/50] Training Loss: 6.2396\n",
            "Epoch [19/50] Training MAE: 6.2396\n",
            "Epoch [19/50] Training RMSE: 9.2409\n",
            "Epoch [19/50] Validation Loss: 6.2308\n",
            "Epoch [19/50] Validation MAE: 6.2308\n",
            "Epoch [19/50] Validation RMSE: 9.2280\n",
            "Epoch [20/50] Training Loss: 6.2392\n",
            "Epoch [20/50] Training MAE: 6.2392\n",
            "Epoch [20/50] Training RMSE: 9.2400\n",
            "Epoch [20/50] Validation Loss: 6.2284\n",
            "Epoch [20/50] Validation MAE: 6.2284\n",
            "Epoch [20/50] Validation RMSE: 9.2268\n",
            "Epoch [21/50] Training Loss: 6.2389\n",
            "Epoch [21/50] Training MAE: 6.2389\n",
            "Epoch [21/50] Training RMSE: 9.2389\n",
            "Epoch [21/50] Validation Loss: 6.2281\n",
            "Epoch [21/50] Validation MAE: 6.2281\n",
            "Epoch [21/50] Validation RMSE: 9.2268\n",
            "Epoch [22/50] Training Loss: 6.2390\n",
            "Epoch [22/50] Training MAE: 6.2390\n",
            "Epoch [22/50] Training RMSE: 9.2398\n",
            "Epoch [22/50] Validation Loss: 6.2267\n",
            "Epoch [22/50] Validation MAE: 6.2267\n",
            "Epoch [22/50] Validation RMSE: 9.2263\n",
            "Epoch [23/50] Training Loss: 6.2385\n",
            "Epoch [23/50] Training MAE: 6.2385\n",
            "Epoch [23/50] Training RMSE: 9.2385\n",
            "Epoch [23/50] Validation Loss: 6.2277\n",
            "Epoch [23/50] Validation MAE: 6.2277\n",
            "Epoch [23/50] Validation RMSE: 9.2262\n",
            "Epoch [24/50] Training Loss: 6.2384\n",
            "Epoch [24/50] Training MAE: 6.2384\n",
            "Epoch [24/50] Training RMSE: 9.2385\n",
            "Epoch [24/50] Validation Loss: 6.2265\n",
            "Epoch [24/50] Validation MAE: 6.2265\n",
            "Epoch [24/50] Validation RMSE: 9.2256\n",
            "Epoch [25/50] Training Loss: 6.2386\n",
            "Epoch [25/50] Training MAE: 6.2386\n",
            "Epoch [25/50] Training RMSE: 9.2386\n",
            "Epoch [25/50] Validation Loss: 6.2275\n",
            "Epoch [25/50] Validation MAE: 6.2275\n",
            "Epoch [25/50] Validation RMSE: 9.2255\n",
            "renew lr to 6.25e-05\n",
            "Epoch [26/50] Training Loss: 6.2372\n",
            "Epoch [26/50] Training MAE: 6.2372\n",
            "Epoch [26/50] Training RMSE: 9.2369\n",
            "Epoch [26/50] Validation Loss: 6.2279\n",
            "Epoch [26/50] Validation MAE: 6.2279\n",
            "Epoch [26/50] Validation RMSE: 9.2249\n",
            "Epoch [27/50] Training Loss: 6.2372\n",
            "Epoch [27/50] Training MAE: 6.2372\n",
            "Epoch [27/50] Training RMSE: 9.2370\n",
            "Epoch [27/50] Validation Loss: 6.2265\n",
            "Epoch [27/50] Validation MAE: 6.2265\n",
            "Epoch [27/50] Validation RMSE: 9.2245\n",
            "Epoch [28/50] Training Loss: 6.2371\n",
            "Epoch [28/50] Training MAE: 6.2371\n",
            "Epoch [28/50] Training RMSE: 9.2368\n",
            "Epoch [28/50] Validation Loss: 6.2281\n",
            "Epoch [28/50] Validation MAE: 6.2281\n",
            "Epoch [28/50] Validation RMSE: 9.2248\n",
            "renew lr to 3.125e-05\n",
            "Epoch [29/50] Training Loss: 6.2365\n",
            "Epoch [29/50] Training MAE: 6.2365\n",
            "Epoch [29/50] Training RMSE: 9.2362\n",
            "Epoch [29/50] Validation Loss: 6.2258\n",
            "Epoch [29/50] Validation MAE: 6.2258\n",
            "Epoch [29/50] Validation RMSE: 9.2237\n",
            "Epoch [30/50] Training Loss: 6.2368\n",
            "Epoch [30/50] Training MAE: 6.2368\n",
            "Epoch [30/50] Training RMSE: 9.2362\n",
            "Epoch [30/50] Validation Loss: 6.2254\n",
            "Epoch [30/50] Validation MAE: 6.2254\n",
            "Epoch [30/50] Validation RMSE: 9.2236\n",
            "Epoch [31/50] Training Loss: 6.2362\n",
            "Epoch [31/50] Training MAE: 6.2362\n",
            "Epoch [31/50] Training RMSE: 9.2357\n",
            "Epoch [31/50] Validation Loss: 6.2250\n",
            "Epoch [31/50] Validation MAE: 6.2250\n",
            "Epoch [31/50] Validation RMSE: 9.2234\n",
            "Epoch [32/50] Training Loss: 6.2362\n",
            "Epoch [32/50] Training MAE: 6.2362\n",
            "Epoch [32/50] Training RMSE: 9.2353\n",
            "Epoch [32/50] Validation Loss: 6.2253\n",
            "Epoch [32/50] Validation MAE: 6.2253\n",
            "Epoch [32/50] Validation RMSE: 9.2232\n",
            "Epoch [33/50] Training Loss: 6.2360\n",
            "Epoch [33/50] Training MAE: 6.2360\n",
            "Epoch [33/50] Training RMSE: 9.2349\n",
            "Epoch [33/50] Validation Loss: 6.2259\n",
            "Epoch [33/50] Validation MAE: 6.2259\n",
            "Epoch [33/50] Validation RMSE: 9.2233\n",
            "renew lr to 1.5625e-05\n",
            "Epoch [34/50] Training Loss: 6.2365\n",
            "Epoch [34/50] Training MAE: 6.2365\n",
            "Epoch [34/50] Training RMSE: 9.2352\n",
            "Epoch [34/50] Validation Loss: 6.2252\n",
            "Epoch [34/50] Validation MAE: 6.2252\n",
            "Epoch [34/50] Validation RMSE: 9.2229\n",
            "Epoch [35/50] Training Loss: 6.2363\n",
            "Epoch [35/50] Training MAE: 6.2363\n",
            "Epoch [35/50] Training RMSE: 9.2352\n",
            "Epoch [35/50] Validation Loss: 6.2250\n",
            "Epoch [35/50] Validation MAE: 6.2250\n",
            "Epoch [35/50] Validation RMSE: 9.2229\n",
            "Epoch [36/50] Training Loss: 6.2363\n",
            "Epoch [36/50] Training MAE: 6.2363\n",
            "Epoch [36/50] Training RMSE: 9.2355\n",
            "Epoch [36/50] Validation Loss: 6.2250\n",
            "Epoch [36/50] Validation MAE: 6.2250\n",
            "Epoch [36/50] Validation RMSE: 9.2227\n",
            "Epoch [37/50] Training Loss: 6.2360\n",
            "Epoch [37/50] Training MAE: 6.2360\n",
            "Epoch [37/50] Training RMSE: 9.2350\n",
            "Epoch [37/50] Validation Loss: 6.2255\n",
            "Epoch [37/50] Validation MAE: 6.2255\n",
            "Epoch [37/50] Validation RMSE: 9.2229\n",
            "Epoch [38/50] Training Loss: 6.2360\n",
            "Epoch [38/50] Training MAE: 6.2360\n",
            "Epoch [38/50] Training RMSE: 9.2347\n",
            "Epoch [38/50] Validation Loss: 6.2255\n",
            "Epoch [38/50] Validation MAE: 6.2255\n",
            "Epoch [38/50] Validation RMSE: 9.2229\n",
            "renew lr to 7.8125e-06\n",
            "Epoch [39/50] Training Loss: 6.2360\n",
            "Epoch [39/50] Training MAE: 6.2360\n",
            "Epoch [39/50] Training RMSE: 9.2351\n",
            "Epoch [39/50] Validation Loss: 6.2247\n",
            "Epoch [39/50] Validation MAE: 6.2247\n",
            "Epoch [39/50] Validation RMSE: 9.2228\n",
            "Epoch [40/50] Training Loss: 6.2359\n",
            "Epoch [40/50] Training MAE: 6.2359\n",
            "Epoch [40/50] Training RMSE: 9.2348\n",
            "Epoch [40/50] Validation Loss: 6.2253\n",
            "Epoch [40/50] Validation MAE: 6.2253\n",
            "Epoch [40/50] Validation RMSE: 9.2228\n",
            "Epoch [41/50] Training Loss: 6.2356\n",
            "Epoch [41/50] Training MAE: 6.2356\n",
            "Epoch [41/50] Training RMSE: 9.2344\n",
            "Epoch [41/50] Validation Loss: 6.2250\n",
            "Epoch [41/50] Validation MAE: 6.2250\n",
            "Epoch [41/50] Validation RMSE: 9.2227\n",
            "Epoch [42/50] Training Loss: 6.2358\n",
            "Epoch [42/50] Training MAE: 6.2358\n",
            "Epoch [42/50] Training RMSE: 9.2346\n",
            "Epoch [42/50] Validation Loss: 6.2252\n",
            "Epoch [42/50] Validation MAE: 6.2252\n",
            "Epoch [42/50] Validation RMSE: 9.2226\n",
            "renew lr to 3.90625e-06\n",
            "Epoch [43/50] Training Loss: 6.2357\n",
            "Epoch [43/50] Training MAE: 6.2357\n",
            "Epoch [43/50] Training RMSE: 9.2343\n",
            "Epoch [43/50] Validation Loss: 6.2252\n",
            "Epoch [43/50] Validation MAE: 6.2252\n",
            "Epoch [43/50] Validation RMSE: 9.2226\n",
            "Epoch [44/50] Training Loss: 6.2356\n",
            "Epoch [44/50] Training MAE: 6.2356\n",
            "Epoch [44/50] Training RMSE: 9.2343\n",
            "Epoch [44/50] Validation Loss: 6.2254\n",
            "Epoch [44/50] Validation MAE: 6.2254\n",
            "Epoch [44/50] Validation RMSE: 9.2226\n",
            "Epoch [45/50] Training Loss: 6.2359\n",
            "Epoch [45/50] Training MAE: 6.2359\n",
            "Epoch [45/50] Training RMSE: 9.2345\n",
            "Epoch [45/50] Validation Loss: 6.2251\n",
            "Epoch [45/50] Validation MAE: 6.2251\n",
            "Epoch [45/50] Validation RMSE: 9.2226\n",
            "Epoch [46/50] Training Loss: 6.2356\n",
            "Epoch [46/50] Training MAE: 6.2356\n",
            "Epoch [46/50] Training RMSE: 9.2348\n",
            "Epoch [46/50] Validation Loss: 6.2253\n",
            "Epoch [46/50] Validation MAE: 6.2253\n",
            "Epoch [46/50] Validation RMSE: 9.2226\n",
            "renew lr to 1.953125e-06\n",
            "Epoch [47/50] Training Loss: 6.2356\n",
            "Epoch [47/50] Training MAE: 6.2356\n",
            "Epoch [47/50] Training RMSE: 9.2344\n",
            "Epoch [47/50] Validation Loss: 6.2251\n",
            "Epoch [47/50] Validation MAE: 6.2251\n",
            "Epoch [47/50] Validation RMSE: 9.2226\n",
            "Epoch [48/50] Training Loss: 6.2358\n",
            "Epoch [48/50] Training MAE: 6.2358\n",
            "Epoch [48/50] Training RMSE: 9.2341\n",
            "Epoch [48/50] Validation Loss: 6.2253\n",
            "Epoch [48/50] Validation MAE: 6.2253\n",
            "Epoch [48/50] Validation RMSE: 9.2226\n",
            "Epoch [49/50] Training Loss: 6.2357\n",
            "Epoch [49/50] Training MAE: 6.2357\n",
            "Epoch [49/50] Training RMSE: 9.2342\n",
            "Epoch [49/50] Validation Loss: 6.2252\n",
            "Epoch [49/50] Validation MAE: 6.2252\n",
            "Epoch [49/50] Validation RMSE: 9.2226\n",
            "Epoch [50/50] Training Loss: 6.2359\n",
            "Epoch [50/50] Training MAE: 6.2359\n",
            "Epoch [50/50] Training RMSE: 9.2348\n",
            "Epoch [50/50] Validation Loss: 6.2253\n",
            "Epoch [50/50] Validation MAE: 6.2253\n",
            "Epoch [50/50] Validation RMSE: 9.2226\n",
            "renew lr to 9.765625e-07\n",
            "Train over.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(out_path, \"space_time_transformer.pt\"))"
      ],
      "metadata": {
        "id": "mtJf_thZxW0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "metrics = {\n",
        "    \"Train Loss\": tot_train_losses,\n",
        "    \"Train MAE\": tot_train_mae,\n",
        "    \"Train RMSE\": tot_train_rmse,\n",
        "    \"Validation Loss\": tot_valid_losses,\n",
        "    \"Validation MAE\": tot_valid_mae,\n",
        "    \"Validation RMSE\": tot_valid_rmse\n",
        "}\n",
        "\n",
        "with open('transformer_training_metrics.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)"
      ],
      "metadata": {
        "id": "roD1x5HqxW3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Training Loss\": [6.3510, 6.2812, 6.2736, 6.2707, 6.2661, 6.2634, 6.2592, 6.2567, 6.2554, 6.2539,\n",
        "                      6.2479, 6.2473, 6.2461, 6.2455, 6.2447, 6.2447, 6.2434, 6.2429, 6.2419, 6.2415,\n",
        "                      6.2410, 6.2396, 6.2392, 6.2389, 6.2390, 6.2385, 6.2384, 6.2386, 6.2372, 6.2372,\n",
        "                      6.2371, 6.2365, 6.2368, 6.2362, 6.2362, 6.2360, 6.2360, 6.2363, 6.2363, 6.2360,\n",
        "                      6.2365, 6.2363, 6.2360, 6.2359, 6.2356, 6.2358, 6.2357, 6.2356, 6.2358, 6.2357],\n",
        "    \"Training MAE\": [6.3510, 6.2812, 6.2736, 6.2707, 6.2661, 6.2634, 6.2592, 6.2567, 6.2554, 6.2539,\n",
        "                     6.2479, 6.2473, 6.2461, 6.2455, 6.2447, 6.2447, 6.2434, 6.2429, 6.2419, 6.2415,\n",
        "                     6.2410, 6.2396, 6.2392, 6.2389, 6.2390, 6.2385, 6.2384, 6.2386, 6.2372, 6.2372,\n",
        "                     6.2371, 6.2365, 6.2368, 6.2362, 6.2362, 6.2360, 6.2360, 6.2363, 6.2363, 6.2360,\n",
        "                     6.2365, 6.2363, 6.2360, 6.2359, 6.2356, 6.2358, 6.2357, 6.2356, 6.2358, 6.2357],\n",
        "    \"Training RMSE\": [9.3998, 9.2843, 9.2776, 9.2747, 9.2702, 9.2675, 9.2633, 9.2607, 9.2589, 9.2570,\n",
        "                      9.2512, 9.2498, 9.2482, 9.2475, 9.2465, 9.2465, 9.2451, 9.2446, 9.2429, 9.2425,\n",
        "                      9.2420, 9.2409, 9.2400, 9.2389, 9.2398, 9.2385, 9.2385, 9.2386, 9.2386, 9.2369,\n",
        "                      9.2370, 9.2368, 9.2368, 9.2362, 9.2362, 9.2357, 9.2357, 9.2351, 9.2352, 9.2348,\n",
        "                      9.2352, 9.2352, 9.2348, 9.2349, 9.2344, 9.2346, 9.2343, 9.2343, 9.2345, 9.2342],\n",
        "    \"Validation Loss\": [6.2801, 6.2655, 6.2593, 6.2550, 6.2511, 6.2468, 6.2474, 6.2456, 6.2456, 6.2442,\n",
        "                        6.2378, 6.2361, 6.2333, 6.2338, 6.2345, 6.2345, 6.2332, 6.2331, 6.2314, 6.2324,\n",
        "                        6.2330, 6.2308, 6.2284, 6.2281, 6.2267, 6.2277, 6.2265, 6.2275, 6.2279, 6.2265,\n",
        "                        6.2265, 6.2265, 6.2258, 6.2254, 6.2250, 6.2253, 6.2250, 6.2253, 6.2251, 6.2252,\n",
        "                        6.2253, 6.2250, 6.2253, 6.2250, 6.2252, 6.2250, 6.2250, 6.2253, 6.2252, 6.2253],\n",
        "    \"Validation MAE\": [6.2801, 6.2655, 6.2593, 6.2550, 6.2511, 6.2468, 6.2474, 6.2456, 6.2456, 6.2442,\n",
        "                       6.2378, 6.2361, 6.2333, 6.2338, 6.2345, 6.2345, 6.2332, 6.2331, 6.2314, 6.2324,\n",
        "                       6.2330, 6.2308, 6.2284, 6.2281, 6.2267, 6.2277, 6.2265, 6.2275, 6.2279, 6.2265,\n",
        "                       6.2265, 6.2265, 6.2258, 6.2254, 6.2250, 6.2253, 6.2250, 6.2253, 6.2251, 6.2252,\n",
        "                       6.2253, 6.2250, 6.2253, 6.2250, 6.2252, 6.2250, 6.2250, 6.2253, 6.2252, 6.2253],\n",
        "    \"Validation RMSE\": [9.2744, 9.2678, 9.2632, 9.2595, 9.2559, 9.2518, 9.2525, 9.2509, 9.2508, 9.2495,\n",
        "                        9.2441, 9.2422, 9.2394, 9.2399, 9.2407, 9.2406, 9.2395, 9.2393, 9.2380, 9.2389,\n",
        "                        9.2394, 9.2373, 9.2348, 9.2344, 9.2329, 9.2340, 9.2327, 9.2339, 9.2343, 9.2328,\n",
        "                        9.2329, 9.2328, 9.2323, 9.2320, 9.2315, 9.2318, 9.2315, 9.2318, 9.2316, 9.2317,\n",
        "                        9.2318, 9.2315, 9.2318, 9.2315, 9.2317, 9.2315, 9.2315, 9.2318, 9.2317, 9.2318]\n",
        "}"
      ],
      "metadata": {
        "id": "9OdsdgHik2QM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(data)\n",
        "metrics_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zcradM9LlBbC",
        "outputId": "236a7b9b-23f0-4af5-9671-dfb3d78ff832"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Epoch  Training Loss  Training MAE  Training RMSE  Validation Loss  \\\n",
              "0       1         6.3510        6.3510         9.3998           6.2801   \n",
              "1       2         6.2812        6.2812         9.2843           6.2655   \n",
              "2       3         6.2736        6.2736         9.2776           6.2593   \n",
              "3       4         6.2707        6.2707         9.2747           6.2550   \n",
              "4       5         6.2661        6.2661         9.2702           6.2511   \n",
              "5       6         6.2634        6.2634         9.2675           6.2468   \n",
              "6       7         6.2592        6.2592         9.2633           6.2474   \n",
              "7       8         6.2567        6.2567         9.2607           6.2456   \n",
              "8       9         6.2554        6.2554         9.2589           6.2456   \n",
              "9      10         6.2539        6.2539         9.2570           6.2442   \n",
              "10     11         6.2479        6.2479         9.2512           6.2378   \n",
              "11     12         6.2473        6.2473         9.2498           6.2361   \n",
              "12     13         6.2461        6.2461         9.2482           6.2333   \n",
              "13     14         6.2455        6.2455         9.2475           6.2338   \n",
              "14     15         6.2447        6.2447         9.2465           6.2345   \n",
              "15     16         6.2447        6.2447         9.2465           6.2345   \n",
              "16     17         6.2434        6.2434         9.2451           6.2332   \n",
              "17     18         6.2429        6.2429         9.2446           6.2331   \n",
              "18     19         6.2419        6.2419         9.2429           6.2314   \n",
              "19     20         6.2415        6.2415         9.2425           6.2324   \n",
              "20     21         6.2410        6.2410         9.2420           6.2330   \n",
              "21     22         6.2396        6.2396         9.2409           6.2308   \n",
              "22     23         6.2392        6.2392         9.2400           6.2284   \n",
              "23     24         6.2389        6.2389         9.2389           6.2281   \n",
              "24     25         6.2390        6.2390         9.2398           6.2267   \n",
              "25     26         6.2385        6.2385         9.2385           6.2277   \n",
              "26     27         6.2384        6.2384         9.2385           6.2265   \n",
              "27     28         6.2386        6.2386         9.2386           6.2275   \n",
              "28     29         6.2372        6.2372         9.2386           6.2279   \n",
              "29     30         6.2372        6.2372         9.2369           6.2265   \n",
              "30     31         6.2371        6.2371         9.2370           6.2265   \n",
              "31     32         6.2365        6.2365         9.2368           6.2265   \n",
              "32     33         6.2368        6.2368         9.2368           6.2258   \n",
              "33     34         6.2362        6.2362         9.2362           6.2254   \n",
              "34     35         6.2362        6.2362         9.2362           6.2250   \n",
              "35     36         6.2360        6.2360         9.2357           6.2253   \n",
              "36     37         6.2360        6.2360         9.2357           6.2250   \n",
              "37     38         6.2363        6.2363         9.2351           6.2253   \n",
              "38     39         6.2363        6.2363         9.2352           6.2251   \n",
              "39     40         6.2360        6.2360         9.2348           6.2252   \n",
              "40     41         6.2365        6.2365         9.2352           6.2253   \n",
              "41     42         6.2363        6.2363         9.2352           6.2250   \n",
              "42     43         6.2360        6.2360         9.2348           6.2253   \n",
              "43     44         6.2359        6.2359         9.2349           6.2250   \n",
              "44     45         6.2356        6.2356         9.2344           6.2252   \n",
              "45     46         6.2358        6.2358         9.2346           6.2250   \n",
              "46     47         6.2357        6.2357         9.2343           6.2250   \n",
              "47     48         6.2356        6.2356         9.2343           6.2253   \n",
              "48     49         6.2358        6.2358         9.2345           6.2252   \n",
              "49     50         6.2357        6.2357         9.2342           6.2253   \n",
              "\n",
              "    Validation MAE  Validation RMSE  \n",
              "0           6.2801           9.2744  \n",
              "1           6.2655           9.2678  \n",
              "2           6.2593           9.2632  \n",
              "3           6.2550           9.2595  \n",
              "4           6.2511           9.2559  \n",
              "5           6.2468           9.2518  \n",
              "6           6.2474           9.2525  \n",
              "7           6.2456           9.2509  \n",
              "8           6.2456           9.2508  \n",
              "9           6.2442           9.2495  \n",
              "10          6.2378           9.2441  \n",
              "11          6.2361           9.2422  \n",
              "12          6.2333           9.2394  \n",
              "13          6.2338           9.2399  \n",
              "14          6.2345           9.2407  \n",
              "15          6.2345           9.2406  \n",
              "16          6.2332           9.2395  \n",
              "17          6.2331           9.2393  \n",
              "18          6.2314           9.2380  \n",
              "19          6.2324           9.2389  \n",
              "20          6.2330           9.2394  \n",
              "21          6.2308           9.2373  \n",
              "22          6.2284           9.2348  \n",
              "23          6.2281           9.2344  \n",
              "24          6.2267           9.2329  \n",
              "25          6.2277           9.2340  \n",
              "26          6.2265           9.2327  \n",
              "27          6.2275           9.2339  \n",
              "28          6.2279           9.2343  \n",
              "29          6.2265           9.2328  \n",
              "30          6.2265           9.2329  \n",
              "31          6.2265           9.2328  \n",
              "32          6.2258           9.2323  \n",
              "33          6.2254           9.2320  \n",
              "34          6.2250           9.2315  \n",
              "35          6.2253           9.2318  \n",
              "36          6.2250           9.2315  \n",
              "37          6.2253           9.2318  \n",
              "38          6.2251           9.2316  \n",
              "39          6.2252           9.2317  \n",
              "40          6.2253           9.2318  \n",
              "41          6.2250           9.2315  \n",
              "42          6.2253           9.2318  \n",
              "43          6.2250           9.2315  \n",
              "44          6.2252           9.2317  \n",
              "45          6.2250           9.2315  \n",
              "46          6.2250           9.2315  \n",
              "47          6.2253           9.2318  \n",
              "48          6.2252           9.2317  \n",
              "49          6.2253           9.2318  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea618b4a-ac9f-4db8-aa6f-08f171dc2ebc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Training MAE</th>\n",
              "      <th>Training RMSE</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Validation MAE</th>\n",
              "      <th>Validation RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>6.3510</td>\n",
              "      <td>6.3510</td>\n",
              "      <td>9.3998</td>\n",
              "      <td>6.2801</td>\n",
              "      <td>6.2801</td>\n",
              "      <td>9.2744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>6.2812</td>\n",
              "      <td>6.2812</td>\n",
              "      <td>9.2843</td>\n",
              "      <td>6.2655</td>\n",
              "      <td>6.2655</td>\n",
              "      <td>9.2678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>6.2736</td>\n",
              "      <td>6.2736</td>\n",
              "      <td>9.2776</td>\n",
              "      <td>6.2593</td>\n",
              "      <td>6.2593</td>\n",
              "      <td>9.2632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>6.2707</td>\n",
              "      <td>6.2707</td>\n",
              "      <td>9.2747</td>\n",
              "      <td>6.2550</td>\n",
              "      <td>6.2550</td>\n",
              "      <td>9.2595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6.2661</td>\n",
              "      <td>6.2661</td>\n",
              "      <td>9.2702</td>\n",
              "      <td>6.2511</td>\n",
              "      <td>6.2511</td>\n",
              "      <td>9.2559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>6.2634</td>\n",
              "      <td>6.2634</td>\n",
              "      <td>9.2675</td>\n",
              "      <td>6.2468</td>\n",
              "      <td>6.2468</td>\n",
              "      <td>9.2518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>6.2592</td>\n",
              "      <td>6.2592</td>\n",
              "      <td>9.2633</td>\n",
              "      <td>6.2474</td>\n",
              "      <td>6.2474</td>\n",
              "      <td>9.2525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>6.2567</td>\n",
              "      <td>6.2567</td>\n",
              "      <td>9.2607</td>\n",
              "      <td>6.2456</td>\n",
              "      <td>6.2456</td>\n",
              "      <td>9.2509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>6.2554</td>\n",
              "      <td>6.2554</td>\n",
              "      <td>9.2589</td>\n",
              "      <td>6.2456</td>\n",
              "      <td>6.2456</td>\n",
              "      <td>9.2508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>6.2539</td>\n",
              "      <td>6.2539</td>\n",
              "      <td>9.2570</td>\n",
              "      <td>6.2442</td>\n",
              "      <td>6.2442</td>\n",
              "      <td>9.2495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>6.2479</td>\n",
              "      <td>6.2479</td>\n",
              "      <td>9.2512</td>\n",
              "      <td>6.2378</td>\n",
              "      <td>6.2378</td>\n",
              "      <td>9.2441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>6.2473</td>\n",
              "      <td>6.2473</td>\n",
              "      <td>9.2498</td>\n",
              "      <td>6.2361</td>\n",
              "      <td>6.2361</td>\n",
              "      <td>9.2422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>6.2461</td>\n",
              "      <td>6.2461</td>\n",
              "      <td>9.2482</td>\n",
              "      <td>6.2333</td>\n",
              "      <td>6.2333</td>\n",
              "      <td>9.2394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>6.2455</td>\n",
              "      <td>6.2455</td>\n",
              "      <td>9.2475</td>\n",
              "      <td>6.2338</td>\n",
              "      <td>6.2338</td>\n",
              "      <td>9.2399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>6.2447</td>\n",
              "      <td>6.2447</td>\n",
              "      <td>9.2465</td>\n",
              "      <td>6.2345</td>\n",
              "      <td>6.2345</td>\n",
              "      <td>9.2407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>6.2447</td>\n",
              "      <td>6.2447</td>\n",
              "      <td>9.2465</td>\n",
              "      <td>6.2345</td>\n",
              "      <td>6.2345</td>\n",
              "      <td>9.2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>6.2434</td>\n",
              "      <td>6.2434</td>\n",
              "      <td>9.2451</td>\n",
              "      <td>6.2332</td>\n",
              "      <td>6.2332</td>\n",
              "      <td>9.2395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>6.2429</td>\n",
              "      <td>6.2429</td>\n",
              "      <td>9.2446</td>\n",
              "      <td>6.2331</td>\n",
              "      <td>6.2331</td>\n",
              "      <td>9.2393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>6.2419</td>\n",
              "      <td>6.2419</td>\n",
              "      <td>9.2429</td>\n",
              "      <td>6.2314</td>\n",
              "      <td>6.2314</td>\n",
              "      <td>9.2380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>6.2415</td>\n",
              "      <td>6.2415</td>\n",
              "      <td>9.2425</td>\n",
              "      <td>6.2324</td>\n",
              "      <td>6.2324</td>\n",
              "      <td>9.2389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>6.2410</td>\n",
              "      <td>6.2410</td>\n",
              "      <td>9.2420</td>\n",
              "      <td>6.2330</td>\n",
              "      <td>6.2330</td>\n",
              "      <td>9.2394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>6.2396</td>\n",
              "      <td>6.2396</td>\n",
              "      <td>9.2409</td>\n",
              "      <td>6.2308</td>\n",
              "      <td>6.2308</td>\n",
              "      <td>9.2373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>6.2392</td>\n",
              "      <td>6.2392</td>\n",
              "      <td>9.2400</td>\n",
              "      <td>6.2284</td>\n",
              "      <td>6.2284</td>\n",
              "      <td>9.2348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>6.2389</td>\n",
              "      <td>6.2389</td>\n",
              "      <td>9.2389</td>\n",
              "      <td>6.2281</td>\n",
              "      <td>6.2281</td>\n",
              "      <td>9.2344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>6.2390</td>\n",
              "      <td>6.2390</td>\n",
              "      <td>9.2398</td>\n",
              "      <td>6.2267</td>\n",
              "      <td>6.2267</td>\n",
              "      <td>9.2329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>6.2385</td>\n",
              "      <td>6.2385</td>\n",
              "      <td>9.2385</td>\n",
              "      <td>6.2277</td>\n",
              "      <td>6.2277</td>\n",
              "      <td>9.2340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>6.2384</td>\n",
              "      <td>6.2384</td>\n",
              "      <td>9.2385</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>9.2327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>6.2386</td>\n",
              "      <td>6.2386</td>\n",
              "      <td>9.2386</td>\n",
              "      <td>6.2275</td>\n",
              "      <td>6.2275</td>\n",
              "      <td>9.2339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>6.2372</td>\n",
              "      <td>6.2372</td>\n",
              "      <td>9.2386</td>\n",
              "      <td>6.2279</td>\n",
              "      <td>6.2279</td>\n",
              "      <td>9.2343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>6.2372</td>\n",
              "      <td>6.2372</td>\n",
              "      <td>9.2369</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>9.2328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>6.2371</td>\n",
              "      <td>6.2371</td>\n",
              "      <td>9.2370</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>9.2329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>6.2365</td>\n",
              "      <td>6.2365</td>\n",
              "      <td>9.2368</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>6.2265</td>\n",
              "      <td>9.2328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>6.2368</td>\n",
              "      <td>6.2368</td>\n",
              "      <td>9.2368</td>\n",
              "      <td>6.2258</td>\n",
              "      <td>6.2258</td>\n",
              "      <td>9.2323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>6.2362</td>\n",
              "      <td>6.2362</td>\n",
              "      <td>9.2362</td>\n",
              "      <td>6.2254</td>\n",
              "      <td>6.2254</td>\n",
              "      <td>9.2320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>6.2362</td>\n",
              "      <td>6.2362</td>\n",
              "      <td>9.2362</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>9.2315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>9.2357</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>9.2318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>9.2357</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>9.2315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>6.2363</td>\n",
              "      <td>6.2363</td>\n",
              "      <td>9.2351</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>9.2318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>6.2363</td>\n",
              "      <td>6.2363</td>\n",
              "      <td>9.2352</td>\n",
              "      <td>6.2251</td>\n",
              "      <td>6.2251</td>\n",
              "      <td>9.2316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>9.2348</td>\n",
              "      <td>6.2252</td>\n",
              "      <td>6.2252</td>\n",
              "      <td>9.2317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>6.2365</td>\n",
              "      <td>6.2365</td>\n",
              "      <td>9.2352</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>9.2318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>6.2363</td>\n",
              "      <td>6.2363</td>\n",
              "      <td>9.2352</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>9.2315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>6.2360</td>\n",
              "      <td>9.2348</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>9.2318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>44</td>\n",
              "      <td>6.2359</td>\n",
              "      <td>6.2359</td>\n",
              "      <td>9.2349</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>9.2315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>45</td>\n",
              "      <td>6.2356</td>\n",
              "      <td>6.2356</td>\n",
              "      <td>9.2344</td>\n",
              "      <td>6.2252</td>\n",
              "      <td>6.2252</td>\n",
              "      <td>9.2317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>46</td>\n",
              "      <td>6.2358</td>\n",
              "      <td>6.2358</td>\n",
              "      <td>9.2346</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>9.2315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>47</td>\n",
              "      <td>6.2357</td>\n",
              "      <td>6.2357</td>\n",
              "      <td>9.2343</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>6.2250</td>\n",
              "      <td>9.2315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>48</td>\n",
              "      <td>6.2356</td>\n",
              "      <td>6.2356</td>\n",
              "      <td>9.2343</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>9.2318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>49</td>\n",
              "      <td>6.2358</td>\n",
              "      <td>6.2358</td>\n",
              "      <td>9.2345</td>\n",
              "      <td>6.2252</td>\n",
              "      <td>6.2252</td>\n",
              "      <td>9.2317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>50</td>\n",
              "      <td>6.2357</td>\n",
              "      <td>6.2357</td>\n",
              "      <td>9.2342</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>6.2253</td>\n",
              "      <td>9.2318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea618b4a-ac9f-4db8-aa6f-08f171dc2ebc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea618b4a-ac9f-4db8-aa6f-08f171dc2ebc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea618b4a-ac9f-4db8-aa6f-08f171dc2ebc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dcdb2479-215a-45b1-b3e9-6e77ec290ad7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcdb2479-215a-45b1-b3e9-6e77ec290ad7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dcdb2479-215a-45b1-b3e9-6e77ec290ad7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c191fe0d-4885-4fa5-8bf2-1666ee0c15c9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c191fe0d-4885-4fa5-8bf2-1666ee0c15c9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuCuVYGDlBjq",
        "outputId": "326bffae-6ae9-4b7b-a74b-60615707ec21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Epoch              1.0000\n",
              "Training Loss      6.2356\n",
              "Training MAE       6.2356\n",
              "Training RMSE      9.2342\n",
              "Validation Loss    6.2250\n",
              "Validation MAE     6.2250\n",
              "Validation RMSE    9.2315\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWfYClRolOu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nj6jlYjjlO-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(metrics)\n",
        "metrics_df.to_csv('transformer_training_metrics.csv', index=False)"
      ],
      "metadata": {
        "id": "byJ3q-XP33SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.read_csv('transformer_training_metrics.csv')\n",
        "metrics_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xGWkhUr133bn",
        "outputId": "c429d1c9-4926-4c54-a8dd-5cc61397a277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Train Loss  Train MAE  Train RMSE  Validation Loss  Validation MAE  \\\n",
              "0     6.350999   6.350999    9.399841         6.350999        6.350999   \n",
              "1     6.281198   6.281198    9.284310         6.281198        6.281198   \n",
              "2     6.273588   6.273588    9.277553         6.273588        6.273588   \n",
              "3     6.270671   6.270671    9.274746         6.270671        6.270671   \n",
              "4     6.266131   6.266130    9.270213         6.266131        6.266130   \n",
              "5     6.263353   6.263353    9.267455         6.263353        6.263353   \n",
              "6     6.259235   6.259235    9.263309         6.259235        6.259235   \n",
              "7     6.256734   6.256733    9.260707         6.256734        6.256733   \n",
              "8     6.255377   6.255377    9.258885         6.255377        6.255377   \n",
              "9     6.253917   6.253917    9.257002         6.253917        6.253917   \n",
              "10    6.247932   6.247932    9.251161         6.247932        6.247932   \n",
              "11    6.247262   6.247262    9.249822         6.247262        6.247262   \n",
              "12    6.246140   6.246140    9.248151         6.246140        6.246140   \n",
              "13    6.245524   6.245524    9.247451         6.245524        6.245524   \n",
              "14    6.244749   6.244749    9.246493         6.244749        6.244749   \n",
              "15    6.241863   6.241863    9.242901         6.241863        6.241863   \n",
              "16    6.241531   6.241532    9.242454         6.241531        6.241532   \n",
              "17    6.240979   6.240980    9.241973         6.240979        6.240980   \n",
              "18    6.239576   6.239576    9.240854         6.239576        6.239576   \n",
              "19    6.239162   6.239162    9.239976         6.239162        6.239162   \n",
              "20    6.238936   6.238936    9.238945         6.238936        6.238936   \n",
              "21    6.239035   6.239035    9.239821         6.239035        6.239035   \n",
              "22    6.238480   6.238480    9.238519         6.238480        6.238480   \n",
              "23    6.238427   6.238428    9.238550         6.238427        6.238428   \n",
              "24    6.238562   6.238562    9.238556         6.238562        6.238562   \n",
              "25    6.237211   6.237212    9.236919         6.237211        6.237212   \n",
              "26    6.237203   6.237202    9.237023         6.237203        6.237202   \n",
              "27    6.237058   6.237058    9.236795         6.237058        6.237058   \n",
              "28    6.236535   6.236535    9.236202         6.236535        6.236535   \n",
              "29    6.236756   6.236756    9.236235         6.236756        6.236756   \n",
              "30    6.236188   6.236188    9.235653         6.236188        6.236188   \n",
              "31    6.236240   6.236240    9.235293         6.236240        6.236240   \n",
              "32    6.235983   6.235983    9.234926         6.235983        6.235983   \n",
              "33    6.236472   6.236472    9.235192         6.236472        6.236472   \n",
              "34    6.236308   6.236307    9.235200         6.236308        6.236307   \n",
              "35    6.236289   6.236288    9.235492         6.236289        6.236288   \n",
              "36    6.235991   6.235991    9.234978         6.235991        6.235991   \n",
              "37    6.235989   6.235989    9.234674         6.235989        6.235989   \n",
              "38    6.236026   6.236026    9.235059         6.236026        6.236026   \n",
              "39    6.235880   6.235880    9.234830         6.235880        6.235880   \n",
              "40    6.235555   6.235555    9.234368         6.235555        6.235555   \n",
              "41    6.235816   6.235816    9.234620         6.235816        6.235816   \n",
              "42    6.235696   6.235696    9.234282         6.235696        6.235696   \n",
              "43    6.235562   6.235562    9.234260         6.235562        6.235562   \n",
              "44    6.235886   6.235886    9.234452         6.235886        6.235886   \n",
              "45    6.235647   6.235647    9.234764         6.235647        6.235647   \n",
              "46    6.235591   6.235591    9.234439         6.235591        6.235591   \n",
              "47    6.235784   6.235784    9.234097         6.235784        6.235784   \n",
              "48    6.235727   6.235727    9.234174         6.235727        6.235727   \n",
              "49    6.235863   6.235863    9.234758         6.235863        6.235863   \n",
              "\n",
              "    Validation RMSE  \n",
              "0          9.399841  \n",
              "1          9.284310  \n",
              "2          9.277553  \n",
              "3          9.274746  \n",
              "4          9.270213  \n",
              "5          9.267455  \n",
              "6          9.263309  \n",
              "7          9.260707  \n",
              "8          9.258885  \n",
              "9          9.257002  \n",
              "10         9.251161  \n",
              "11         9.249822  \n",
              "12         9.248151  \n",
              "13         9.247451  \n",
              "14         9.246493  \n",
              "15         9.242901  \n",
              "16         9.242454  \n",
              "17         9.241973  \n",
              "18         9.240854  \n",
              "19         9.239976  \n",
              "20         9.238945  \n",
              "21         9.239821  \n",
              "22         9.238519  \n",
              "23         9.238550  \n",
              "24         9.238556  \n",
              "25         9.236919  \n",
              "26         9.237023  \n",
              "27         9.236795  \n",
              "28         9.236202  \n",
              "29         9.236235  \n",
              "30         9.235653  \n",
              "31         9.235293  \n",
              "32         9.234926  \n",
              "33         9.235192  \n",
              "34         9.235200  \n",
              "35         9.235492  \n",
              "36         9.234978  \n",
              "37         9.234674  \n",
              "38         9.235059  \n",
              "39         9.234830  \n",
              "40         9.234368  \n",
              "41         9.234620  \n",
              "42         9.234282  \n",
              "43         9.234260  \n",
              "44         9.234452  \n",
              "45         9.234764  \n",
              "46         9.234439  \n",
              "47         9.234097  \n",
              "48         9.234174  \n",
              "49         9.234758  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bfb7db0-52e3-4328-b0af-fac22c2f8b0d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Train MAE</th>\n",
              "      <th>Train RMSE</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Validation MAE</th>\n",
              "      <th>Validation RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.350999</td>\n",
              "      <td>6.350999</td>\n",
              "      <td>9.399841</td>\n",
              "      <td>6.350999</td>\n",
              "      <td>6.350999</td>\n",
              "      <td>9.399841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.281198</td>\n",
              "      <td>6.281198</td>\n",
              "      <td>9.284310</td>\n",
              "      <td>6.281198</td>\n",
              "      <td>6.281198</td>\n",
              "      <td>9.284310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.273588</td>\n",
              "      <td>6.273588</td>\n",
              "      <td>9.277553</td>\n",
              "      <td>6.273588</td>\n",
              "      <td>6.273588</td>\n",
              "      <td>9.277553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.270671</td>\n",
              "      <td>6.270671</td>\n",
              "      <td>9.274746</td>\n",
              "      <td>6.270671</td>\n",
              "      <td>6.270671</td>\n",
              "      <td>9.274746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.266131</td>\n",
              "      <td>6.266130</td>\n",
              "      <td>9.270213</td>\n",
              "      <td>6.266131</td>\n",
              "      <td>6.266130</td>\n",
              "      <td>9.270213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.263353</td>\n",
              "      <td>6.263353</td>\n",
              "      <td>9.267455</td>\n",
              "      <td>6.263353</td>\n",
              "      <td>6.263353</td>\n",
              "      <td>9.267455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6.259235</td>\n",
              "      <td>6.259235</td>\n",
              "      <td>9.263309</td>\n",
              "      <td>6.259235</td>\n",
              "      <td>6.259235</td>\n",
              "      <td>9.263309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6.256734</td>\n",
              "      <td>6.256733</td>\n",
              "      <td>9.260707</td>\n",
              "      <td>6.256734</td>\n",
              "      <td>6.256733</td>\n",
              "      <td>9.260707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6.255377</td>\n",
              "      <td>6.255377</td>\n",
              "      <td>9.258885</td>\n",
              "      <td>6.255377</td>\n",
              "      <td>6.255377</td>\n",
              "      <td>9.258885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6.253917</td>\n",
              "      <td>6.253917</td>\n",
              "      <td>9.257002</td>\n",
              "      <td>6.253917</td>\n",
              "      <td>6.253917</td>\n",
              "      <td>9.257002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6.247932</td>\n",
              "      <td>6.247932</td>\n",
              "      <td>9.251161</td>\n",
              "      <td>6.247932</td>\n",
              "      <td>6.247932</td>\n",
              "      <td>9.251161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6.247262</td>\n",
              "      <td>6.247262</td>\n",
              "      <td>9.249822</td>\n",
              "      <td>6.247262</td>\n",
              "      <td>6.247262</td>\n",
              "      <td>9.249822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>6.246140</td>\n",
              "      <td>6.246140</td>\n",
              "      <td>9.248151</td>\n",
              "      <td>6.246140</td>\n",
              "      <td>6.246140</td>\n",
              "      <td>9.248151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>6.245524</td>\n",
              "      <td>6.245524</td>\n",
              "      <td>9.247451</td>\n",
              "      <td>6.245524</td>\n",
              "      <td>6.245524</td>\n",
              "      <td>9.247451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>6.244749</td>\n",
              "      <td>6.244749</td>\n",
              "      <td>9.246493</td>\n",
              "      <td>6.244749</td>\n",
              "      <td>6.244749</td>\n",
              "      <td>9.246493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6.241863</td>\n",
              "      <td>6.241863</td>\n",
              "      <td>9.242901</td>\n",
              "      <td>6.241863</td>\n",
              "      <td>6.241863</td>\n",
              "      <td>9.242901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6.241531</td>\n",
              "      <td>6.241532</td>\n",
              "      <td>9.242454</td>\n",
              "      <td>6.241531</td>\n",
              "      <td>6.241532</td>\n",
              "      <td>9.242454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6.240979</td>\n",
              "      <td>6.240980</td>\n",
              "      <td>9.241973</td>\n",
              "      <td>6.240979</td>\n",
              "      <td>6.240980</td>\n",
              "      <td>9.241973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>6.239576</td>\n",
              "      <td>6.239576</td>\n",
              "      <td>9.240854</td>\n",
              "      <td>6.239576</td>\n",
              "      <td>6.239576</td>\n",
              "      <td>9.240854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>6.239162</td>\n",
              "      <td>6.239162</td>\n",
              "      <td>9.239976</td>\n",
              "      <td>6.239162</td>\n",
              "      <td>6.239162</td>\n",
              "      <td>9.239976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>6.238936</td>\n",
              "      <td>6.238936</td>\n",
              "      <td>9.238945</td>\n",
              "      <td>6.238936</td>\n",
              "      <td>6.238936</td>\n",
              "      <td>9.238945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>6.239035</td>\n",
              "      <td>6.239035</td>\n",
              "      <td>9.239821</td>\n",
              "      <td>6.239035</td>\n",
              "      <td>6.239035</td>\n",
              "      <td>9.239821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>6.238480</td>\n",
              "      <td>6.238480</td>\n",
              "      <td>9.238519</td>\n",
              "      <td>6.238480</td>\n",
              "      <td>6.238480</td>\n",
              "      <td>9.238519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>6.238427</td>\n",
              "      <td>6.238428</td>\n",
              "      <td>9.238550</td>\n",
              "      <td>6.238427</td>\n",
              "      <td>6.238428</td>\n",
              "      <td>9.238550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>6.238562</td>\n",
              "      <td>6.238562</td>\n",
              "      <td>9.238556</td>\n",
              "      <td>6.238562</td>\n",
              "      <td>6.238562</td>\n",
              "      <td>9.238556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>6.237211</td>\n",
              "      <td>6.237212</td>\n",
              "      <td>9.236919</td>\n",
              "      <td>6.237211</td>\n",
              "      <td>6.237212</td>\n",
              "      <td>9.236919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>6.237203</td>\n",
              "      <td>6.237202</td>\n",
              "      <td>9.237023</td>\n",
              "      <td>6.237203</td>\n",
              "      <td>6.237202</td>\n",
              "      <td>9.237023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>6.237058</td>\n",
              "      <td>6.237058</td>\n",
              "      <td>9.236795</td>\n",
              "      <td>6.237058</td>\n",
              "      <td>6.237058</td>\n",
              "      <td>9.236795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>6.236535</td>\n",
              "      <td>6.236535</td>\n",
              "      <td>9.236202</td>\n",
              "      <td>6.236535</td>\n",
              "      <td>6.236535</td>\n",
              "      <td>9.236202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>6.236756</td>\n",
              "      <td>6.236756</td>\n",
              "      <td>9.236235</td>\n",
              "      <td>6.236756</td>\n",
              "      <td>6.236756</td>\n",
              "      <td>9.236235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>6.236188</td>\n",
              "      <td>6.236188</td>\n",
              "      <td>9.235653</td>\n",
              "      <td>6.236188</td>\n",
              "      <td>6.236188</td>\n",
              "      <td>9.235653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>6.236240</td>\n",
              "      <td>6.236240</td>\n",
              "      <td>9.235293</td>\n",
              "      <td>6.236240</td>\n",
              "      <td>6.236240</td>\n",
              "      <td>9.235293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>6.235983</td>\n",
              "      <td>6.235983</td>\n",
              "      <td>9.234926</td>\n",
              "      <td>6.235983</td>\n",
              "      <td>6.235983</td>\n",
              "      <td>9.234926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>6.236472</td>\n",
              "      <td>6.236472</td>\n",
              "      <td>9.235192</td>\n",
              "      <td>6.236472</td>\n",
              "      <td>6.236472</td>\n",
              "      <td>9.235192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>6.236308</td>\n",
              "      <td>6.236307</td>\n",
              "      <td>9.235200</td>\n",
              "      <td>6.236308</td>\n",
              "      <td>6.236307</td>\n",
              "      <td>9.235200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>6.236289</td>\n",
              "      <td>6.236288</td>\n",
              "      <td>9.235492</td>\n",
              "      <td>6.236289</td>\n",
              "      <td>6.236288</td>\n",
              "      <td>9.235492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>6.235991</td>\n",
              "      <td>6.235991</td>\n",
              "      <td>9.234978</td>\n",
              "      <td>6.235991</td>\n",
              "      <td>6.235991</td>\n",
              "      <td>9.234978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>6.235989</td>\n",
              "      <td>6.235989</td>\n",
              "      <td>9.234674</td>\n",
              "      <td>6.235989</td>\n",
              "      <td>6.235989</td>\n",
              "      <td>9.234674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>6.236026</td>\n",
              "      <td>6.236026</td>\n",
              "      <td>9.235059</td>\n",
              "      <td>6.236026</td>\n",
              "      <td>6.236026</td>\n",
              "      <td>9.235059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>6.235880</td>\n",
              "      <td>6.235880</td>\n",
              "      <td>9.234830</td>\n",
              "      <td>6.235880</td>\n",
              "      <td>6.235880</td>\n",
              "      <td>9.234830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>6.235555</td>\n",
              "      <td>6.235555</td>\n",
              "      <td>9.234368</td>\n",
              "      <td>6.235555</td>\n",
              "      <td>6.235555</td>\n",
              "      <td>9.234368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>6.235816</td>\n",
              "      <td>6.235816</td>\n",
              "      <td>9.234620</td>\n",
              "      <td>6.235816</td>\n",
              "      <td>6.235816</td>\n",
              "      <td>9.234620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>6.235696</td>\n",
              "      <td>6.235696</td>\n",
              "      <td>9.234282</td>\n",
              "      <td>6.235696</td>\n",
              "      <td>6.235696</td>\n",
              "      <td>9.234282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>6.235562</td>\n",
              "      <td>6.235562</td>\n",
              "      <td>9.234260</td>\n",
              "      <td>6.235562</td>\n",
              "      <td>6.235562</td>\n",
              "      <td>9.234260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>6.235886</td>\n",
              "      <td>6.235886</td>\n",
              "      <td>9.234452</td>\n",
              "      <td>6.235886</td>\n",
              "      <td>6.235886</td>\n",
              "      <td>9.234452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>6.235647</td>\n",
              "      <td>6.235647</td>\n",
              "      <td>9.234764</td>\n",
              "      <td>6.235647</td>\n",
              "      <td>6.235647</td>\n",
              "      <td>9.234764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>6.235591</td>\n",
              "      <td>6.235591</td>\n",
              "      <td>9.234439</td>\n",
              "      <td>6.235591</td>\n",
              "      <td>6.235591</td>\n",
              "      <td>9.234439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>6.235784</td>\n",
              "      <td>6.235784</td>\n",
              "      <td>9.234097</td>\n",
              "      <td>6.235784</td>\n",
              "      <td>6.235784</td>\n",
              "      <td>9.234097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>6.235727</td>\n",
              "      <td>6.235727</td>\n",
              "      <td>9.234174</td>\n",
              "      <td>6.235727</td>\n",
              "      <td>6.235727</td>\n",
              "      <td>9.234174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>6.235863</td>\n",
              "      <td>6.235863</td>\n",
              "      <td>9.234758</td>\n",
              "      <td>6.235863</td>\n",
              "      <td>6.235863</td>\n",
              "      <td>9.234758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bfb7db0-52e3-4328-b0af-fac22c2f8b0d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2bfb7db0-52e3-4328-b0af-fac22c2f8b0d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2bfb7db0-52e3-4328-b0af-fac22c2f8b0d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2621bc99-a239-48a4-9ba5-aec7fcaf9b6a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2621bc99-a239-48a4-9ba5-aec7fcaf9b6a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2621bc99-a239-48a4-9ba5-aec7fcaf9b6a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2fd8d355-6566-4782-9130-0a05ccaa2d61\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2fd8d355-6566-4782-9130-0a05ccaa2d61 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NELRB5KU33jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rhs_emau33qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6eRK32ZxW6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, num_features, num_layers=3, d_model=64, nhead=4, dim_feedforward=256, dropout=0.1):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.linear_in = nn.Linear(num_features, d_model)\n",
        "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            self.transformer_encoder_layer, num_layers=num_layers)\n",
        "        self.linear_out = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.linear_in(src)  # [batch_size, seq_len, d_model]\n",
        "        src = src.permute(1, 0, 2)  # [seq_len, batch_size, d_model]\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.permute(1, 0, 2)  # [batch_size, seq_len, d_model]\n",
        "        output = self.linear_out(output[:, -1, :])  # Use the output of the last time step\n",
        "        return output\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = TimeSeriesTransformer(num_features=X_train.shape[1]).to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "def check_nan_inf(tensor):\n",
        "    if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Check for NaN or Inf in data\n",
        "assert not check_nan_inf(X_train), \"NaN or Inf in X_train\"\n",
        "assert not check_nan_inf(y_train), \"NaN or Inf in y_train\"\n",
        "assert not check_nan_inf(X_val), \"NaN or Inf in X_val\"\n",
        "assert not check_nan_inf(y_val), \"NaN or Inf in y_val\"\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for features, targets in train_loader:\n",
        "        features, targets = features.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "\n",
        "        if torch.isnan(outputs).any():\n",
        "              print(\"NaN detected in model outputs\")\n",
        "              print(\"Features causing NaN:\", features)\n",
        "              raise ValueError(\"NaN detected in model outputs\")\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        if torch.isnan(loss).any():\n",
        "            raise ValueError(\"NaN detected in loss\")\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in val_loader:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "            outputs = model(features)\n",
        "            val_loss += loss_function(outputs, targets).item()\n",
        "\n",
        "            if torch.isnan(outputs).any():\n",
        "              print(\"NaN detected in model outputs\")\n",
        "              print(\"Features causing NaN:\", features)\n",
        "              raise ValueError(\"NaN detected in model outputs\")\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Validation Loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "jiIOOPeojjYm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "791347bb-52e9-4749-ea99-c8fb8144c35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN detected in model outputs\n",
            "Features causing NaN: tensor([[[1.1200e+02, 3.9000e+02, 5.3307e+06,  ..., 2.0001e+00,\n",
            "          4.9149e-03, 2.0947e-02]],\n",
            "\n",
            "        [[1.4200e+02, 5.1000e+02, 1.4141e+06,  ..., 2.0001e+00,\n",
            "          6.4599e-03, 5.5993e-02]],\n",
            "\n",
            "        [[5.4000e+01, 4.2000e+02, 3.9293e+06,  ..., 1.9999e+00,\n",
            "          5.1270e-03, 1.9226e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[7.2000e+01, 4.9000e+02, 4.3639e+05,  ..., 1.9998e+00,\n",
            "          5.4252e-03, 1.6973e-02]],\n",
            "\n",
            "        [[1.1600e+02, 4.1000e+02, 5.9743e+06,  ..., 1.9995e+00,\n",
            "          4.6003e-03, 1.9872e-02]],\n",
            "\n",
            "        [[1.1000e+02, 4.5000e+02, 1.4784e+07,  ..., 1.9991e+00,\n",
            "          4.1548e-03, 1.9975e-02]]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-fd40b830b6d3>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaN detected in model outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Features causing NaN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaN detected in model outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: NaN detected in model outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oDCy-n65jjbQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}