{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AMrXZ0k3j0_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJuQWPOfjov0"
      },
      "outputs": [],
      "source": [
        "# a simplified version of DeepAR model\n",
        "# https://arxiv.org/pdf/1704.04110.pdf\n",
        "\n",
        "# references\n",
        "# LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "# deepAR: https://github.com/zhykoties/TimeSeries/blob/master/model/net.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5eAq8yCjov2"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "df = pd.read_csv('drive/MyDrive/kaggle/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXJNnWLljov3"
      },
      "outputs": [],
      "source": [
        "# data preprocessing\n",
        "\n",
        "# drop row with nan values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "features_df = df.drop([\"target\", \"row_id\", \"time_id\"], axis=1)\n",
        "target_df = df['target']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcbTkct1jov3"
      },
      "outputs": [],
      "source": [
        "# train test split, dataloader\n",
        "batch_size = 1024\n",
        "\n",
        "features_tensor = torch.tensor(features_scaled, dtype=torch.float32)\n",
        "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB30DbJejov3"
      },
      "outputs": [],
      "source": [
        "# simplified DeepAR model\n",
        "\n",
        "class DeepAR(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, lstm_layers=2, device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')):\n",
        "        super(DeepAR, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.device = device\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=self.lstm_layers, batch_first=True)\n",
        "\n",
        "        self.distribution_mu = nn.Linear(hidden_size * self.lstm_layers, 1)\n",
        "        self.distribution_presigma = nn.Linear(hidden_size * self.lstm_layers, 1)\n",
        "        self.distribution_sigma = nn.Softplus()     # make sure sigma is positive\n",
        "\n",
        "    '''\n",
        "        x: (batch_size, seq_len, input_size)\n",
        "        hidden_state: (num_layers, batch_size, hidden_size)\n",
        "        cell_state: (num_layers, batch_size, hidden_size)\n",
        "    '''\n",
        "    def forward(self, x, hidden_state=None, cell_state=None):\n",
        "        x = x.unsqueeze(1) # seq len is 1\n",
        "\n",
        "        if hidden_state is None:\n",
        "            hidden_state = self.init_hidden(x.shape[0])\n",
        "        if cell_state is None:\n",
        "            cell_state = self.init_cell(x.shape[0])\n",
        "\n",
        "        lstm_out, (hidden, cell) = self.lstm(x, (hidden_state, cell_state))\n",
        "        hidden_permute = hidden.permute(1, 0, 2).contiguous().view(hidden.shape[1], -1)\n",
        "\n",
        "        # Predicting mu and sigma\n",
        "        mu = self.distribution_mu(hidden_permute)\n",
        "        pre_sigma = self.distribution_presigma(hidden_permute)\n",
        "        sigma = self.distribution_sigma(pre_sigma)\n",
        "\n",
        "        return mu, sigma, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.lstm_layers, batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "    def init_cell(self, batch_size):\n",
        "        return torch.zeros(self.lstm_layers, batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "# gaussian log likelihood loss\n",
        "def gaussian_likelihood_loss(mu, sigma, y):\n",
        "    return torch.mean(0.5 * torch.log(sigma**2) + 0.5 * ((y - mu) / sigma)**2)\n",
        "\n",
        "# mae\n",
        "def mae(predictions, targets):\n",
        "    return torch.mean(torch.abs(predictions - targets))\n",
        "\n",
        "# mse\n",
        "def mse(predictions, targets):\n",
        "    return torch.mean((predictions - targets) ** 2)\n",
        "\n",
        "# rmse\n",
        "def rmse(predictions, targets):\n",
        "    return torch.sqrt(mse(predictions, targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITLaC5enjov4",
        "outputId": "cacd79b0-9321-4baa-95f8-84c491967dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Step 1, Loss: 75.4194\n",
            "Epoch 1, Step 101, Loss: 71.5931\n",
            "Epoch 1, Step 201, Loss: 67.1535\n",
            "Epoch 1, Step 301, Loss: 73.2590\n",
            "Epoch 1, Step 401, Loss: 65.5580\n",
            "Epoch 1, Step 501, Loss: 70.3176\n",
            "Epoch 1, Step 601, Loss: 66.9521\n",
            "Epoch 1, Step 701, Loss: 70.4690\n",
            "Epoch 1, Step 801, Loss: 70.6991\n",
            "Epoch 1, Step 901, Loss: 65.9483\n",
            "Epoch 1, Step 1001, Loss: 74.3493\n",
            "Epoch 1, Step 1101, Loss: 70.3176\n",
            "Epoch 1, Step 1201, Loss: 75.2050\n",
            "Epoch 1, Step 1301, Loss: 52.6095\n",
            "Epoch 1, Step 1401, Loss: 81.4270\n",
            "Epoch 1, Step 1501, Loss: 66.6275\n",
            "Epoch 1, Step 1601, Loss: 64.2615\n",
            "Epoch 1, Step 1701, Loss: 56.7750\n",
            "Epoch 1, Step 1801, Loss: 68.9637\n",
            "Epoch 1, Loss: 70.5462, MAE: 5.7093, MSE: 70.2121, RMSE: 8.3527\n",
            "Epoch 2, Step 1, Loss: 76.0803\n",
            "Epoch 2, Step 101, Loss: 63.1941\n",
            "Epoch 2, Step 201, Loss: 62.3890\n",
            "Epoch 2, Step 301, Loss: 66.7512\n",
            "Epoch 2, Step 401, Loss: 59.4048\n",
            "Epoch 2, Step 501, Loss: 56.2814\n",
            "Epoch 2, Step 601, Loss: 55.6256\n",
            "Epoch 2, Step 701, Loss: 56.9672\n",
            "Epoch 2, Step 801, Loss: 63.1124\n",
            "Epoch 2, Step 901, Loss: 58.6226\n",
            "Epoch 2, Step 1001, Loss: 58.6326\n",
            "Epoch 2, Step 1101, Loss: 51.1895\n",
            "Epoch 2, Step 1201, Loss: 47.4707\n",
            "Epoch 2, Step 1301, Loss: 51.7646\n",
            "Epoch 2, Step 1401, Loss: 51.5582\n",
            "Epoch 2, Step 1501, Loss: 65.8865\n",
            "Epoch 2, Step 1601, Loss: 58.2070\n",
            "Epoch 2, Step 1701, Loss: 56.9754\n",
            "Epoch 2, Step 1801, Loss: 51.4552\n",
            "Epoch 2, Loss: 59.7191, MAE: 5.7090, MSE: 70.2077, RMSE: 8.3524\n",
            "Epoch 3, Step 1, Loss: 56.8902\n",
            "Epoch 3, Step 101, Loss: 48.3096\n",
            "Epoch 3, Step 201, Loss: 47.6294\n",
            "Epoch 3, Step 301, Loss: 52.6080\n",
            "Epoch 3, Step 401, Loss: 54.5855\n",
            "Epoch 3, Step 501, Loss: 46.3116\n",
            "Epoch 3, Step 601, Loss: 41.3750\n",
            "Epoch 3, Step 701, Loss: 46.1616\n",
            "Epoch 3, Step 801, Loss: 50.6425\n",
            "Epoch 3, Step 901, Loss: 47.4812\n",
            "Epoch 3, Step 1001, Loss: 45.8432\n",
            "Epoch 3, Step 1101, Loss: 46.4637\n",
            "Epoch 3, Step 1201, Loss: 38.5074\n",
            "Epoch 3, Step 1301, Loss: 39.6477\n",
            "Epoch 3, Step 1401, Loss: 41.1693\n",
            "Epoch 3, Step 1501, Loss: 47.6316\n",
            "Epoch 3, Step 1601, Loss: 37.8375\n",
            "Epoch 3, Step 1701, Loss: 39.8832\n",
            "Epoch 3, Step 1801, Loss: 51.9311\n",
            "Epoch 3, Loss: 48.0304, MAE: 5.7085, MSE: 70.2000, RMSE: 8.3517\n",
            "Epoch 4, Step 1, Loss: 38.5183\n",
            "Epoch 4, Step 101, Loss: 39.1364\n",
            "Epoch 4, Step 201, Loss: 57.1152\n",
            "Epoch 4, Step 301, Loss: 43.3692\n",
            "Epoch 4, Step 401, Loss: 34.1418\n",
            "Epoch 4, Step 501, Loss: 42.5233\n",
            "Epoch 4, Step 601, Loss: 38.4827\n",
            "Epoch 4, Step 701, Loss: 36.6684\n",
            "Epoch 4, Step 801, Loss: 38.8175\n",
            "Epoch 4, Step 901, Loss: 35.4963\n",
            "Epoch 4, Step 1001, Loss: 35.1862\n",
            "Epoch 4, Step 1101, Loss: 36.0635\n",
            "Epoch 4, Step 1201, Loss: 27.5088\n",
            "Epoch 4, Step 1301, Loss: 40.1249\n",
            "Epoch 4, Step 1401, Loss: 35.1544\n",
            "Epoch 4, Step 1501, Loss: 43.3174\n",
            "Epoch 4, Step 1601, Loss: 46.2519\n",
            "Epoch 4, Step 1701, Loss: 30.2818\n",
            "Epoch 4, Step 1801, Loss: 31.5871\n",
            "Epoch 4, Loss: 36.2808, MAE: 5.7078, MSE: 70.1905, RMSE: 8.3519\n",
            "Epoch 5, Step 1, Loss: 30.2330\n",
            "Epoch 5, Step 101, Loss: 32.0425\n",
            "Epoch 5, Step 201, Loss: 28.5120\n",
            "Epoch 5, Step 301, Loss: 26.9808\n",
            "Epoch 5, Step 401, Loss: 28.7063\n",
            "Epoch 5, Step 501, Loss: 26.9292\n",
            "Epoch 5, Step 601, Loss: 26.8537\n",
            "Epoch 5, Step 701, Loss: 24.4131\n",
            "Epoch 5, Step 801, Loss: 28.7001\n",
            "Epoch 5, Step 901, Loss: 26.6931\n",
            "Epoch 5, Step 1001, Loss: 24.2252\n",
            "Epoch 5, Step 1101, Loss: 20.6353\n",
            "Epoch 5, Step 1201, Loss: 24.4918\n",
            "Epoch 5, Step 1301, Loss: 24.4429\n",
            "Epoch 5, Step 1401, Loss: 20.5507\n",
            "Epoch 5, Step 1501, Loss: 20.7379\n",
            "Epoch 5, Step 1601, Loss: 22.1660\n",
            "Epoch 5, Step 1701, Loss: 22.2252\n",
            "Epoch 5, Step 1801, Loss: 18.3941\n",
            "Epoch 5, Loss: 25.9110, MAE: 5.7074, MSE: 70.1844, RMSE: 8.3510\n",
            "Epoch 6, Step 1, Loss: 21.5018\n",
            "Epoch 6, Step 101, Loss: 23.4745\n",
            "Epoch 6, Step 201, Loss: 21.7124\n",
            "Epoch 6, Step 301, Loss: 20.6964\n",
            "Epoch 6, Step 401, Loss: 17.3840\n",
            "Epoch 6, Step 501, Loss: 18.6778\n",
            "Epoch 6, Step 601, Loss: 17.2739\n",
            "Epoch 6, Step 701, Loss: 17.5244\n",
            "Epoch 6, Step 801, Loss: 17.6485\n",
            "Epoch 6, Step 901, Loss: 15.0904\n",
            "Epoch 6, Step 1001, Loss: 15.2653\n",
            "Epoch 6, Step 1101, Loss: 16.5549\n",
            "Epoch 6, Step 1201, Loss: 17.1208\n",
            "Epoch 6, Step 1301, Loss: 16.2782\n",
            "Epoch 6, Step 1401, Loss: 15.8301\n",
            "Epoch 6, Step 1501, Loss: 16.8650\n",
            "Epoch 6, Step 1601, Loss: 16.0432\n",
            "Epoch 6, Step 1701, Loss: 15.1308\n",
            "Epoch 6, Step 1801, Loss: 14.6312\n",
            "Epoch 6, Loss: 17.8864, MAE: 5.7072, MSE: 70.1827, RMSE: 8.3509\n",
            "Epoch 7, Step 1, Loss: 13.8520\n",
            "Epoch 7, Step 101, Loss: 13.3649\n",
            "Epoch 7, Step 201, Loss: 14.5849\n",
            "Epoch 7, Step 301, Loss: 11.3458\n",
            "Epoch 7, Step 401, Loss: 13.5018\n",
            "Epoch 7, Step 501, Loss: 13.6722\n",
            "Epoch 7, Step 601, Loss: 11.9829\n",
            "Epoch 7, Step 701, Loss: 12.9094\n",
            "Epoch 7, Step 801, Loss: 10.5854\n",
            "Epoch 7, Step 901, Loss: 12.4733\n",
            "Epoch 7, Step 1001, Loss: 11.3526\n",
            "Epoch 7, Step 1101, Loss: 10.2609\n",
            "Epoch 7, Step 1201, Loss: 10.5176\n",
            "Epoch 7, Step 1301, Loss: 11.5318\n",
            "Epoch 7, Step 1401, Loss: 9.4323\n",
            "Epoch 7, Step 1501, Loss: 13.6189\n",
            "Epoch 7, Step 1601, Loss: 9.6765\n",
            "Epoch 7, Step 1701, Loss: 10.6027\n",
            "Epoch 7, Step 1801, Loss: 9.5436\n",
            "Epoch 7, Loss: 12.2895, MAE: 5.7072, MSE: 70.1828, RMSE: 8.3513\n",
            "Epoch 8, Step 1, Loss: 8.5157\n",
            "Epoch 8, Step 101, Loss: 9.0727\n",
            "Epoch 8, Step 201, Loss: 8.3784\n",
            "Epoch 8, Step 301, Loss: 8.7341\n",
            "Epoch 8, Step 401, Loss: 8.6956\n",
            "Epoch 8, Step 501, Loss: 8.6017\n",
            "Epoch 8, Step 601, Loss: 10.8661\n",
            "Epoch 8, Step 701, Loss: 8.1202\n",
            "Epoch 8, Step 801, Loss: 10.1836\n",
            "Epoch 8, Step 901, Loss: 7.3846\n",
            "Epoch 8, Step 1001, Loss: 7.2211\n",
            "Epoch 8, Step 1101, Loss: 8.0850\n",
            "Epoch 8, Step 1201, Loss: 7.4667\n",
            "Epoch 8, Step 1301, Loss: 9.3919\n",
            "Epoch 8, Step 1401, Loss: 6.7873\n",
            "Epoch 8, Step 1501, Loss: 8.0474\n",
            "Epoch 8, Step 1601, Loss: 7.5300\n",
            "Epoch 8, Step 1701, Loss: 9.7709\n",
            "Epoch 8, Step 1801, Loss: 7.4369\n",
            "Epoch 8, Loss: 8.6449, MAE: 5.7073, MSE: 70.1830, RMSE: 8.3505\n",
            "Epoch 9, Step 1, Loss: 6.3134\n",
            "Epoch 9, Step 101, Loss: 7.1371\n",
            "Epoch 9, Step 201, Loss: 6.6202\n",
            "Epoch 9, Step 301, Loss: 6.6834\n",
            "Epoch 9, Step 401, Loss: 7.5123\n",
            "Epoch 9, Step 501, Loss: 7.4963\n",
            "Epoch 9, Step 601, Loss: 6.1168\n",
            "Epoch 9, Step 701, Loss: 5.9322\n",
            "Epoch 9, Step 801, Loss: 5.3824\n",
            "Epoch 9, Step 901, Loss: 6.1443\n",
            "Epoch 9, Step 1001, Loss: 5.8231\n",
            "Epoch 9, Step 1101, Loss: 6.1528\n",
            "Epoch 9, Step 1201, Loss: 6.2094\n",
            "Epoch 9, Step 1301, Loss: 5.6978\n",
            "Epoch 9, Step 1401, Loss: 5.7291\n",
            "Epoch 9, Step 1501, Loss: 5.6980\n",
            "Epoch 9, Step 1601, Loss: 6.0442\n",
            "Epoch 9, Step 1701, Loss: 5.5083\n",
            "Epoch 9, Step 1801, Loss: 5.8230\n",
            "Epoch 9, Loss: 6.3431, MAE: 5.7072, MSE: 70.1828, RMSE: 8.3517\n",
            "Epoch 10, Step 1, Loss: 5.7611\n",
            "Epoch 10, Step 101, Loss: 4.7831\n",
            "Epoch 10, Step 201, Loss: 4.8767\n",
            "Epoch 10, Step 301, Loss: 4.8285\n",
            "Epoch 10, Step 401, Loss: 5.0299\n",
            "Epoch 10, Step 501, Loss: 4.8576\n",
            "Epoch 10, Step 601, Loss: 5.0706\n",
            "Epoch 10, Step 701, Loss: 4.9294\n",
            "Epoch 10, Step 801, Loss: 4.5741\n",
            "Epoch 10, Step 901, Loss: 4.7506\n",
            "Epoch 10, Step 1001, Loss: 4.8933\n",
            "Epoch 10, Step 1101, Loss: 4.9253\n",
            "Epoch 10, Step 1201, Loss: 4.6414\n",
            "Epoch 10, Step 1301, Loss: 4.6839\n",
            "Epoch 10, Step 1401, Loss: 4.5408\n",
            "Epoch 10, Step 1501, Loss: 4.3094\n",
            "Epoch 10, Step 1601, Loss: 4.9947\n",
            "Epoch 10, Step 1701, Loss: 4.2431\n",
            "Epoch 10, Step 1801, Loss: 4.2840\n",
            "Epoch 10, Loss: 4.9067, MAE: 5.7072, MSE: 70.1823, RMSE: 8.3511\n",
            "Epoch 11, Step 1, Loss: 4.4500\n",
            "Epoch 11, Step 101, Loss: 4.0669\n",
            "Epoch 11, Step 201, Loss: 4.8073\n",
            "Epoch 11, Step 301, Loss: 4.8165\n",
            "Epoch 11, Step 401, Loss: 4.0271\n",
            "Epoch 11, Step 501, Loss: 4.3964\n",
            "Epoch 11, Step 601, Loss: 4.0973\n",
            "Epoch 11, Step 701, Loss: 4.3383\n",
            "Epoch 11, Step 801, Loss: 3.8348\n",
            "Epoch 11, Step 901, Loss: 3.7703\n",
            "Epoch 11, Step 1001, Loss: 3.5614\n",
            "Epoch 11, Step 1101, Loss: 4.2145\n",
            "Epoch 11, Step 1201, Loss: 4.0315\n",
            "Epoch 11, Step 1301, Loss: 3.8721\n",
            "Epoch 11, Step 1401, Loss: 3.7640\n",
            "Epoch 11, Step 1501, Loss: 3.2846\n",
            "Epoch 11, Step 1601, Loss: 3.8831\n",
            "Epoch 11, Step 1701, Loss: 3.5898\n",
            "Epoch 11, Step 1801, Loss: 3.7887\n",
            "Epoch 11, Loss: 4.0152, MAE: 5.7072, MSE: 70.1821, RMSE: 8.3510\n",
            "Epoch 12, Step 1, Loss: 3.4081\n",
            "Epoch 12, Step 101, Loss: 3.7004\n",
            "Epoch 12, Step 201, Loss: 3.5568\n",
            "Epoch 12, Step 301, Loss: 3.9459\n",
            "Epoch 12, Step 401, Loss: 3.4236\n",
            "Epoch 12, Step 501, Loss: 3.2524\n",
            "Epoch 12, Step 601, Loss: 3.6174\n",
            "Epoch 12, Step 701, Loss: 3.4172\n",
            "Epoch 12, Step 801, Loss: 3.4223\n",
            "Epoch 12, Step 901, Loss: 3.1716\n",
            "Epoch 12, Step 1001, Loss: 3.2168\n",
            "Epoch 12, Step 1101, Loss: 3.2071\n",
            "Epoch 12, Step 1201, Loss: 3.1467\n",
            "Epoch 12, Step 1301, Loss: 3.6273\n",
            "Epoch 12, Step 1401, Loss: 3.9141\n",
            "Epoch 12, Step 1501, Loss: 3.2481\n",
            "Epoch 12, Step 1601, Loss: 3.4380\n",
            "Epoch 12, Step 1701, Loss: 3.5090\n",
            "Epoch 12, Step 1801, Loss: 3.3956\n",
            "Epoch 12, Loss: 3.4619, MAE: 5.7072, MSE: 70.1820, RMSE: 8.3508\n",
            "Epoch 13, Step 1, Loss: 3.0855\n",
            "Epoch 13, Step 101, Loss: 2.7942\n",
            "Epoch 13, Step 201, Loss: 2.9977\n",
            "Epoch 13, Step 301, Loss: 3.1803\n",
            "Epoch 13, Step 401, Loss: 3.0544\n",
            "Epoch 13, Step 501, Loss: 3.0129\n",
            "Epoch 13, Step 601, Loss: 3.5234\n",
            "Epoch 13, Step 701, Loss: 3.0149\n",
            "Epoch 13, Step 801, Loss: 2.9262\n",
            "Epoch 13, Step 901, Loss: 3.1737\n",
            "Epoch 13, Step 1001, Loss: 3.0534\n",
            "Epoch 13, Step 1101, Loss: 2.7410\n",
            "Epoch 13, Step 1201, Loss: 3.0185\n",
            "Epoch 13, Step 1301, Loss: 2.8550\n",
            "Epoch 13, Step 1401, Loss: 3.1294\n",
            "Epoch 13, Step 1501, Loss: 3.0066\n",
            "Epoch 13, Step 1601, Loss: 3.3208\n",
            "Epoch 13, Step 1701, Loss: 2.9029\n",
            "Epoch 13, Step 1801, Loss: 2.8785\n",
            "Epoch 13, Loss: 3.1195, MAE: 5.7072, MSE: 70.1822, RMSE: 8.3504\n",
            "Epoch 14, Step 1, Loss: 2.7490\n",
            "Epoch 14, Step 101, Loss: 2.8093\n",
            "Epoch 14, Step 201, Loss: 3.2764\n",
            "Epoch 14, Step 301, Loss: 2.8839\n",
            "Epoch 14, Step 401, Loss: 2.8934\n",
            "Epoch 14, Step 501, Loss: 4.8433\n",
            "Epoch 14, Step 601, Loss: 3.9629\n",
            "Epoch 14, Step 701, Loss: 2.7847\n",
            "Epoch 14, Step 801, Loss: 2.8912\n",
            "Epoch 14, Step 901, Loss: 3.0410\n",
            "Epoch 14, Step 1001, Loss: 2.8484\n",
            "Epoch 14, Step 1101, Loss: 2.9404\n",
            "Epoch 14, Step 1201, Loss: 2.6785\n",
            "Epoch 14, Step 1301, Loss: 2.7497\n",
            "Epoch 14, Step 1401, Loss: 3.2052\n",
            "Epoch 14, Step 1501, Loss: 2.6243\n",
            "Epoch 14, Step 1601, Loss: 2.8547\n",
            "Epoch 14, Step 1701, Loss: 2.9244\n",
            "Epoch 14, Step 1801, Loss: 2.9650\n",
            "Epoch 14, Loss: 2.9068, MAE: 5.7072, MSE: 70.1821, RMSE: 8.3508\n",
            "Epoch 15, Step 1, Loss: 3.0802\n",
            "Epoch 15, Step 101, Loss: 2.8320\n",
            "Epoch 15, Step 201, Loss: 2.8297\n",
            "Epoch 15, Step 301, Loss: 2.7903\n",
            "Epoch 15, Step 401, Loss: 2.7615\n",
            "Epoch 15, Step 501, Loss: 2.6521\n",
            "Epoch 15, Step 601, Loss: 2.7855\n",
            "Epoch 15, Step 701, Loss: 2.6740\n",
            "Epoch 15, Step 801, Loss: 2.7049\n",
            "Epoch 15, Step 901, Loss: 2.7773\n",
            "Epoch 15, Step 1001, Loss: 2.7525\n",
            "Epoch 15, Step 1101, Loss: 2.7468\n",
            "Epoch 15, Step 1201, Loss: 2.5895\n",
            "Epoch 15, Step 1301, Loss: 2.6623\n",
            "Epoch 15, Step 1401, Loss: 2.6603\n",
            "Epoch 15, Step 1501, Loss: 2.7603\n",
            "Epoch 15, Step 1601, Loss: 2.7914\n",
            "Epoch 15, Step 1701, Loss: 2.5918\n",
            "Epoch 15, Step 1801, Loss: 2.8673\n",
            "Epoch 15, Loss: 2.7780, MAE: 5.7072, MSE: 70.1820, RMSE: 8.3506\n",
            "Epoch 16, Step 1, Loss: 2.6994\n",
            "Epoch 16, Step 101, Loss: 2.5662\n",
            "Epoch 16, Step 201, Loss: 2.5898\n",
            "Epoch 16, Step 301, Loss: 2.6716\n",
            "Epoch 16, Step 401, Loss: 2.6508\n",
            "Epoch 16, Step 501, Loss: 3.0161\n",
            "Epoch 16, Step 601, Loss: 2.7730\n",
            "Epoch 16, Step 701, Loss: 2.6803\n",
            "Epoch 16, Step 801, Loss: 2.5883\n",
            "Epoch 16, Step 901, Loss: 2.6427\n",
            "Epoch 16, Step 1001, Loss: 2.6633\n",
            "Epoch 16, Step 1101, Loss: 2.8180\n",
            "Epoch 16, Step 1201, Loss: 2.6477\n",
            "Epoch 16, Step 1301, Loss: 2.6307\n",
            "Epoch 16, Step 1401, Loss: 2.6931\n",
            "Epoch 16, Step 1501, Loss: 2.7815\n",
            "Epoch 16, Step 1601, Loss: 2.5962\n",
            "Epoch 16, Step 1701, Loss: 2.6330\n",
            "Epoch 16, Step 1801, Loss: 2.7180\n",
            "Epoch 16, Loss: 2.7020, MAE: 5.7072, MSE: 70.1820, RMSE: 8.3507\n",
            "Epoch 17, Step 1, Loss: 2.5532\n",
            "Epoch 17, Step 101, Loss: 2.6352\n",
            "Epoch 17, Step 201, Loss: 2.6077\n",
            "Epoch 17, Step 301, Loss: 2.6123\n",
            "Epoch 17, Step 401, Loss: 2.6545\n",
            "Epoch 17, Step 501, Loss: 2.6557\n",
            "Epoch 17, Step 601, Loss: 2.8479\n",
            "Epoch 17, Step 701, Loss: 2.6496\n",
            "Epoch 17, Step 801, Loss: 2.6734\n",
            "Epoch 17, Step 901, Loss: 2.7127\n",
            "Epoch 17, Step 1001, Loss: 2.6530\n",
            "Epoch 17, Step 1101, Loss: 2.5736\n",
            "Epoch 17, Step 1201, Loss: 2.6864\n",
            "Epoch 17, Step 1301, Loss: 2.9591\n",
            "Epoch 17, Step 1401, Loss: 2.6069\n",
            "Epoch 17, Step 1501, Loss: 2.7988\n",
            "Epoch 17, Step 1601, Loss: 2.6812\n",
            "Epoch 17, Step 1701, Loss: 2.7099\n",
            "Epoch 17, Step 1801, Loss: 3.3802\n",
            "Epoch 17, Loss: 2.6606, MAE: 5.7072, MSE: 70.1824, RMSE: 8.3518\n",
            "Epoch 18, Step 1, Loss: 2.6199\n",
            "Epoch 18, Step 101, Loss: 2.6614\n",
            "Epoch 18, Step 201, Loss: 2.6209\n",
            "Epoch 18, Step 301, Loss: 2.5463\n",
            "Epoch 18, Step 401, Loss: 2.5587\n",
            "Epoch 18, Step 501, Loss: 2.9339\n",
            "Epoch 18, Step 601, Loss: 2.6330\n",
            "Epoch 18, Step 701, Loss: 2.6352\n",
            "Epoch 18, Step 801, Loss: 2.6809\n",
            "Epoch 18, Step 901, Loss: 2.5387\n",
            "Epoch 18, Step 1001, Loss: 2.6554\n",
            "Epoch 18, Step 1101, Loss: 2.6210\n",
            "Epoch 18, Step 1201, Loss: 2.7054\n",
            "Epoch 18, Step 1301, Loss: 2.6637\n",
            "Epoch 18, Step 1401, Loss: 2.5661\n",
            "Epoch 18, Step 1501, Loss: 2.5886\n",
            "Epoch 18, Step 1601, Loss: 2.6403\n",
            "Epoch 18, Step 1701, Loss: 2.6009\n",
            "Epoch 18, Step 1801, Loss: 2.6336\n",
            "Epoch 18, Loss: 2.6405, MAE: 5.7072, MSE: 70.1822, RMSE: 8.3513\n",
            "Epoch 19, Step 1, Loss: 2.6071\n",
            "Epoch 19, Step 101, Loss: 2.5796\n",
            "Epoch 19, Step 201, Loss: 2.5301\n",
            "Epoch 19, Step 301, Loss: 2.6442\n",
            "Epoch 19, Step 401, Loss: 2.5770\n",
            "Epoch 19, Step 501, Loss: 2.6578\n",
            "Epoch 19, Step 601, Loss: 2.5816\n",
            "Epoch 19, Step 701, Loss: 3.0084\n",
            "Epoch 19, Step 801, Loss: 2.5911\n",
            "Epoch 19, Step 901, Loss: 2.7014\n",
            "Epoch 19, Step 1001, Loss: 2.5971\n",
            "Epoch 19, Step 1101, Loss: 2.5085\n",
            "Epoch 19, Step 1201, Loss: 3.1402\n",
            "Epoch 19, Step 1301, Loss: 2.6062\n",
            "Epoch 19, Step 1401, Loss: 2.7033\n",
            "Epoch 19, Step 1501, Loss: 2.5515\n",
            "Epoch 19, Step 1601, Loss: 2.5781\n",
            "Epoch 19, Step 1701, Loss: 2.6023\n",
            "Epoch 19, Step 1801, Loss: 2.7062\n",
            "Epoch 19, Loss: 2.6322, MAE: 5.7072, MSE: 70.1822, RMSE: 8.3508\n",
            "Epoch 20, Step 1, Loss: 2.5840\n",
            "Epoch 20, Step 101, Loss: 2.7931\n",
            "Epoch 20, Step 201, Loss: 2.6247\n",
            "Epoch 20, Step 301, Loss: 2.6642\n",
            "Epoch 20, Step 401, Loss: 2.5775\n",
            "Epoch 20, Step 501, Loss: 2.6023\n",
            "Epoch 20, Step 601, Loss: 2.6701\n",
            "Epoch 20, Step 701, Loss: 2.5840\n",
            "Epoch 20, Step 801, Loss: 2.6116\n",
            "Epoch 20, Step 901, Loss: 2.6591\n",
            "Epoch 20, Step 1001, Loss: 2.6688\n",
            "Epoch 20, Step 1101, Loss: 2.6400\n",
            "Epoch 20, Step 1201, Loss: 3.0348\n",
            "Epoch 20, Step 1301, Loss: 2.6135\n",
            "Epoch 20, Step 1401, Loss: 2.5901\n",
            "Epoch 20, Step 1501, Loss: 2.6292\n",
            "Epoch 20, Step 1601, Loss: 2.5762\n",
            "Epoch 20, Step 1701, Loss: 2.6232\n",
            "Epoch 20, Step 1801, Loss: 2.5708\n",
            "Epoch 20, Loss: 2.6296, MAE: 5.7072, MSE: 70.1822, RMSE: 8.3508\n",
            "Epoch 21, Step 1, Loss: 2.6389\n",
            "Epoch 21, Step 101, Loss: 2.5894\n",
            "Epoch 21, Step 201, Loss: 2.6940\n",
            "Epoch 21, Step 301, Loss: 2.5784\n",
            "Epoch 21, Step 401, Loss: 2.5102\n",
            "Epoch 21, Step 501, Loss: 2.6456\n",
            "Epoch 21, Step 601, Loss: 2.6267\n",
            "Epoch 21, Step 701, Loss: 2.5617\n",
            "Epoch 21, Step 801, Loss: 2.5143\n",
            "Epoch 21, Step 901, Loss: 2.5810\n",
            "Epoch 21, Step 1001, Loss: 2.6689\n",
            "Epoch 21, Step 1101, Loss: 2.6647\n",
            "Epoch 21, Step 1201, Loss: 2.7363\n",
            "Epoch 21, Step 1301, Loss: 2.5995\n",
            "Epoch 21, Step 1401, Loss: 2.7074\n",
            "Epoch 21, Step 1501, Loss: 2.6445\n",
            "Epoch 21, Step 1601, Loss: 2.5975\n",
            "Epoch 21, Step 1701, Loss: 2.5902\n",
            "Epoch 21, Step 1801, Loss: 2.6292\n",
            "Epoch 21, Loss: 2.6287, MAE: 5.7072, MSE: 70.1826, RMSE: 8.3508\n",
            "Epoch 22, Step 1, Loss: 2.5304\n",
            "Epoch 22, Step 101, Loss: 2.6921\n",
            "Epoch 22, Step 201, Loss: 2.5298\n",
            "Epoch 22, Step 301, Loss: 2.5520\n",
            "Epoch 22, Step 401, Loss: 2.6813\n",
            "Epoch 22, Step 501, Loss: 2.6810\n",
            "Epoch 22, Step 601, Loss: 2.6832\n",
            "Epoch 22, Step 701, Loss: 2.5820\n",
            "Epoch 22, Step 801, Loss: 2.7604\n",
            "Epoch 22, Step 901, Loss: 2.6434\n",
            "Epoch 22, Step 1001, Loss: 2.6078\n",
            "Epoch 22, Step 1101, Loss: 2.5902\n",
            "Epoch 22, Step 1201, Loss: 2.7199\n",
            "Epoch 22, Step 1301, Loss: 2.6220\n",
            "Epoch 22, Step 1401, Loss: 2.6179\n",
            "Epoch 22, Step 1501, Loss: 2.7155\n",
            "Epoch 22, Step 1601, Loss: 2.8297\n",
            "Epoch 22, Step 1701, Loss: 2.5990\n",
            "Epoch 22, Step 1801, Loss: 2.5421\n",
            "Epoch 22, Loss: 2.6282, MAE: 5.7072, MSE: 70.1822, RMSE: 8.3508\n",
            "Epoch 23, Step 1, Loss: 2.6318\n",
            "Epoch 23, Step 101, Loss: 2.6693\n",
            "Epoch 23, Step 201, Loss: 2.5514\n",
            "Epoch 23, Step 301, Loss: 2.5328\n",
            "Epoch 23, Step 401, Loss: 2.8813\n",
            "Epoch 23, Step 501, Loss: 2.5070\n",
            "Epoch 23, Step 601, Loss: 2.5722\n",
            "Epoch 23, Step 701, Loss: 2.5758\n",
            "Epoch 23, Step 801, Loss: 2.5139\n",
            "Epoch 23, Step 901, Loss: 2.5473\n",
            "Epoch 23, Step 1001, Loss: 2.5870\n",
            "Epoch 23, Step 1101, Loss: 2.6543\n",
            "Epoch 23, Step 1201, Loss: 2.6389\n",
            "Epoch 23, Step 1301, Loss: 2.6410\n",
            "Epoch 23, Step 1401, Loss: 2.6472\n",
            "Epoch 23, Step 1501, Loss: 2.6242\n",
            "Epoch 23, Step 1601, Loss: 2.5943\n",
            "Epoch 23, Step 1701, Loss: 2.5718\n",
            "Epoch 23, Step 1801, Loss: 2.5816\n",
            "Epoch 23, Loss: 2.6279, MAE: 5.7072, MSE: 70.1820, RMSE: 8.3511\n",
            "Epoch 24, Step 1, Loss: 2.5623\n",
            "Epoch 24, Step 101, Loss: 2.5870\n",
            "Epoch 24, Step 201, Loss: 2.5691\n",
            "Epoch 24, Step 301, Loss: 2.7814\n",
            "Epoch 24, Step 401, Loss: 2.5906\n",
            "Epoch 24, Step 501, Loss: 2.6012\n",
            "Epoch 24, Step 601, Loss: 2.6119\n",
            "Epoch 24, Step 701, Loss: 2.5293\n",
            "Epoch 24, Step 801, Loss: 2.5764\n",
            "Epoch 24, Step 901, Loss: 2.5791\n",
            "Epoch 24, Step 1001, Loss: 2.6157\n",
            "Epoch 24, Step 1101, Loss: 2.6218\n",
            "Epoch 24, Step 1201, Loss: 2.5763\n",
            "Epoch 24, Step 1301, Loss: 2.6637\n",
            "Epoch 24, Step 1401, Loss: 2.7043\n",
            "Epoch 24, Step 1501, Loss: 2.6325\n",
            "Epoch 24, Step 1601, Loss: 2.5477\n",
            "Epoch 24, Step 1701, Loss: 2.6429\n",
            "Epoch 24, Step 1801, Loss: 2.6032\n",
            "Epoch 24, Loss: 2.6276, MAE: 5.7072, MSE: 70.1819, RMSE: 8.3506\n",
            "Epoch 25, Step 1, Loss: 2.6727\n",
            "Epoch 25, Step 101, Loss: 2.6109\n",
            "Epoch 25, Step 201, Loss: 2.6021\n",
            "Epoch 25, Step 301, Loss: 2.6543\n",
            "Epoch 25, Step 401, Loss: 2.6036\n",
            "Epoch 25, Step 501, Loss: 2.6207\n",
            "Epoch 25, Step 601, Loss: 2.6599\n",
            "Epoch 25, Step 701, Loss: 2.5464\n",
            "Epoch 25, Step 801, Loss: 2.6298\n",
            "Epoch 25, Step 901, Loss: 2.6720\n",
            "Epoch 25, Step 1001, Loss: 2.5482\n",
            "Epoch 25, Step 1101, Loss: 2.5780\n",
            "Epoch 25, Step 1201, Loss: 2.5700\n",
            "Epoch 25, Step 1301, Loss: 2.6162\n",
            "Epoch 25, Step 1401, Loss: 2.6424\n",
            "Epoch 25, Step 1501, Loss: 2.5815\n",
            "Epoch 25, Step 1601, Loss: 2.6531\n",
            "Epoch 25, Step 1701, Loss: 2.6011\n",
            "Epoch 25, Step 1801, Loss: 2.6660\n",
            "Epoch 25, Loss: 2.6273, MAE: 5.7072, MSE: 70.1818, RMSE: 8.3502\n",
            "Epoch 26, Step 1, Loss: 2.7299\n",
            "Epoch 26, Step 101, Loss: 2.6694\n",
            "Epoch 26, Step 201, Loss: 2.6276\n",
            "Epoch 26, Step 301, Loss: 2.6326\n",
            "Epoch 26, Step 401, Loss: 2.5664\n",
            "Epoch 26, Step 501, Loss: 2.6224\n",
            "Epoch 26, Step 601, Loss: 2.6065\n",
            "Epoch 26, Step 701, Loss: 2.6767\n",
            "Epoch 26, Step 801, Loss: 2.6523\n",
            "Epoch 26, Step 901, Loss: 2.6898\n",
            "Epoch 26, Step 1001, Loss: 2.6042\n",
            "Epoch 26, Step 1101, Loss: 2.5776\n",
            "Epoch 26, Step 1201, Loss: 2.6839\n",
            "Epoch 26, Step 1301, Loss: 2.6521\n",
            "Epoch 26, Step 1401, Loss: 2.6246\n",
            "Epoch 26, Step 1501, Loss: 3.4716\n",
            "Epoch 26, Step 1601, Loss: 2.6033\n",
            "Epoch 26, Step 1701, Loss: 2.6534\n",
            "Epoch 26, Step 1801, Loss: 2.5590\n",
            "Epoch 26, Loss: 2.6270, MAE: 5.7072, MSE: 70.1818, RMSE: 8.3509\n",
            "Epoch 27, Step 1, Loss: 2.6073\n",
            "Epoch 27, Step 101, Loss: 2.6868\n",
            "Epoch 27, Step 201, Loss: 2.6031\n",
            "Epoch 27, Step 301, Loss: 2.5743\n",
            "Epoch 27, Step 401, Loss: 2.5383\n",
            "Epoch 27, Step 501, Loss: 2.5999\n",
            "Epoch 27, Step 601, Loss: 2.6336\n",
            "Epoch 27, Step 701, Loss: 2.5444\n",
            "Epoch 27, Step 801, Loss: 2.9547\n",
            "Epoch 27, Step 901, Loss: 2.6027\n",
            "Epoch 27, Step 1001, Loss: 2.6072\n",
            "Epoch 27, Step 1101, Loss: 2.7412\n",
            "Epoch 27, Step 1201, Loss: 2.6866\n",
            "Epoch 27, Step 1301, Loss: 2.6093\n",
            "Epoch 27, Step 1401, Loss: 2.5757\n",
            "Epoch 27, Step 1501, Loss: 2.6252\n",
            "Epoch 27, Step 1601, Loss: 2.6576\n",
            "Epoch 27, Step 1701, Loss: 2.5393\n",
            "Epoch 27, Step 1801, Loss: 2.6293\n",
            "Epoch 27, Loss: 2.6268, MAE: 5.7072, MSE: 70.1817, RMSE: 8.3515\n",
            "Epoch 28, Step 1, Loss: 2.6274\n",
            "Epoch 28, Step 101, Loss: 2.6158\n",
            "Epoch 28, Step 201, Loss: 2.6447\n",
            "Epoch 28, Step 301, Loss: 2.5597\n",
            "Epoch 28, Step 401, Loss: 2.6673\n",
            "Epoch 28, Step 501, Loss: 2.5534\n",
            "Epoch 28, Step 601, Loss: 2.6650\n",
            "Epoch 28, Step 701, Loss: 2.5288\n",
            "Epoch 28, Step 801, Loss: 2.6747\n",
            "Epoch 28, Step 901, Loss: 2.6254\n",
            "Epoch 28, Step 1001, Loss: 2.6126\n",
            "Epoch 28, Step 1101, Loss: 2.6073\n",
            "Epoch 28, Step 1201, Loss: 2.5979\n",
            "Epoch 28, Step 1301, Loss: 2.5700\n",
            "Epoch 28, Step 1401, Loss: 2.5636\n",
            "Epoch 28, Step 1501, Loss: 2.5041\n",
            "Epoch 28, Step 1601, Loss: 2.7081\n",
            "Epoch 28, Step 1701, Loss: 2.5354\n",
            "Epoch 28, Step 1801, Loss: 2.6001\n",
            "Epoch 28, Loss: 2.6266, MAE: 5.7072, MSE: 70.1819, RMSE: 8.3494\n",
            "Epoch 29, Step 1, Loss: 2.6072\n",
            "Epoch 29, Step 101, Loss: 2.6524\n",
            "Epoch 29, Step 201, Loss: 2.5416\n",
            "Epoch 29, Step 301, Loss: 2.7134\n",
            "Epoch 29, Step 401, Loss: 2.6538\n",
            "Epoch 29, Step 501, Loss: 2.5672\n",
            "Epoch 29, Step 601, Loss: 2.6201\n",
            "Epoch 29, Step 701, Loss: 2.5377\n",
            "Epoch 29, Step 801, Loss: 2.5326\n",
            "Epoch 29, Step 901, Loss: 2.5764\n",
            "Epoch 29, Step 1001, Loss: 2.5272\n",
            "Epoch 29, Step 1101, Loss: 2.6057\n",
            "Epoch 29, Step 1201, Loss: 2.6598\n",
            "Epoch 29, Step 1301, Loss: 2.5924\n",
            "Epoch 29, Step 1401, Loss: 2.5977\n",
            "Epoch 29, Step 1501, Loss: 2.5690\n",
            "Epoch 29, Step 1601, Loss: 2.5916\n",
            "Epoch 29, Step 1701, Loss: 2.6172\n",
            "Epoch 29, Step 1801, Loss: 2.5358\n",
            "Epoch 29, Loss: 2.6265, MAE: 5.7072, MSE: 70.1818, RMSE: 8.3509\n",
            "Epoch 30, Step 1, Loss: 2.6208\n",
            "Epoch 30, Step 101, Loss: 2.6892\n",
            "Epoch 30, Step 201, Loss: 2.6382\n",
            "Epoch 30, Step 301, Loss: 2.5201\n",
            "Epoch 30, Step 401, Loss: 2.5816\n",
            "Epoch 30, Step 501, Loss: 2.7225\n",
            "Epoch 30, Step 601, Loss: 2.5943\n",
            "Epoch 30, Step 701, Loss: 2.6595\n",
            "Epoch 30, Step 801, Loss: 2.5818\n",
            "Epoch 30, Step 901, Loss: 2.6294\n",
            "Epoch 30, Step 1001, Loss: 2.5596\n",
            "Epoch 30, Step 1101, Loss: 2.7024\n",
            "Epoch 30, Step 1201, Loss: 2.5753\n",
            "Epoch 30, Step 1301, Loss: 2.5467\n",
            "Epoch 30, Step 1401, Loss: 2.5706\n",
            "Epoch 30, Step 1501, Loss: 2.5581\n",
            "Epoch 30, Step 1601, Loss: 2.6860\n",
            "Epoch 30, Step 1701, Loss: 2.6115\n",
            "Epoch 30, Step 1801, Loss: 2.5317\n",
            "Epoch 30, Loss: 2.6264, MAE: 5.7072, MSE: 70.1820, RMSE: 8.3511\n",
            "TestingLoss: 2.6261, MAE: 5.7203, MSE: 70.1514, RMSE: 8.3535\n"
          ]
        }
      ],
      "source": [
        "# train and evaluate\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DeepAR(input_size=14, hidden_size=30, lstm_layers=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "epochs = 30\n",
        "loss_list, mae_list, mse_list, rmse_list = [], [], [], []\n",
        "\n",
        "# training\n",
        "for epoch in range(epochs):\n",
        "    total_loss, total_mae, total_mse, total_rmse, count = 0, 0, 0, 0, 0\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        hidden_state, cell_state = model.init_hidden(batch_size), model.init_cell(batch_size)\n",
        "        mu, sigma, hidden_state, cell_state = model(x, hidden_state, cell_state)\n",
        "\n",
        "        hidden_state = hidden_state.detach()\n",
        "        cell_state = cell_state.detach()\n",
        "\n",
        "        loss = gaussian_likelihood_loss(mu, sigma, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_mae += mae(mu, y).item()\n",
        "        total_mse += mse(mu, y).item()\n",
        "        total_rmse += rmse(mu, y).item()\n",
        "        count += 1\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch {epoch+1}, Step {i+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    avg_mae = total_mae / count\n",
        "    avg_mse = total_mse / count\n",
        "    avg_rmse = total_rmse / count\n",
        "\n",
        "    loss_list.append(avg_loss)\n",
        "    mae_list.append(avg_mae)\n",
        "    mse_list.append(avg_mse)\n",
        "    rmse_list.append(avg_rmse)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}, MAE: {avg_mae:.4f}, MSE: {avg_mse:.4f}, RMSE: {avg_rmse:.4f}')\n",
        "\n",
        "# evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_loss, total_mae, total_mse, total_rmse, count = 0, 0, 0, 0, 0\n",
        "\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        hidden_state, cell_state = model.init_hidden(batch_size), model.init_cell(batch_size)\n",
        "        mu, sigma, hidden_state, cell_state = model(x, hidden_state, cell_state)\n",
        "\n",
        "        hidden_state = hidden_state.detach()\n",
        "        cell_state = cell_state.detach()\n",
        "\n",
        "        loss = gaussian_likelihood_loss(mu, sigma, y)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_mae += mae(mu, y).item()\n",
        "        total_mse += mse(mu, y).item()\n",
        "        total_rmse += rmse(mu, y).item()\n",
        "        count += 1\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    avg_mae = total_mae / count\n",
        "    avg_mse = total_mse / count\n",
        "    avg_rmse = total_rmse / count\n",
        "\n",
        "    print(f'TestingLoss: {avg_loss:.4f}, MAE: {avg_mae:.4f}, MSE: {avg_mse:.4f}, RMSE: {avg_rmse:.4f}')\n",
        "\n",
        "# save model\n",
        "torch.save(model.state_dict(), 'simple_deep_ar.pth')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "proj-hGPfCqDx",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}