{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erOz2J1KyNgT",
        "outputId": "ac0998c5-fc33-46d2-fad7-5d747cd1c3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a simplified version of DeepAR model\n",
        "# https://arxiv.org/pdf/1704.04110.pdf\n",
        "\n",
        "# references\n",
        "# LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "# deepAR: https://github.com/zhykoties/TimeSeries/blob/master/model/net.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import gc\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "yJup4-H5yeHe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data with new features\n",
        "df = pd.read_csv('drive/MyDrive/kaggle/train_with_new_features_new.csv')"
      ],
      "metadata": {
        "id": "Ff7w2JatyZue"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = df.drop([\"target\", \"row_id\", \"time_id\"], axis=1)\n",
        "target_df = df['target']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features_df)"
      ],
      "metadata": {
        "id": "iACs-_GNzFKx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split, dataloader\n",
        "batch_size = 1024\n",
        "\n",
        "features_tensor = torch.tensor(features_scaled, dtype=torch.float32)\n",
        "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Y7IZ9H5BysIB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplified DeepAR model\n",
        "\n",
        "class DeepAR(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, lstm_layers=2, device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')):\n",
        "        super(DeepAR, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.device = device\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=self.lstm_layers, batch_first=True)\n",
        "\n",
        "        self.distribution_mu = nn.Linear(hidden_size * self.lstm_layers, 1)\n",
        "        self.distribution_presigma = nn.Linear(hidden_size * self.lstm_layers, 1)\n",
        "        self.distribution_sigma = nn.Softplus()     # make sure sigma is positive\n",
        "\n",
        "    '''\n",
        "        x: (batch_size, seq_len, input_size)\n",
        "        hidden_state: (num_layers, batch_size, hidden_size)\n",
        "        cell_state: (num_layers, batch_size, hidden_size)\n",
        "    '''\n",
        "    def forward(self, x, hidden_state=None, cell_state=None):\n",
        "        x = x.unsqueeze(1) # seq len is 1\n",
        "\n",
        "        if hidden_state is None:\n",
        "            hidden_state = self.init_hidden(x.shape[0])\n",
        "        if cell_state is None:\n",
        "            cell_state = self.init_cell(x.shape[0])\n",
        "\n",
        "        lstm_out, (hidden, cell) = self.lstm(x, (hidden_state, cell_state))\n",
        "        hidden_permute = hidden.permute(1, 0, 2).contiguous().view(hidden.shape[1], -1)\n",
        "\n",
        "        # Predicting mu and sigma\n",
        "        mu = self.distribution_mu(hidden_permute)\n",
        "        pre_sigma = self.distribution_presigma(hidden_permute)\n",
        "        sigma = self.distribution_sigma(pre_sigma)\n",
        "\n",
        "        return mu, sigma, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.lstm_layers, batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "    def init_cell(self, batch_size):\n",
        "        return torch.zeros(self.lstm_layers, batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "# gaussian log likelihood loss\n",
        "def gaussian_likelihood_loss(mu, sigma, y):\n",
        "    return torch.mean(0.5 * torch.log(sigma**2) + 0.5 * ((y - mu) / sigma)**2)\n",
        "\n",
        "# mae\n",
        "def mae(predictions, targets):\n",
        "    return torch.mean(torch.abs(predictions - targets))\n",
        "\n",
        "# mse\n",
        "def mse(predictions, targets):\n",
        "    return torch.mean((predictions - targets) ** 2)\n",
        "\n",
        "# rmse\n",
        "def rmse(predictions, targets):\n",
        "    return torch.sqrt(mse(predictions, targets))"
      ],
      "metadata": {
        "id": "ZNfzkmG6zkEJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and evaluate\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DeepAR(input_size=21, hidden_size=30, lstm_layers=2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "epochs = 30\n",
        "loss_list, mae_list, mse_list, rmse_list = [], [], [], []\n",
        "\n",
        "# training\n",
        "for epoch in range(epochs):\n",
        "    total_loss, total_mae, total_mse, total_rmse, count = 0, 0, 0, 0, 0\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        hidden_state, cell_state = model.init_hidden(batch_size), model.init_cell(batch_size)\n",
        "        mu, sigma, hidden_state, cell_state = model(x, hidden_state, cell_state)\n",
        "\n",
        "        hidden_state = hidden_state.detach()\n",
        "        cell_state = cell_state.detach()\n",
        "\n",
        "        loss = gaussian_likelihood_loss(mu, sigma, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_mae += mae(mu, y).item()\n",
        "        total_mse += mse(mu, y).item()\n",
        "        total_rmse += rmse(mu, y).item()\n",
        "        count += 1\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch {epoch+1}, Step {i+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    avg_mae = total_mae / count\n",
        "    avg_mse = total_mse / count\n",
        "    avg_rmse = total_rmse / count\n",
        "\n",
        "    loss_list.append(avg_loss)\n",
        "    mae_list.append(avg_mae)\n",
        "    mse_list.append(avg_mse)\n",
        "    rmse_list.append(avg_rmse)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}, MAE: {avg_mae:.4f}, MSE: {avg_mse:.4f}, RMSE: {avg_rmse:.4f}')\n",
        "\n",
        "# evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_loss, total_mae, total_mse, total_rmse, count = 0, 0, 0, 0, 0\n",
        "\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        hidden_state, cell_state = model.init_hidden(batch_size), model.init_cell(batch_size)\n",
        "        mu, sigma, hidden_state, cell_state = model(x, hidden_state, cell_state)\n",
        "\n",
        "        hidden_state = hidden_state.detach()\n",
        "        cell_state = cell_state.detach()\n",
        "\n",
        "        loss = gaussian_likelihood_loss(mu, sigma, y)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_mae += mae(mu, y).item()\n",
        "        total_mse += mse(mu, y).item()\n",
        "        total_rmse += rmse(mu, y).item()\n",
        "        count += 1\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    avg_mae = total_mae / count\n",
        "    avg_mse = total_mse / count\n",
        "    avg_rmse = total_rmse / count\n",
        "\n",
        "    print(f'TestingLoss: {avg_loss:.4f}, MAE: {avg_mae:.4f}, MSE: {avg_mse:.4f}, RMSE: {avg_rmse:.4f}')\n",
        "\n",
        "# save model\n",
        "torch.save(model.state_dict(), 'simple_deep_ar_with_new_fe.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiW6p-VLzmJs",
        "outputId": "dc226471-7992-4c58-b785-68f3ab0f5adb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Step 1, Loss: 49.4580\n",
            "Epoch 1, Step 101, Loss: 63.5983\n",
            "Epoch 1, Step 201, Loss: 70.7789\n",
            "Epoch 1, Step 301, Loss: 60.6694\n",
            "Epoch 1, Step 401, Loss: 46.0297\n",
            "Epoch 1, Step 501, Loss: 55.3965\n",
            "Epoch 1, Step 601, Loss: 70.9823\n",
            "Epoch 1, Step 701, Loss: 57.7740\n",
            "Epoch 1, Step 801, Loss: 58.1219\n",
            "Epoch 1, Step 901, Loss: 60.6345\n",
            "Epoch 1, Step 1001, Loss: 59.1574\n",
            "Epoch 1, Step 1101, Loss: 53.1490\n",
            "Epoch 1, Step 1201, Loss: 75.8951\n",
            "Epoch 1, Step 1301, Loss: 67.5621\n",
            "Epoch 1, Step 1401, Loss: 57.1084\n",
            "Epoch 1, Step 1501, Loss: 48.1916\n",
            "Epoch 1, Step 1601, Loss: 58.5585\n",
            "Epoch 1, Step 1701, Loss: 46.0477\n",
            "Epoch 1, Step 1801, Loss: 93.5870\n",
            "Epoch 1, Loss: 60.8926, MAE: 5.7072, MSE: 70.1827, RMSE: 8.3513\n",
            "Epoch 2, Step 1, Loss: 64.9380\n",
            "Epoch 2, Step 101, Loss: 53.5038\n",
            "Epoch 2, Step 201, Loss: 55.0116\n",
            "Epoch 2, Step 301, Loss: 55.2506\n",
            "Epoch 2, Step 401, Loss: 55.0566\n",
            "Epoch 2, Step 501, Loss: 95.6142\n",
            "Epoch 2, Step 601, Loss: 40.9143\n",
            "Epoch 2, Step 701, Loss: 54.5988\n",
            "Epoch 2, Step 801, Loss: 42.3558\n",
            "Epoch 2, Step 901, Loss: 50.4284\n",
            "Epoch 2, Step 1001, Loss: 47.3762\n",
            "Epoch 2, Step 1101, Loss: 47.8152\n",
            "Epoch 2, Step 1201, Loss: 49.2693\n",
            "Epoch 2, Step 1301, Loss: 40.7070\n",
            "Epoch 2, Step 1401, Loss: 44.9766\n",
            "Epoch 2, Step 1501, Loss: 45.5249\n",
            "Epoch 2, Step 1601, Loss: 40.9702\n",
            "Epoch 2, Step 1701, Loss: 42.0922\n",
            "Epoch 2, Step 1801, Loss: 44.3191\n",
            "Epoch 2, Loss: 50.1472, MAE: 5.7073, MSE: 70.1832, RMSE: 8.3506\n",
            "Epoch 3, Step 1, Loss: 43.4521\n",
            "Epoch 3, Step 101, Loss: 70.2132\n",
            "Epoch 3, Step 201, Loss: 44.3075\n",
            "Epoch 3, Step 301, Loss: 44.0491\n",
            "Epoch 3, Step 401, Loss: 37.9778\n",
            "Epoch 3, Step 501, Loss: 40.3916\n",
            "Epoch 3, Step 601, Loss: 39.5827\n",
            "Epoch 3, Step 701, Loss: 41.1004\n",
            "Epoch 3, Step 801, Loss: 69.0360\n",
            "Epoch 3, Step 901, Loss: 35.6199\n",
            "Epoch 3, Step 1001, Loss: 34.7077\n",
            "Epoch 3, Step 1101, Loss: 42.1949\n",
            "Epoch 3, Step 1201, Loss: 38.8755\n",
            "Epoch 3, Step 1301, Loss: 34.7403\n",
            "Epoch 3, Step 1401, Loss: 28.6745\n",
            "Epoch 3, Step 1501, Loss: 37.2196\n",
            "Epoch 3, Step 1601, Loss: 35.4261\n",
            "Epoch 3, Step 1701, Loss: 43.6680\n",
            "Epoch 3, Step 1801, Loss: 36.5466\n",
            "Epoch 3, Loss: 38.8910, MAE: 5.7073, MSE: 70.1835, RMSE: 8.3518\n",
            "Epoch 4, Step 1, Loss: 29.5880\n",
            "Epoch 4, Step 101, Loss: 31.8136\n",
            "Epoch 4, Step 201, Loss: 28.0273\n",
            "Epoch 4, Step 301, Loss: 25.3729\n",
            "Epoch 4, Step 401, Loss: 32.0201\n",
            "Epoch 4, Step 501, Loss: 49.6636\n",
            "Epoch 4, Step 601, Loss: 27.4260\n",
            "Epoch 4, Step 701, Loss: 24.8964\n",
            "Epoch 4, Step 801, Loss: 27.6134\n",
            "Epoch 4, Step 901, Loss: 28.1270\n",
            "Epoch 4, Step 1001, Loss: 24.0271\n",
            "Epoch 4, Step 1101, Loss: 33.9483\n",
            "Epoch 4, Step 1201, Loss: 27.6208\n",
            "Epoch 4, Step 1301, Loss: 24.8254\n",
            "Epoch 4, Step 1401, Loss: 23.2067\n",
            "Epoch 4, Step 1501, Loss: 21.8894\n",
            "Epoch 4, Step 1601, Loss: 28.7748\n",
            "Epoch 4, Step 1701, Loss: 24.9879\n",
            "Epoch 4, Step 1801, Loss: 24.3338\n",
            "Epoch 4, Loss: 28.4276, MAE: 5.7073, MSE: 70.1839, RMSE: 8.3512\n",
            "Epoch 5, Step 1, Loss: 24.7155\n",
            "Epoch 5, Step 101, Loss: 18.5276\n",
            "Epoch 5, Step 201, Loss: 23.9248\n",
            "Epoch 5, Step 301, Loss: 22.4745\n",
            "Epoch 5, Step 401, Loss: 23.9798\n",
            "Epoch 5, Step 501, Loss: 21.2636\n",
            "Epoch 5, Step 601, Loss: 16.9042\n",
            "Epoch 5, Step 701, Loss: 18.8119\n",
            "Epoch 5, Step 801, Loss: 18.2785\n",
            "Epoch 5, Step 901, Loss: 20.3711\n",
            "Epoch 5, Step 1001, Loss: 17.3524\n",
            "Epoch 5, Step 1101, Loss: 18.4496\n",
            "Epoch 5, Step 1201, Loss: 19.0487\n",
            "Epoch 5, Step 1301, Loss: 17.5371\n",
            "Epoch 5, Step 1401, Loss: 16.3071\n",
            "Epoch 5, Step 1501, Loss: 18.9164\n",
            "Epoch 5, Step 1601, Loss: 15.2664\n",
            "Epoch 5, Step 1701, Loss: 14.2967\n",
            "Epoch 5, Step 1801, Loss: 17.6047\n",
            "Epoch 5, Loss: 19.9194, MAE: 5.7073, MSE: 70.1838, RMSE: 8.3508\n",
            "Epoch 6, Step 1, Loss: 17.5092\n",
            "Epoch 6, Step 101, Loss: 15.3869\n",
            "Epoch 6, Step 201, Loss: 14.6677\n",
            "Epoch 6, Step 301, Loss: 13.0592\n",
            "Epoch 6, Step 401, Loss: 14.7614\n",
            "Epoch 6, Step 501, Loss: 13.6994\n",
            "Epoch 6, Step 601, Loss: 15.3654\n",
            "Epoch 6, Step 701, Loss: 13.8536\n",
            "Epoch 6, Step 801, Loss: 10.6008\n",
            "Epoch 6, Step 901, Loss: 12.9827\n",
            "Epoch 6, Step 1001, Loss: 10.5501\n",
            "Epoch 6, Step 1101, Loss: 12.4920\n",
            "Epoch 6, Step 1201, Loss: 11.6543\n",
            "Epoch 6, Step 1301, Loss: 13.1270\n",
            "Epoch 6, Step 1401, Loss: 10.8397\n",
            "Epoch 6, Step 1501, Loss: 10.8959\n",
            "Epoch 6, Step 1601, Loss: 11.0149\n",
            "Epoch 6, Step 1701, Loss: 13.5262\n",
            "Epoch 6, Step 1801, Loss: 11.9819\n",
            "Epoch 6, Loss: 13.7249, MAE: 5.7073, MSE: 70.1833, RMSE: 8.3520\n",
            "Epoch 7, Step 1, Loss: 9.2330\n",
            "Epoch 7, Step 101, Loss: 13.6069\n",
            "Epoch 7, Step 201, Loss: 9.5912\n",
            "Epoch 7, Step 301, Loss: 8.4092\n",
            "Epoch 7, Step 401, Loss: 9.5102\n",
            "Epoch 7, Step 501, Loss: 10.6018\n",
            "Epoch 7, Step 601, Loss: 9.0616\n",
            "Epoch 7, Step 701, Loss: 9.7277\n",
            "Epoch 7, Step 801, Loss: 10.8318\n",
            "Epoch 7, Step 901, Loss: 15.2628\n",
            "Epoch 7, Step 1001, Loss: 8.9524\n",
            "Epoch 7, Step 1101, Loss: 8.8897\n",
            "Epoch 7, Step 1201, Loss: 8.4872\n",
            "Epoch 7, Step 1301, Loss: 8.2159\n",
            "Epoch 7, Step 1401, Loss: 7.6729\n",
            "Epoch 7, Step 1501, Loss: 8.6913\n",
            "Epoch 7, Step 1601, Loss: 9.1495\n",
            "Epoch 7, Step 1701, Loss: 8.2660\n",
            "Epoch 7, Step 1801, Loss: 8.6937\n",
            "Epoch 7, Loss: 9.5566, MAE: 5.7073, MSE: 70.1833, RMSE: 8.3504\n",
            "Epoch 8, Step 1, Loss: 7.4581\n",
            "Epoch 8, Step 101, Loss: 6.9683\n",
            "Epoch 8, Step 201, Loss: 7.5713\n",
            "Epoch 8, Step 301, Loss: 5.9418\n",
            "Epoch 8, Step 401, Loss: 6.6789\n",
            "Epoch 8, Step 501, Loss: 6.6948\n",
            "Epoch 8, Step 601, Loss: 6.7254\n",
            "Epoch 8, Step 701, Loss: 7.6836\n",
            "Epoch 8, Step 801, Loss: 7.6972\n",
            "Epoch 8, Step 901, Loss: 6.3681\n",
            "Epoch 8, Step 1001, Loss: 5.6502\n",
            "Epoch 8, Step 1101, Loss: 8.0359\n",
            "Epoch 8, Step 1201, Loss: 6.9559\n",
            "Epoch 8, Step 1301, Loss: 5.8124\n",
            "Epoch 8, Step 1401, Loss: 5.3872\n",
            "Epoch 8, Step 1501, Loss: 12.3925\n",
            "Epoch 8, Step 1601, Loss: 5.3023\n",
            "Epoch 8, Step 1701, Loss: 5.2154\n",
            "Epoch 8, Step 1801, Loss: 6.1318\n",
            "Epoch 8, Loss: 6.8981, MAE: 5.7073, MSE: 70.1834, RMSE: 8.3507\n",
            "Epoch 9, Step 1, Loss: 6.3390\n",
            "Epoch 9, Step 101, Loss: 6.1385\n",
            "Epoch 9, Step 201, Loss: 6.4596\n",
            "Epoch 9, Step 301, Loss: 4.9847\n",
            "Epoch 9, Step 401, Loss: 5.5307\n",
            "Epoch 9, Step 501, Loss: 4.9243\n",
            "Epoch 9, Step 601, Loss: 4.9908\n",
            "Epoch 9, Step 701, Loss: 5.0007\n",
            "Epoch 9, Step 801, Loss: 5.0480\n",
            "Epoch 9, Step 901, Loss: 5.1813\n",
            "Epoch 9, Step 1001, Loss: 5.2357\n",
            "Epoch 9, Step 1101, Loss: 5.9619\n",
            "Epoch 9, Step 1201, Loss: 5.0854\n",
            "Epoch 9, Step 1301, Loss: 8.9790\n",
            "Epoch 9, Step 1401, Loss: 4.9459\n",
            "Epoch 9, Step 1501, Loss: 4.9318\n",
            "Epoch 9, Step 1601, Loss: 4.5448\n",
            "Epoch 9, Step 1701, Loss: 4.6533\n",
            "Epoch 9, Step 1801, Loss: 4.5720\n",
            "Epoch 9, Loss: 5.2408, MAE: 5.7073, MSE: 70.1832, RMSE: 8.3512\n",
            "Epoch 10, Step 1, Loss: 4.4233\n",
            "Epoch 10, Step 101, Loss: 4.1834\n",
            "Epoch 10, Step 201, Loss: 4.9481\n",
            "Epoch 10, Step 301, Loss: 3.9282\n",
            "Epoch 10, Step 401, Loss: 4.5458\n",
            "Epoch 10, Step 501, Loss: 4.1543\n",
            "Epoch 10, Step 601, Loss: 4.5154\n",
            "Epoch 10, Step 701, Loss: 4.7646\n",
            "Epoch 10, Step 801, Loss: 3.8726\n",
            "Epoch 10, Step 901, Loss: 4.0337\n",
            "Epoch 10, Step 1001, Loss: 3.6228\n",
            "Epoch 10, Step 1101, Loss: 3.5093\n",
            "Epoch 10, Step 1201, Loss: 4.6682\n",
            "Epoch 10, Step 1301, Loss: 3.9215\n",
            "Epoch 10, Step 1401, Loss: 4.2158\n",
            "Epoch 10, Step 1501, Loss: 4.2656\n",
            "Epoch 10, Step 1601, Loss: 3.6017\n",
            "Epoch 10, Step 1701, Loss: 3.7704\n",
            "Epoch 10, Step 1801, Loss: 3.7752\n",
            "Epoch 10, Loss: 4.2157, MAE: 5.7073, MSE: 70.1835, RMSE: 8.3500\n",
            "Epoch 11, Step 1, Loss: 3.9933\n",
            "Epoch 11, Step 101, Loss: 3.5199\n",
            "Epoch 11, Step 201, Loss: 4.0084\n",
            "Epoch 11, Step 301, Loss: 3.3213\n",
            "Epoch 11, Step 401, Loss: 3.9557\n",
            "Epoch 11, Step 501, Loss: 3.3123\n",
            "Epoch 11, Step 601, Loss: 3.5604\n",
            "Epoch 11, Step 701, Loss: 3.5085\n",
            "Epoch 11, Step 801, Loss: 3.5638\n",
            "Epoch 11, Step 901, Loss: 3.8208\n",
            "Epoch 11, Step 1001, Loss: 3.3000\n",
            "Epoch 11, Step 1101, Loss: 3.3638\n",
            "Epoch 11, Step 1201, Loss: 3.3333\n",
            "Epoch 11, Step 1301, Loss: 3.4751\n",
            "Epoch 11, Step 1401, Loss: 3.2026\n",
            "Epoch 11, Step 1501, Loss: 3.3201\n",
            "Epoch 11, Step 1601, Loss: 3.3348\n",
            "Epoch 11, Step 1701, Loss: 3.9370\n",
            "Epoch 11, Step 1801, Loss: 3.7644\n",
            "Epoch 11, Loss: 3.5801, MAE: 5.7073, MSE: 70.1836, RMSE: 8.3504\n",
            "Epoch 12, Step 1, Loss: 3.7802\n",
            "Epoch 12, Step 101, Loss: 3.2294\n",
            "Epoch 12, Step 201, Loss: 3.5559\n",
            "Epoch 12, Step 301, Loss: 3.2510\n",
            "Epoch 12, Step 401, Loss: 3.4325\n",
            "Epoch 12, Step 501, Loss: 3.5119\n",
            "Epoch 12, Step 601, Loss: 3.1699\n",
            "Epoch 12, Step 701, Loss: 3.2535\n",
            "Epoch 12, Step 801, Loss: 3.0470\n",
            "Epoch 12, Step 901, Loss: 3.5722\n",
            "Epoch 12, Step 1001, Loss: 3.0394\n",
            "Epoch 12, Step 1101, Loss: 2.8832\n",
            "Epoch 12, Step 1201, Loss: 2.9890\n",
            "Epoch 12, Step 1301, Loss: 3.0664\n",
            "Epoch 12, Step 1401, Loss: 3.4543\n",
            "Epoch 12, Step 1501, Loss: 2.9890\n",
            "Epoch 12, Step 1601, Loss: 2.9251\n",
            "Epoch 12, Step 1701, Loss: 2.8932\n",
            "Epoch 12, Step 1801, Loss: 2.8402\n",
            "Epoch 12, Loss: 3.1870, MAE: 5.7073, MSE: 70.1835, RMSE: 8.3515\n",
            "Epoch 13, Step 1, Loss: 3.0495\n",
            "Epoch 13, Step 101, Loss: 2.8688\n",
            "Epoch 13, Step 201, Loss: 2.9193\n",
            "Epoch 13, Step 301, Loss: 2.9149\n",
            "Epoch 13, Step 401, Loss: 2.9777\n",
            "Epoch 13, Step 501, Loss: 2.8765\n",
            "Epoch 13, Step 601, Loss: 3.2660\n",
            "Epoch 13, Step 701, Loss: 2.7305\n",
            "Epoch 13, Step 801, Loss: 3.0008\n",
            "Epoch 13, Step 901, Loss: 2.9502\n",
            "Epoch 13, Step 1001, Loss: 3.0429\n",
            "Epoch 13, Step 1101, Loss: 2.8808\n",
            "Epoch 13, Step 1201, Loss: 2.8178\n",
            "Epoch 13, Step 1301, Loss: 2.8270\n",
            "Epoch 13, Step 1401, Loss: 2.8191\n",
            "Epoch 13, Step 1501, Loss: 2.8986\n",
            "Epoch 13, Step 1601, Loss: 2.7363\n",
            "Epoch 13, Step 1701, Loss: 2.8087\n",
            "Epoch 13, Step 1801, Loss: 2.7856\n",
            "Epoch 13, Loss: 2.9456, MAE: 5.7073, MSE: 70.1838, RMSE: 8.3501\n",
            "Epoch 14, Step 1, Loss: 2.9494\n",
            "Epoch 14, Step 101, Loss: 2.6543\n",
            "Epoch 14, Step 201, Loss: 3.0066\n",
            "Epoch 14, Step 301, Loss: 2.9943\n",
            "Epoch 14, Step 401, Loss: 2.9040\n",
            "Epoch 14, Step 501, Loss: 2.8079\n",
            "Epoch 14, Step 601, Loss: 2.6673\n",
            "Epoch 14, Step 701, Loss: 2.7722\n",
            "Epoch 14, Step 801, Loss: 2.8490\n",
            "Epoch 14, Step 901, Loss: 2.8155\n",
            "Epoch 14, Step 1001, Loss: 2.8889\n",
            "Epoch 14, Step 1101, Loss: 2.7558\n",
            "Epoch 14, Step 1201, Loss: 2.7938\n",
            "Epoch 14, Step 1301, Loss: 2.7538\n",
            "Epoch 14, Step 1401, Loss: 2.6374\n",
            "Epoch 14, Step 1501, Loss: 2.7170\n",
            "Epoch 14, Step 1601, Loss: 2.7210\n",
            "Epoch 14, Step 1701, Loss: 2.5895\n",
            "Epoch 14, Step 1801, Loss: 2.7448\n",
            "Epoch 14, Loss: 2.8000, MAE: 5.7073, MSE: 70.1834, RMSE: 8.3519\n",
            "Epoch 15, Step 1, Loss: 2.9178\n",
            "Epoch 15, Step 101, Loss: 2.5871\n",
            "Epoch 15, Step 201, Loss: 2.8965\n",
            "Epoch 15, Step 301, Loss: 2.6373\n",
            "Epoch 15, Step 401, Loss: 2.6860\n",
            "Epoch 15, Step 501, Loss: 2.7590\n",
            "Epoch 15, Step 601, Loss: 2.7304\n",
            "Epoch 15, Step 701, Loss: 2.7123\n",
            "Epoch 15, Step 801, Loss: 2.7080\n",
            "Epoch 15, Step 901, Loss: 2.6315\n",
            "Epoch 15, Step 1001, Loss: 2.6433\n",
            "Epoch 15, Step 1101, Loss: 2.5510\n",
            "Epoch 15, Step 1201, Loss: 2.6055\n",
            "Epoch 15, Step 1301, Loss: 2.7737\n",
            "Epoch 15, Step 1401, Loss: 2.6604\n",
            "Epoch 15, Step 1501, Loss: 2.6897\n",
            "Epoch 15, Step 1601, Loss: 2.7341\n",
            "Epoch 15, Step 1701, Loss: 2.7144\n",
            "Epoch 15, Step 1801, Loss: 2.6056\n",
            "Epoch 15, Loss: 2.7141, MAE: 5.7073, MSE: 70.1836, RMSE: 8.3514\n",
            "Epoch 16, Step 1, Loss: 2.5986\n",
            "Epoch 16, Step 101, Loss: 2.7386\n",
            "Epoch 16, Step 201, Loss: 2.6790\n",
            "Epoch 16, Step 301, Loss: 2.6166\n",
            "Epoch 16, Step 401, Loss: 2.6095\n",
            "Epoch 16, Step 501, Loss: 2.6666\n",
            "Epoch 16, Step 601, Loss: 2.5953\n",
            "Epoch 16, Step 701, Loss: 2.5635\n",
            "Epoch 16, Step 801, Loss: 2.6837\n",
            "Epoch 16, Step 901, Loss: 2.5886\n",
            "Epoch 16, Step 1001, Loss: 2.5563\n",
            "Epoch 16, Step 1101, Loss: 2.6843\n",
            "Epoch 16, Step 1201, Loss: 2.7165\n",
            "Epoch 16, Step 1301, Loss: 2.6536\n",
            "Epoch 16, Step 1401, Loss: 2.6967\n",
            "Epoch 16, Step 1501, Loss: 2.6941\n",
            "Epoch 16, Step 1601, Loss: 2.5588\n",
            "Epoch 16, Step 1701, Loss: 2.7045\n",
            "Epoch 16, Step 1801, Loss: 2.5740\n",
            "Epoch 16, Loss: 2.6668, MAE: 5.7073, MSE: 70.1834, RMSE: 8.3512\n",
            "Epoch 17, Step 1, Loss: 2.6696\n",
            "Epoch 17, Step 101, Loss: 2.5604\n",
            "Epoch 17, Step 201, Loss: 2.6102\n",
            "Epoch 17, Step 301, Loss: 2.6101\n",
            "Epoch 17, Step 401, Loss: 2.6856\n",
            "Epoch 17, Step 501, Loss: 2.5876\n",
            "Epoch 17, Step 601, Loss: 2.6467\n",
            "Epoch 17, Step 701, Loss: 2.6560\n",
            "Epoch 17, Step 801, Loss: 2.5785\n",
            "Epoch 17, Step 901, Loss: 2.5722\n",
            "Epoch 17, Step 1001, Loss: 2.6570\n",
            "Epoch 17, Step 1101, Loss: 2.6919\n",
            "Epoch 17, Step 1201, Loss: 2.5839\n",
            "Epoch 17, Step 1301, Loss: 2.9116\n",
            "Epoch 17, Step 1401, Loss: 2.5389\n",
            "Epoch 17, Step 1501, Loss: 2.5808\n",
            "Epoch 17, Step 1601, Loss: 2.6787\n",
            "Epoch 17, Step 1701, Loss: 2.6072\n",
            "Epoch 17, Step 1801, Loss: 2.6578\n",
            "Epoch 17, Loss: 2.6432, MAE: 5.7073, MSE: 70.1832, RMSE: 8.3504\n",
            "Epoch 18, Step 1, Loss: 2.7050\n",
            "Epoch 18, Step 101, Loss: 2.6329\n",
            "Epoch 18, Step 201, Loss: 2.6209\n",
            "Epoch 18, Step 301, Loss: 2.6492\n",
            "Epoch 18, Step 401, Loss: 2.7087\n",
            "Epoch 18, Step 501, Loss: 2.6291\n",
            "Epoch 18, Step 601, Loss: 2.6674\n",
            "Epoch 18, Step 701, Loss: 2.7958\n",
            "Epoch 18, Step 801, Loss: 2.5854\n",
            "Epoch 18, Step 901, Loss: 2.5716\n",
            "Epoch 18, Step 1001, Loss: 2.6313\n",
            "Epoch 18, Step 1101, Loss: 2.6194\n",
            "Epoch 18, Step 1201, Loss: 2.6140\n",
            "Epoch 18, Step 1301, Loss: 2.6969\n",
            "Epoch 18, Step 1401, Loss: 2.5848\n",
            "Epoch 18, Step 1501, Loss: 2.6433\n",
            "Epoch 18, Step 1601, Loss: 2.5991\n",
            "Epoch 18, Step 1701, Loss: 2.6415\n",
            "Epoch 18, Step 1801, Loss: 2.7021\n",
            "Epoch 18, Loss: 2.6333, MAE: 5.7073, MSE: 70.1836, RMSE: 8.3511\n",
            "Epoch 19, Step 1, Loss: 2.6467\n",
            "Epoch 19, Step 101, Loss: 2.7326\n",
            "Epoch 19, Step 201, Loss: 2.6964\n",
            "Epoch 19, Step 301, Loss: 2.6278\n",
            "Epoch 19, Step 401, Loss: 2.6660\n",
            "Epoch 19, Step 501, Loss: 2.5769\n",
            "Epoch 19, Step 601, Loss: 2.5573\n",
            "Epoch 19, Step 701, Loss: 2.6026\n",
            "Epoch 19, Step 801, Loss: 2.9392\n",
            "Epoch 19, Step 901, Loss: 2.5694\n",
            "Epoch 19, Step 1001, Loss: 2.5927\n",
            "Epoch 19, Step 1101, Loss: 2.6129\n",
            "Epoch 19, Step 1201, Loss: 2.5862\n",
            "Epoch 19, Step 1301, Loss: 2.7757\n",
            "Epoch 19, Step 1401, Loss: 2.6650\n",
            "Epoch 19, Step 1501, Loss: 2.6820\n",
            "Epoch 19, Step 1601, Loss: 2.5493\n",
            "Epoch 19, Step 1701, Loss: 2.5334\n",
            "Epoch 19, Step 1801, Loss: 2.6437\n",
            "Epoch 19, Loss: 2.6300, MAE: 5.7073, MSE: 70.1831, RMSE: 8.3506\n",
            "Epoch 20, Step 1, Loss: 2.6459\n",
            "Epoch 20, Step 101, Loss: 2.7482\n",
            "Epoch 20, Step 201, Loss: 2.5599\n",
            "Epoch 20, Step 301, Loss: 2.5501\n",
            "Epoch 20, Step 401, Loss: 2.5866\n",
            "Epoch 20, Step 501, Loss: 2.6759\n",
            "Epoch 20, Step 601, Loss: 2.5917\n",
            "Epoch 20, Step 701, Loss: 2.6043\n",
            "Epoch 20, Step 801, Loss: 2.5844\n",
            "Epoch 20, Step 901, Loss: 2.5955\n",
            "Epoch 20, Step 1001, Loss: 2.5597\n",
            "Epoch 20, Step 1101, Loss: 2.6210\n",
            "Epoch 20, Step 1201, Loss: 2.6236\n",
            "Epoch 20, Step 1301, Loss: 2.6134\n",
            "Epoch 20, Step 1401, Loss: 2.6202\n",
            "Epoch 20, Step 1501, Loss: 2.5998\n",
            "Epoch 20, Step 1601, Loss: 2.5683\n",
            "Epoch 20, Step 1701, Loss: 2.6121\n",
            "Epoch 20, Step 1801, Loss: 2.5774\n",
            "Epoch 20, Loss: 2.6290, MAE: 5.7073, MSE: 70.1835, RMSE: 8.3508\n",
            "Epoch 21, Step 1, Loss: 2.6532\n",
            "Epoch 21, Step 101, Loss: 2.6067\n",
            "Epoch 21, Step 201, Loss: 2.5947\n",
            "Epoch 21, Step 301, Loss: 3.1140\n",
            "Epoch 21, Step 401, Loss: 2.9927\n",
            "Epoch 21, Step 501, Loss: 2.6889\n",
            "Epoch 21, Step 601, Loss: 2.5693\n",
            "Epoch 21, Step 701, Loss: 2.6878\n",
            "Epoch 21, Step 801, Loss: 2.6165\n",
            "Epoch 21, Step 901, Loss: 2.6300\n",
            "Epoch 21, Step 1001, Loss: 2.7084\n",
            "Epoch 21, Step 1101, Loss: 2.6571\n",
            "Epoch 21, Step 1201, Loss: 2.6899\n",
            "Epoch 21, Step 1301, Loss: 2.6988\n",
            "Epoch 21, Step 1401, Loss: 2.6708\n",
            "Epoch 21, Step 1501, Loss: 2.6629\n",
            "Epoch 21, Step 1601, Loss: 2.6500\n",
            "Epoch 21, Step 1701, Loss: 2.5804\n",
            "Epoch 21, Step 1801, Loss: 2.5724\n",
            "Epoch 21, Loss: 2.6284, MAE: 5.7073, MSE: 70.1829, RMSE: 8.3507\n",
            "Epoch 22, Step 1, Loss: 2.6743\n",
            "Epoch 22, Step 101, Loss: 2.6536\n",
            "Epoch 22, Step 201, Loss: 2.6317\n",
            "Epoch 22, Step 301, Loss: 2.6232\n",
            "Epoch 22, Step 401, Loss: 2.5688\n",
            "Epoch 22, Step 501, Loss: 2.6491\n",
            "Epoch 22, Step 601, Loss: 2.5952\n",
            "Epoch 22, Step 701, Loss: 2.6168\n",
            "Epoch 22, Step 801, Loss: 2.7702\n",
            "Epoch 22, Step 901, Loss: 2.6656\n",
            "Epoch 22, Step 1001, Loss: 2.7161\n",
            "Epoch 22, Step 1101, Loss: 2.5886\n",
            "Epoch 22, Step 1201, Loss: 2.5535\n",
            "Epoch 22, Step 1301, Loss: 2.6492\n",
            "Epoch 22, Step 1401, Loss: 2.8229\n",
            "Epoch 22, Step 1501, Loss: 3.1153\n",
            "Epoch 22, Step 1601, Loss: 2.5437\n",
            "Epoch 22, Step 1701, Loss: 2.5758\n",
            "Epoch 22, Step 1801, Loss: 2.7456\n",
            "Epoch 22, Loss: 2.6280, MAE: 5.7073, MSE: 70.1835, RMSE: 8.3515\n",
            "Epoch 23, Step 1, Loss: 2.5698\n",
            "Epoch 23, Step 101, Loss: 2.5720\n",
            "Epoch 23, Step 201, Loss: 2.5704\n",
            "Epoch 23, Step 301, Loss: 2.6183\n",
            "Epoch 23, Step 401, Loss: 2.6282\n",
            "Epoch 23, Step 501, Loss: 2.6108\n",
            "Epoch 23, Step 601, Loss: 2.5767\n",
            "Epoch 23, Step 701, Loss: 2.6127\n",
            "Epoch 23, Step 801, Loss: 2.6316\n",
            "Epoch 23, Step 901, Loss: 2.5664\n",
            "Epoch 23, Step 1001, Loss: 2.6452\n",
            "Epoch 23, Step 1101, Loss: 2.7141\n",
            "Epoch 23, Step 1201, Loss: 2.6131\n",
            "Epoch 23, Step 1301, Loss: 2.6226\n",
            "Epoch 23, Step 1401, Loss: 2.6215\n",
            "Epoch 23, Step 1501, Loss: 2.5918\n",
            "Epoch 23, Step 1601, Loss: 2.6831\n",
            "Epoch 23, Step 1701, Loss: 2.7595\n",
            "Epoch 23, Step 1801, Loss: 2.5870\n",
            "Epoch 23, Loss: 2.6276, MAE: 5.7073, MSE: 70.1830, RMSE: 8.3508\n",
            "Epoch 24, Step 1, Loss: 2.6272\n",
            "Epoch 24, Step 101, Loss: 2.5859\n",
            "Epoch 24, Step 201, Loss: 2.5833\n",
            "Epoch 24, Step 301, Loss: 2.7524\n",
            "Epoch 24, Step 401, Loss: 2.5804\n",
            "Epoch 24, Step 501, Loss: 2.5666\n",
            "Epoch 24, Step 601, Loss: 2.5860\n",
            "Epoch 24, Step 701, Loss: 2.6498\n",
            "Epoch 24, Step 801, Loss: 2.5412\n",
            "Epoch 24, Step 901, Loss: 2.6309\n",
            "Epoch 24, Step 1001, Loss: 2.5917\n",
            "Epoch 24, Step 1101, Loss: 2.8478\n",
            "Epoch 24, Step 1201, Loss: 2.6428\n",
            "Epoch 24, Step 1301, Loss: 2.6613\n",
            "Epoch 24, Step 1401, Loss: 2.5706\n",
            "Epoch 24, Step 1501, Loss: 2.6025\n",
            "Epoch 24, Step 1601, Loss: 2.5881\n",
            "Epoch 24, Step 1701, Loss: 2.5743\n",
            "Epoch 24, Step 1801, Loss: 2.6026\n",
            "Epoch 24, Loss: 2.6273, MAE: 5.7072, MSE: 70.1826, RMSE: 8.3502\n",
            "Epoch 25, Step 1, Loss: 2.5614\n",
            "Epoch 25, Step 101, Loss: 2.5442\n",
            "Epoch 25, Step 201, Loss: 2.6314\n",
            "Epoch 25, Step 301, Loss: 2.6637\n",
            "Epoch 25, Step 401, Loss: 2.5675\n",
            "Epoch 25, Step 501, Loss: 2.5785\n",
            "Epoch 25, Step 601, Loss: 2.7066\n",
            "Epoch 25, Step 701, Loss: 2.6216\n",
            "Epoch 25, Step 801, Loss: 2.6169\n",
            "Epoch 25, Step 901, Loss: 2.5773\n",
            "Epoch 25, Step 1001, Loss: 2.8059\n",
            "Epoch 25, Step 1101, Loss: 2.6051\n",
            "Epoch 25, Step 1201, Loss: 2.5811\n",
            "Epoch 25, Step 1301, Loss: 2.6832\n",
            "Epoch 25, Step 1401, Loss: 2.6197\n",
            "Epoch 25, Step 1501, Loss: 2.5451\n",
            "Epoch 25, Step 1601, Loss: 2.5703\n",
            "Epoch 25, Step 1701, Loss: 2.6434\n",
            "Epoch 25, Step 1801, Loss: 2.6067\n",
            "Epoch 25, Loss: 2.6271, MAE: 5.7072, MSE: 70.1827, RMSE: 8.3511\n",
            "Epoch 26, Step 1, Loss: 2.5922\n",
            "Epoch 26, Step 101, Loss: 2.6462\n",
            "Epoch 26, Step 201, Loss: 2.5579\n",
            "Epoch 26, Step 301, Loss: 2.5601\n",
            "Epoch 26, Step 401, Loss: 2.4875\n",
            "Epoch 26, Step 501, Loss: 2.6259\n",
            "Epoch 26, Step 601, Loss: 2.6756\n",
            "Epoch 26, Step 701, Loss: 2.5892\n",
            "Epoch 26, Step 801, Loss: 2.5521\n",
            "Epoch 26, Step 901, Loss: 2.5931\n",
            "Epoch 26, Step 1001, Loss: 2.5643\n",
            "Epoch 26, Step 1101, Loss: 2.6654\n",
            "Epoch 26, Step 1201, Loss: 2.6326\n",
            "Epoch 26, Step 1301, Loss: 2.6211\n",
            "Epoch 26, Step 1401, Loss: 2.7061\n",
            "Epoch 26, Step 1501, Loss: 2.6130\n",
            "Epoch 26, Step 1601, Loss: 2.6154\n",
            "Epoch 26, Step 1701, Loss: 2.8236\n",
            "Epoch 26, Step 1801, Loss: 2.6136\n",
            "Epoch 26, Loss: 2.6269, MAE: 5.7072, MSE: 70.1828, RMSE: 8.3513\n",
            "Epoch 27, Step 1, Loss: 2.6018\n",
            "Epoch 27, Step 101, Loss: 2.6005\n",
            "Epoch 27, Step 201, Loss: 2.6115\n",
            "Epoch 27, Step 301, Loss: 2.6723\n",
            "Epoch 27, Step 401, Loss: 2.6771\n",
            "Epoch 27, Step 501, Loss: 2.5485\n",
            "Epoch 27, Step 601, Loss: 2.6133\n",
            "Epoch 27, Step 701, Loss: 2.5990\n",
            "Epoch 27, Step 801, Loss: 2.5590\n",
            "Epoch 27, Step 901, Loss: 3.3166\n",
            "Epoch 27, Step 1001, Loss: 2.6058\n",
            "Epoch 27, Step 1101, Loss: 2.5560\n",
            "Epoch 27, Step 1201, Loss: 2.5680\n",
            "Epoch 27, Step 1301, Loss: 2.5699\n",
            "Epoch 27, Step 1401, Loss: 2.5280\n",
            "Epoch 27, Step 1501, Loss: 2.6415\n",
            "Epoch 27, Step 1601, Loss: 2.6188\n",
            "Epoch 27, Step 1701, Loss: 2.5901\n",
            "Epoch 27, Step 1801, Loss: 2.5673\n",
            "Epoch 27, Loss: 2.6267, MAE: 5.7072, MSE: 70.1826, RMSE: 8.3498\n",
            "Epoch 28, Step 1, Loss: 2.7016\n",
            "Epoch 28, Step 101, Loss: 2.6548\n",
            "Epoch 28, Step 201, Loss: 2.7996\n",
            "Epoch 28, Step 301, Loss: 2.5909\n",
            "Epoch 28, Step 401, Loss: 2.6746\n",
            "Epoch 28, Step 501, Loss: 2.5965\n",
            "Epoch 28, Step 601, Loss: 2.5750\n",
            "Epoch 28, Step 701, Loss: 2.5674\n",
            "Epoch 28, Step 801, Loss: 2.6086\n",
            "Epoch 28, Step 901, Loss: 2.7406\n",
            "Epoch 28, Step 1001, Loss: 2.6205\n",
            "Epoch 28, Step 1101, Loss: 2.5319\n",
            "Epoch 28, Step 1201, Loss: 2.6140\n",
            "Epoch 28, Step 1301, Loss: 2.5828\n",
            "Epoch 28, Step 1401, Loss: 2.6818\n",
            "Epoch 28, Step 1501, Loss: 2.6200\n",
            "Epoch 28, Step 1601, Loss: 2.6175\n",
            "Epoch 28, Step 1701, Loss: 2.6327\n",
            "Epoch 28, Step 1801, Loss: 2.5808\n",
            "Epoch 28, Loss: 2.6266, MAE: 5.7072, MSE: 70.1825, RMSE: 8.3506\n",
            "Epoch 29, Step 1, Loss: 2.5690\n",
            "Epoch 29, Step 101, Loss: 2.6108\n",
            "Epoch 29, Step 201, Loss: 2.7814\n",
            "Epoch 29, Step 301, Loss: 2.6990\n",
            "Epoch 29, Step 401, Loss: 2.6426\n",
            "Epoch 29, Step 501, Loss: 2.6215\n",
            "Epoch 29, Step 601, Loss: 2.6323\n",
            "Epoch 29, Step 701, Loss: 2.5842\n",
            "Epoch 29, Step 801, Loss: 2.6201\n",
            "Epoch 29, Step 901, Loss: 2.5556\n",
            "Epoch 29, Step 1001, Loss: 2.5914\n",
            "Epoch 29, Step 1101, Loss: 2.7578\n",
            "Epoch 29, Step 1201, Loss: 3.6387\n",
            "Epoch 29, Step 1301, Loss: 2.6835\n",
            "Epoch 29, Step 1401, Loss: 2.5379\n",
            "Epoch 29, Step 1501, Loss: 2.5886\n",
            "Epoch 29, Step 1601, Loss: 2.6875\n",
            "Epoch 29, Step 1701, Loss: 2.6575\n",
            "Epoch 29, Step 1801, Loss: 2.5925\n",
            "Epoch 29, Loss: 2.6265, MAE: 5.7072, MSE: 70.1826, RMSE: 8.3500\n",
            "Epoch 30, Step 1, Loss: 2.5692\n",
            "Epoch 30, Step 101, Loss: 2.6272\n",
            "Epoch 30, Step 201, Loss: 2.5998\n",
            "Epoch 30, Step 301, Loss: 2.6522\n",
            "Epoch 30, Step 401, Loss: 2.6488\n",
            "Epoch 30, Step 501, Loss: 2.6172\n",
            "Epoch 30, Step 601, Loss: 2.6369\n",
            "Epoch 30, Step 701, Loss: 2.6359\n",
            "Epoch 30, Step 801, Loss: 2.5780\n",
            "Epoch 30, Step 901, Loss: 2.6282\n",
            "Epoch 30, Step 1001, Loss: 2.6562\n",
            "Epoch 30, Step 1101, Loss: 2.6055\n",
            "Epoch 30, Step 1201, Loss: 2.5481\n",
            "Epoch 30, Step 1301, Loss: 2.6426\n",
            "Epoch 30, Step 1401, Loss: 2.5969\n",
            "Epoch 30, Step 1501, Loss: 2.5968\n",
            "Epoch 30, Step 1601, Loss: 2.5383\n",
            "Epoch 30, Step 1701, Loss: 2.6450\n",
            "Epoch 30, Step 1801, Loss: 2.5892\n",
            "Epoch 30, Loss: 2.6264, MAE: 5.7072, MSE: 70.1823, RMSE: 8.3515\n",
            "TestingLoss: 2.6261, MAE: 5.7204, MSE: 70.1524, RMSE: 8.3536\n"
          ]
        }
      ]
    }
  ]
}